{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bin_Conv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e6FA9yDPjTR",
        "colab_type": "code",
        "outputId": "46ba1eb9-ac1e-447e-fe0c-2c08d3815f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization, MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import InputSpec, Layer, Dense, Conv2D\n",
        "from keras import constraints\n",
        "from keras import initializers\n",
        "\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAk6OKdsT2v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Clip(constraints.Constraint):\n",
        "    def __init__(self, min_value, max_value=None):\n",
        "        self.min_value = min_value\n",
        "        self.max_value = max_value\n",
        "        if not self.max_value:\n",
        "            self.max_value = -self.min_value\n",
        "        if self.min_value > self.max_value:\n",
        "            self.min_value, self.max_value = self.max_value, self.min_value\n",
        "\n",
        "    def __call__(self, p):\n",
        "        return K.clip(p, self.min_value, self.max_value)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"min_value\": self.min_value,\n",
        "                \"max_value\": self.max_value}\n",
        "\n",
        "\n",
        "class BinaryDense(Dense):\n",
        "    ''' Binarized Dense layer\n",
        "    References: \n",
        "    \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\" [http://arxiv.org/abs/1602.02830]\n",
        "    '''\n",
        "    def __init__(self, units, H=1., kernel_lr_multiplier='Glorot', bias_lr_multiplier=None, **kwargs):\n",
        "        super(BinaryDense, self).__init__(units, **kwargs)\n",
        "        self.H = H\n",
        "        self.kernel_lr_multiplier = kernel_lr_multiplier\n",
        "        self.bias_lr_multiplier = bias_lr_multiplier\n",
        "        \n",
        "        super(BinaryDense, self).__init__(units, **kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[1]\n",
        "\n",
        "        if self.H == 'Glorot':\n",
        "            self.H = np.float32(np.sqrt(1.5 / (input_dim + self.units)))\n",
        "            #print('Glorot H: {}'.format(self.H))\n",
        "        if self.kernel_lr_multiplier == 'Glorot':\n",
        "            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5 / (input_dim + self.units)))\n",
        "            #print('Glorot learning rate multiplier: {}'.format(self.kernel_lr_multiplier))\n",
        "            \n",
        "        self.kernel_constraint = Clip(-self.H, self.H)\n",
        "        self.kernel_initializer = initializers.RandomUniform(-self.H, self.H)\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='kernel',\n",
        "                                     regularizer=self.kernel_regularizer,\n",
        "                                     constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier, self.bias_lr_multiplier]\n",
        "            self.bias = self.add_weight(shape=(self.output_dim,),\n",
        "                                     initializer=self.bias_initializer,\n",
        "                                     name='bias',\n",
        "                                     regularizer=self.bias_regularizer,\n",
        "                                     constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier]\n",
        "            self.bias = None\n",
        "\n",
        "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        binary_kernel = binarize(self.kernel, H=self.H)\n",
        "        output = K.dot(inputs, binary_kernel)\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {'H': self.H,\n",
        "                  'kernel_lr_multiplier': self.kernel_lr_multiplier,\n",
        "                  'bias_lr_multiplier': self.bias_lr_multiplier}\n",
        "        base_config = super(BinaryDense, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class BinaryConv2D(Conv2D):\n",
        "    '''Binarized Convolution2D layer\n",
        "    References: \n",
        "    \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\" [http://arxiv.org/abs/1602.02830]\n",
        "    '''\n",
        "    def __init__(self, filters, kernel_lr_multiplier='Glorot', \n",
        "                 bias_lr_multiplier=None, H=1., **kwargs):\n",
        "        super(BinaryConv2D, self).__init__(filters, **kwargs)\n",
        "        self.H = H\n",
        "        self.kernel_lr_multiplier = kernel_lr_multiplier\n",
        "        self.bias_lr_multiplier = bias_lr_multiplier\n",
        "        \n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1 \n",
        "        if input_shape[channel_axis] is None:\n",
        "                raise ValueError('The channel dimension of the inputs '\n",
        "                                 'should be defined. Found `None`.')\n",
        "\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "            \n",
        "        base = self.kernel_size[0] * self.kernel_size[1]\n",
        "        if self.H == 'Glorot':\n",
        "            nb_input = int(input_dim * base)\n",
        "            nb_output = int(self.filters * base)\n",
        "            self.H = np.float32(np.sqrt(1.5 / (nb_input + nb_output)))\n",
        "            #print('Glorot H: {}'.format(self.H))\n",
        "            \n",
        "        if self.kernel_lr_multiplier == 'Glorot':\n",
        "            nb_input = int(input_dim * base)\n",
        "            nb_output = int(self.filters * base)\n",
        "            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5/ (nb_input + nb_output)))\n",
        "            #print('Glorot learning rate multiplier: {}'.format(self.lr_multiplier))\n",
        "\n",
        "        self.kernel_constraint = Clip(-self.H, self.H)\n",
        "        self.kernel_initializer = initializers.RandomUniform(-self.H, self.H)\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='kernel',\n",
        "                                 regularizer=self.kernel_regularizer,\n",
        "                                 constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier, self.bias_lr_multiplier]\n",
        "            self.bias = self.add_weight((self.output_dim,),\n",
        "                                     initializer=self.bias_initializers,\n",
        "                                     name='bias',\n",
        "                                     regularizer=self.bias_regularizer,\n",
        "                                     constraint=self.bias_constraint)\n",
        "\n",
        "        else:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier]\n",
        "            self.bias = None\n",
        "\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        binary_kernel = binarize(self.kernel, H=self.H) \n",
        "        outputs = K.conv2d(\n",
        "            inputs,\n",
        "            binary_kernel,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {'H': self.H,\n",
        "                  'kernel_lr_multiplier': self.kernel_lr_multiplier,\n",
        "                  'bias_lr_multiplier': self.bias_lr_multiplier}\n",
        "        base_config = super(BinaryConv2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-vBlXdVTeZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def round_through(x):\n",
        "    '''Element-wise rounding to the closest integer with full gradient propagation.\n",
        "    A trick from [Sergey Ioffe](http://stackoverflow.com/a/36480182)\n",
        "    '''\n",
        "    rounded = K.round(x)\n",
        "    return x + K.stop_gradient(rounded - x)\n",
        "\n",
        "\n",
        "def _hard_sigmoid(x):\n",
        "    '''Hard sigmoid different from the more conventional form (see definition of K.hard_sigmoid).\n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    x = (0.5 * x) + 0.5\n",
        "    return K.clip(x, 0, 1)\n",
        "\n",
        "\n",
        "def binary_sigmoid(x):\n",
        "    '''Binary hard sigmoid for training binarized neural network.\n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    return round_through(_hard_sigmoid(x))\n",
        "\n",
        "\n",
        "def binary_tanh(x):\n",
        "    '''Binary hard sigmoid for training binarized neural network.\n",
        "     The neurons' activations binarization function\n",
        "     It behaves like the sign function during forward propagation\n",
        "     And like:\n",
        "        hard_tanh(x) = 2 * _hard_sigmoid(x) - 1 \n",
        "        clear gradient when |x| > 1 during back propagation\n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    return 2 * round_through(_hard_sigmoid(x)) - 1\n",
        "\n",
        "\n",
        "def binarize(W, H=1):\n",
        "    '''The weights' binarization function, \n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    # [-H, H] -> -H or H\n",
        "    Wb = H * binary_tanh(W / H)\n",
        "    return Wb\n",
        "\n",
        "\n",
        "def _mean_abs(x, axis=None, keepdims=False):\n",
        "    return K.stop_gradient(K.mean(K.abs(x), axis=axis, keepdims=keepdims))\n",
        "\n",
        "    \n",
        "def xnorize(W, H=1., axis=None, keepdims=False):\n",
        "    Wb = binarize(W, H)\n",
        "    Wa = _mean_abs(W, axis, keepdims)\n",
        "    \n",
        "    return Wa, Wb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOW1t5XBTRvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H = 1.\n",
        "kernel_lr_multiplier = 'Glorot'\n",
        "\n",
        "# nn\n",
        "batch_size = 50\n",
        "epochs = 20 \n",
        "channels = 1\n",
        "img_rows = 28 \n",
        "img_cols = 28 \n",
        "filters = 32 \n",
        "kernel_size = (3, 3)\n",
        "pool_size = (2, 2)\n",
        "hidden_units = 128\n",
        "classes = 10\n",
        "use_bias = False\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 1e-3\n",
        "lr_end = 1e-4\n",
        "lr_decay = (lr_end / lr_start)**(1. / epochs)\n",
        "\n",
        "# BN\n",
        "epsilon = 1e-6\n",
        "momentum = 0.9\n",
        "\n",
        "# dropout\n",
        "#p1 = 0.25\n",
        "#p2 = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iefjU46TUiY",
        "colab_type": "code",
        "outputId": "31aa916a-720f-4bb3-9649-53c1949c977b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 1, 28, 28)\n",
        "X_test = X_test.reshape(10000, 1, 28, 28)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, classes) * 2 - 1 # -1 or 1 for hinge loss\n",
        "Y_test = np_utils.to_categorical(y_test, classes) * 2 - 1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePTIBhmrTWdR",
        "colab_type": "code",
        "outputId": "710ebb83-596f-438f-90d2-eed20d3388d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        }
      },
      "source": [
        "model = Sequential()\n",
        "# conv1\n",
        "model.add(BinaryConv2D(32, kernel_size=(3,3), input_shape=(channels, img_rows, img_cols),data_format='channels_first',H=H, kernel_lr_multiplier=kernel_lr_multiplier,use_bias=use_bias, name='conv1'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn1'))\n",
        "model.add(Activation(binary_tanh, name='act1'))\n",
        "# conv2\n",
        "model.add(BinaryConv2D(64, kernel_size=(3,3),  data_format='channels_first',H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias, name='conv2'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn2'))\n",
        "model.add(Activation(binary_tanh, name='act2'))\n",
        "#Maxpool\n",
        "model.add(MaxPooling2D(pool_size=(2,2), name='pool2', data_format='channels_first'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn3'))\n",
        "model.add(Activation(binary_tanh, name='act3'))\n",
        "\n",
        "model.add(Flatten())\n",
        "# dense1\n",
        "model.add(BinaryDense(128, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias, name='dense5'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn4'))\n",
        "model.add(Activation(binary_tanh, name='act5'))\n",
        "# dense2\n",
        "model.add(BinaryDense(10, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias, name='dense6'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn5'))\n",
        "\n",
        "opt = Adam(lr=lr_start) \n",
        "model.compile(loss='squared_hinge', optimizer=opt, metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (BinaryConv2D)         (None, 32, 26, 26)        288       \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 32, 26, 26)        128       \n",
            "_________________________________________________________________\n",
            "act1 (Activation)            (None, 32, 26, 26)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (BinaryConv2D)         (None, 64, 24, 24)        18432     \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 64, 24, 24)        256       \n",
            "_________________________________________________________________\n",
            "act2 (Activation)            (None, 64, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 64, 12, 12)        0         \n",
            "_________________________________________________________________\n",
            "bn3 (BatchNormalization)     (None, 64, 12, 12)        256       \n",
            "_________________________________________________________________\n",
            "act3 (Activation)            (None, 64, 12, 12)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense5 (BinaryDense)         (None, 128)               1179648   \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "act5 (Activation)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense6 (BinaryDense)         (None, 10)                1280      \n",
            "_________________________________________________________________\n",
            "bn5 (BatchNormalization)     (None, 10)                40        \n",
            "=================================================================\n",
            "Total params: 1,200,840\n",
            "Trainable params: 1,200,244\n",
            "Non-trainable params: 596\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1zVyWGqTbZC",
        "colab_type": "code",
        "outputId": "8c08445d-f0fc-417a-e6ae-bc69bfbaa215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr_scheduler = LearningRateScheduler(lambda e: lr_start * lr_decay ** e)\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=batch_size, epochs=75,\n",
        "                    verbose=1, validation_data=(X_test, Y_test),\n",
        "                    callbacks=[lr_scheduler])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/75\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 20s 325us/step - loss: 0.4378 - acc: 0.7346 - val_loss: 0.1436 - val_acc: 0.7936\n",
            "Epoch 2/75\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.1338 - acc: 0.8052 - val_loss: 0.1130 - val_acc: 0.8241\n",
            "Epoch 3/75\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.1158 - acc: 0.8258 - val_loss: 0.1064 - val_acc: 0.8342\n",
            "Epoch 4/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.1069 - acc: 0.8398 - val_loss: 0.0944 - val_acc: 0.8530\n",
            "Epoch 5/75\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.1015 - acc: 0.8484 - val_loss: 0.0991 - val_acc: 0.8420\n",
            "Epoch 6/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0988 - acc: 0.8521 - val_loss: 0.0943 - val_acc: 0.8513\n",
            "Epoch 7/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0973 - acc: 0.8539 - val_loss: 0.0926 - val_acc: 0.8553\n",
            "Epoch 8/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0937 - acc: 0.8599 - val_loss: 0.0869 - val_acc: 0.8607\n",
            "Epoch 9/75\n",
            "60000/60000 [==============================] - 16s 258us/step - loss: 0.0934 - acc: 0.8605 - val_loss: 0.0882 - val_acc: 0.8644\n",
            "Epoch 10/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0935 - acc: 0.8624 - val_loss: 0.0898 - val_acc: 0.8611\n",
            "Epoch 11/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0917 - acc: 0.8631 - val_loss: 0.0895 - val_acc: 0.8608\n",
            "Epoch 12/75\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0916 - acc: 0.8645 - val_loss: 0.0871 - val_acc: 0.8646\n",
            "Epoch 13/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0901 - acc: 0.8662 - val_loss: 0.0892 - val_acc: 0.8611\n",
            "Epoch 14/75\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0895 - acc: 0.8672 - val_loss: 0.0872 - val_acc: 0.8637\n",
            "Epoch 15/75\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0886 - acc: 0.8690 - val_loss: 0.0842 - val_acc: 0.8681\n",
            "Epoch 16/75\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0877 - acc: 0.8703 - val_loss: 0.0833 - val_acc: 0.8674\n",
            "Epoch 17/75\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0870 - acc: 0.8723 - val_loss: 0.0885 - val_acc: 0.8607\n",
            "Epoch 18/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0876 - acc: 0.8703 - val_loss: 0.0904 - val_acc: 0.8599\n",
            "Epoch 19/75\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.0868 - acc: 0.8728 - val_loss: 0.0856 - val_acc: 0.8648\n",
            "Epoch 20/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0865 - acc: 0.8725 - val_loss: 0.0814 - val_acc: 0.8707\n",
            "Epoch 21/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0856 - acc: 0.8756 - val_loss: 0.0861 - val_acc: 0.8657\n",
            "Epoch 22/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0863 - acc: 0.8723 - val_loss: 0.0886 - val_acc: 0.8639\n",
            "Epoch 23/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0857 - acc: 0.8737 - val_loss: 0.0847 - val_acc: 0.8649\n",
            "Epoch 24/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0856 - acc: 0.8724 - val_loss: 0.0840 - val_acc: 0.8695\n",
            "Epoch 25/75\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0864 - acc: 0.8725 - val_loss: 0.0837 - val_acc: 0.8673\n",
            "Epoch 26/75\n",
            "60000/60000 [==============================] - 16s 258us/step - loss: 0.0856 - acc: 0.8746 - val_loss: 0.0833 - val_acc: 0.8679\n",
            "Epoch 27/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0854 - acc: 0.8746 - val_loss: 0.0826 - val_acc: 0.8702\n",
            "Epoch 28/75\n",
            "60000/60000 [==============================] - 16s 258us/step - loss: 0.0849 - acc: 0.8748 - val_loss: 0.0858 - val_acc: 0.8682\n",
            "Epoch 29/75\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.0847 - acc: 0.8744 - val_loss: 0.0797 - val_acc: 0.8740\n",
            "Epoch 30/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0842 - acc: 0.8765 - val_loss: 0.0820 - val_acc: 0.8698\n",
            "Epoch 31/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0841 - acc: 0.8754 - val_loss: 0.0853 - val_acc: 0.8684\n",
            "Epoch 32/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0853 - acc: 0.8750 - val_loss: 0.0832 - val_acc: 0.8674\n",
            "Epoch 33/75\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0843 - acc: 0.8760 - val_loss: 0.0817 - val_acc: 0.8677\n",
            "Epoch 34/75\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0850 - acc: 0.8751 - val_loss: 0.0911 - val_acc: 0.8571\n",
            "Epoch 35/75\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0842 - acc: 0.8764 - val_loss: 0.0860 - val_acc: 0.8660\n",
            "Epoch 36/75\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0840 - acc: 0.8761 - val_loss: 0.0846 - val_acc: 0.8680\n",
            "Epoch 37/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0835 - acc: 0.8771 - val_loss: 0.0837 - val_acc: 0.8692\n",
            "Epoch 38/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0839 - acc: 0.8765 - val_loss: 0.0832 - val_acc: 0.8681\n",
            "Epoch 39/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0845 - acc: 0.8742 - val_loss: 0.0843 - val_acc: 0.8640\n",
            "Epoch 40/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0825 - acc: 0.8789 - val_loss: 0.0903 - val_acc: 0.8624\n",
            "Epoch 41/75\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.0833 - acc: 0.8779 - val_loss: 0.0849 - val_acc: 0.8670\n",
            "Epoch 42/75\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.0844 - acc: 0.8750 - val_loss: 0.0848 - val_acc: 0.8637\n",
            "Epoch 43/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0843 - acc: 0.8754 - val_loss: 0.0930 - val_acc: 0.8541\n",
            "Epoch 44/75\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0840 - acc: 0.8773 - val_loss: 0.0822 - val_acc: 0.8713\n",
            "Epoch 45/75\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.0841 - acc: 0.8773 - val_loss: 0.0842 - val_acc: 0.8669\n",
            "Epoch 46/75\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.0845 - acc: 0.8758 - val_loss: 0.0877 - val_acc: 0.8630\n",
            "Epoch 47/75\n",
            "60000/60000 [==============================] - 16s 258us/step - loss: 0.0841 - acc: 0.8779 - val_loss: 0.0826 - val_acc: 0.8707\n",
            "Epoch 48/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0835 - acc: 0.8763 - val_loss: 0.0922 - val_acc: 0.8575\n",
            "Epoch 49/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0844 - acc: 0.8763 - val_loss: 0.0859 - val_acc: 0.8648\n",
            "Epoch 50/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0832 - acc: 0.8775 - val_loss: 0.0849 - val_acc: 0.8671\n",
            "Epoch 51/75\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0837 - acc: 0.8776 - val_loss: 0.0853 - val_acc: 0.8649\n",
            "Epoch 52/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0841 - acc: 0.8760 - val_loss: 0.0902 - val_acc: 0.8637\n",
            "Epoch 53/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0837 - acc: 0.8763 - val_loss: 0.0820 - val_acc: 0.8735\n",
            "Epoch 54/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0844 - acc: 0.8751 - val_loss: 0.0845 - val_acc: 0.8697\n",
            "Epoch 55/75\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.0838 - acc: 0.8776 - val_loss: 0.0846 - val_acc: 0.8687\n",
            "Epoch 56/75\n",
            "60000/60000 [==============================] - 16s 258us/step - loss: 0.0834 - acc: 0.8769 - val_loss: 0.0840 - val_acc: 0.8681\n",
            "Epoch 57/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0836 - acc: 0.8774 - val_loss: 0.0832 - val_acc: 0.8705\n",
            "Epoch 58/75\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0838 - acc: 0.8758 - val_loss: 0.0796 - val_acc: 0.8718\n",
            "Epoch 59/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0834 - acc: 0.8775 - val_loss: 0.0898 - val_acc: 0.8592\n",
            "Epoch 60/75\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0836 - acc: 0.8761 - val_loss: 0.0824 - val_acc: 0.8725\n",
            "Epoch 61/75\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0834 - acc: 0.8767 - val_loss: 0.0822 - val_acc: 0.8708\n",
            "Epoch 62/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0836 - acc: 0.8776 - val_loss: 0.0820 - val_acc: 0.8708\n",
            "Epoch 63/75\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0834 - acc: 0.8777 - val_loss: 0.0840 - val_acc: 0.8669\n",
            "Epoch 64/75\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0842 - acc: 0.8757 - val_loss: 0.0845 - val_acc: 0.8682\n",
            "Epoch 65/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0836 - acc: 0.8774 - val_loss: 0.0827 - val_acc: 0.8685\n",
            "Epoch 66/75\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0838 - acc: 0.8766 - val_loss: 0.0845 - val_acc: 0.8667\n",
            "Epoch 67/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0837 - acc: 0.8783 - val_loss: 0.0812 - val_acc: 0.8724\n",
            "Epoch 68/75\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.0837 - acc: 0.8764 - val_loss: 0.0859 - val_acc: 0.8662\n",
            "Epoch 69/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0827 - acc: 0.8784 - val_loss: 0.0849 - val_acc: 0.8649\n",
            "Epoch 70/75\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0838 - acc: 0.8768 - val_loss: 0.0813 - val_acc: 0.8728\n",
            "Epoch 71/75\n",
            "60000/60000 [==============================] - 16s 265us/step - loss: 0.0830 - acc: 0.8773 - val_loss: 0.0781 - val_acc: 0.8741\n",
            "Epoch 72/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0826 - acc: 0.8780 - val_loss: 0.0882 - val_acc: 0.8677\n",
            "Epoch 73/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0831 - acc: 0.8761 - val_loss: 0.0861 - val_acc: 0.8642\n",
            "Epoch 74/75\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0835 - acc: 0.8768 - val_loss: 0.0863 - val_acc: 0.8630\n",
            "Epoch 75/75\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.0836 - acc: 0.8779 - val_loss: 0.0831 - val_acc: 0.8690\n",
            "Test score: 0.0831484024554491\n",
            "Test accuracy: 0.869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2EJKgBRnrm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
        "\n",
        "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
        "    confidence1 = []\n",
        "    confidence2 = []\n",
        "    acc1 = 0\n",
        "    acc2 = 0\n",
        "    for num in range(confidence_mtx1.shape[0]):\n",
        "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
        "        if np.argmax(confidence_mtx1[num,:]) == np.argmax(label_vec1[num]):\n",
        "            acc1 += 1\n",
        "\n",
        "    for num in range(confidence_mtx2.shape[0]):\n",
        "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
        "        if np.argmax(confidence_mtx2[num,:]) == np.argmax(label_vec2[num]):\n",
        "            acc2 += 1\n",
        "    confidence1 = np.array(confidence1)\n",
        "    confidence2 = np.array(confidence2)\n",
        "\n",
        "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
        "\n",
        "\n",
        "    #sort_confidence = np.sort(confidence1)\n",
        "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
        "    max_accuracy = 0.5\n",
        "    best_precision = 0.5\n",
        "    best_recall = 0.5\n",
        "    for num in range(len(sort_confidence)):\n",
        "        delta = sort_confidence[num]\n",
        "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
        "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
        "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
        "        if accuracy_now > max_accuracy:\n",
        "            max_accuracy = accuracy_now\n",
        "            best_precision = ratio1/(ratio1+ratio2)\n",
        "            best_recall = ratio1\n",
        "    print('maximum inference accuracy is:', max_accuracy)\n",
        "    print('precision is:', best_precision)\n",
        "    print('recall is:', best_recall)\n",
        "    return max_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeS7hyhhnvxd",
        "colab_type": "code",
        "outputId": "7f97b03f-668f-4310-cf46-d8ebdfcc9262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "output_train=model.predict(X_train)\n",
        "output_test=model.predict(X_test)\n",
        "Y_test = Y_test.astype('int')\n",
        "Y_train = Y_train.astype('int')\n",
        "\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, Y_train, Y_test)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy for training and test- (0.8868333333333334, 0.869)\n",
            "maximum inference accuracy is: 0.5545249999999999\n",
            "precision is: 0.5033647749257157\n",
            "recall is: 8.15685\n",
            "Maximum Accuracy: 0.5545249999999999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}