{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XNOR_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlDrOwObKYlU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "02f64e41-772a-46a2-9266-ebf3d9ff5d08"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import InputSpec, Layer, Dense, Conv2D\n",
        "from keras import constraints\n",
        "from keras import initializers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5pKtQLPJ_Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xnorize(W, H=1., axis=None, keepdims=False):\n",
        "    Wb = binarize(W, H)\n",
        "    Wa = _mean_abs(W, axis, keepdims)\n",
        "    \n",
        "    return Wa, Wb\n",
        "  \n",
        "  \n",
        "def _mean_abs(x, axis=None, keepdims=False):\n",
        "    return K.stop_gradient(K.mean(K.abs(x), axis=axis, keepdims=keepdims))\n",
        "  \n",
        "  \n",
        "def _hard_sigmoid(x):\n",
        "    '''Hard sigmoid different from the more conventional form (see definition of K.hard_sigmoid).\n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    x = (0.5 * x) + 0.5\n",
        "    return K.clip(x, 0, 1)\n",
        "\n",
        "def round_through(x):\n",
        "    '''Element-wise rounding to the closest integer with full gradient propagation.\n",
        "    A trick from [Sergey Ioffe](http://stackoverflow.com/a/36480182)\n",
        "    '''\n",
        "    rounded = K.round(x)\n",
        "    return x + K.stop_gradient(rounded - x)\n",
        "  \n",
        "  \n",
        "def binarize(W, H=1):\n",
        "    '''The weights' binarization function, \n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    # [-H, H] -> -H or H\n",
        "    Wb = H * binary_tanh(W / H)\n",
        "    return Wb\n",
        "  \n",
        "def binary_tanh(x):\n",
        "    '''Binary hard sigmoid for training binarized neural network.\n",
        "     The neurons' activations binarization function\n",
        "     It behaves like the sign function during forward propagation\n",
        "     And like:\n",
        "        hard_tanh(x) = 2 * _hard_sigmoid(x) - 1 \n",
        "        clear gradient when |x| > 1 during back propagation\n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    return 2 * round_through(_hard_sigmoid(x)) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq7j149bKNi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Clip(constraints.Constraint):\n",
        "    def __init__(self, min_value, max_value=None):\n",
        "        self.min_value = min_value\n",
        "        self.max_value = max_value\n",
        "        if not self.max_value:\n",
        "            self.max_value = -self.min_value\n",
        "        if self.min_value > self.max_value:\n",
        "            self.min_value, self.max_value = self.max_value, self.min_value\n",
        "\n",
        "    def __call__(self, p):\n",
        "        return K.clip(p, self.min_value, self.max_value)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"min_value\": self.min_value,\n",
        "                \"max_value\": self.max_value}\n",
        "\n",
        "\n",
        "class BinaryDense(Dense):\n",
        "    ''' Binarized Dense layer\n",
        "    References: \n",
        "    \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\" [http://arxiv.org/abs/1602.02830]\n",
        "    '''\n",
        "    def __init__(self, units, H=1., kernel_lr_multiplier='Glorot', bias_lr_multiplier=None, **kwargs):\n",
        "        super(BinaryDense, self).__init__(units, **kwargs)\n",
        "        self.H = H\n",
        "        self.kernel_lr_multiplier = kernel_lr_multiplier\n",
        "        self.bias_lr_multiplier = bias_lr_multiplier\n",
        "        \n",
        "        super(BinaryDense, self).__init__(units, **kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[1]\n",
        "\n",
        "        if self.H == 'Glorot':\n",
        "            self.H = np.float32(np.sqrt(1.5 / (input_dim + self.units)))\n",
        "            #print('Glorot H: {}'.format(self.H))\n",
        "        if self.kernel_lr_multiplier == 'Glorot':\n",
        "            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5 / (input_dim + self.units)))\n",
        "            #print('Glorot learning rate multiplier: {}'.format(self.lr_multiplier))\n",
        "            \n",
        "        self.kernel_constraint = Clip(-self.H, self.H)\n",
        "        self.kernel_initializer = initializers.RandomUniform(-self.H, self.H)\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='kernel',\n",
        "                                     regularizer=self.kernel_regularizer,\n",
        "                                     constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier, self.bias_lr_multiplier]\n",
        "            self.bias = self.add_weight(shape=(self.output_dim,),\n",
        "                                     initializer=self.bias_initializer,\n",
        "                                     name='bias',\n",
        "                                     regularizer=self.bias_regularizer,\n",
        "                                     constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier]\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        binary_kernel = binarize(self.kernel, H=self.H)\n",
        "        output = K.dot(inputs, binary_kernel)\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {'H': self.H,\n",
        "                  'W_lr_multiplier': self.W_lr_multiplier,\n",
        "                  'b_lr_multiplier': self.b_lr_multiplier}\n",
        "        base_config = super(BinaryDense, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class BinaryConv2D(Conv2D):\n",
        "    '''Binarized Convolution2D layer\n",
        "    References: \n",
        "    \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\" [http://arxiv.org/abs/1602.02830]\n",
        "    '''\n",
        "    def __init__(self, filters, kernel_lr_multiplier='Glorot', \n",
        "                 bias_lr_multiplier=None, H=1., **kwargs):\n",
        "        super(BinaryConv2D, self).__init__(filters, **kwargs)\n",
        "        self.H = H\n",
        "        self.kernel_lr_multiplier = kernel_lr_multiplier\n",
        "        self.bias_lr_multiplier = bias_lr_multiplier\n",
        "        \n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1 \n",
        "        if input_shape[channel_axis] is None:\n",
        "                raise ValueError('The channel dimension of the inputs '\n",
        "                                 'should be defined. Found `None`.')\n",
        "\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "            \n",
        "        base = self.kernel_size[0] * self.kernel_size[1]\n",
        "        if self.H == 'Glorot':\n",
        "            nb_input = int(input_dim * base)\n",
        "            nb_output = int(self.filters * base)\n",
        "            self.H = np.float32(np.sqrt(1.5 / (nb_input + nb_output)))\n",
        "            #print('Glorot H: {}'.format(self.H))\n",
        "            \n",
        "        if self.kernel_lr_multiplier == 'Glorot':\n",
        "            nb_input = int(input_dim * base)\n",
        "            nb_output = int(self.filters * base)\n",
        "            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5/ (nb_input + nb_output)))\n",
        "            #print('Glorot learning rate multiplier: {}'.format(self.lr_multiplier))\n",
        "\n",
        "        self.kernel_constraint = Clip(-self.H, self.H)\n",
        "        self.kernel_initializer = initializers.RandomUniform(-self.H, self.H)\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='kernel',\n",
        "                                 regularizer=self.kernel_regularizer,\n",
        "                                 constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier, self.bias_lr_multiplier]\n",
        "            self.bias = self.add_weight((self.output_dim,),\n",
        "                                     initializer=self.bias_initializers,\n",
        "                                     name='bias',\n",
        "                                     regularizer=self.bias_regularizer,\n",
        "                                     constraint=self.bias_constraint)\n",
        "\n",
        "        else:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier]\n",
        "            self.bias = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        binary_kernel = binarize(self.kernel, H=self.H) \n",
        "        outputs = K.conv2d(\n",
        "            inputs,\n",
        "            binary_kernel,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {'H': self.H,\n",
        "                  'kernel_lr_multiplier': self.kernel_lr_multiplier,\n",
        "                  'bias_lr_multiplier': self.bias_lr_multiplier}\n",
        "        base_config = super(BinaryConv2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVINO38NJwhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XnorDense(BinaryDense):\n",
        "    '''XNOR Dense layer\n",
        "    References: \n",
        "    - [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](http://arxiv.org/abs/1603.05279)\n",
        "    '''\n",
        "    def call(self, inputs, mask=None):\n",
        "        inputs_a, inputs_b = xnorize(inputs, 1., axis=1, keepdims=True) # (nb_sample, 1)\n",
        "        kernel_a, kernel_b = xnorize(self.kernel, self.H, axis=0, keepdims=True) # (1, units)\n",
        "        output = K.dot(inputs_b, kernel_b) * kernel_a * inputs_a\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class XnorConv2D(BinaryConv2D):\n",
        "    '''XNOR Conv2D layer\n",
        "    References: \n",
        "    - [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](http://arxiv.org/abs/1603.05279)\n",
        "    '''\n",
        "    def call(self, inputs):\n",
        "        _, kernel_b = xnorize(self.kernel, self.H)\n",
        "        _, inputs_b = xnorize(inputs)\n",
        "        outputs = K.conv2d(inputs_b, kernel_b, strides=self.strides,\n",
        "                           padding=self.padding,\n",
        "                           data_format=self.data_format,\n",
        "                           dilation_rate=self.dilation_rate)\n",
        "\n",
        "        # calculate Wa and xa\n",
        "        \n",
        "        # kernel_a\n",
        "        mask = K.reshape(self.kernel, (-1, self.filters)) # self.nb_row * self.nb_col * channels, filters \n",
        "        kernel_a = K.stop_gradient(K.mean(K.abs(mask), axis=0)) # filters\n",
        "        \n",
        "        # inputs_a\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1 \n",
        "        mask = K.mean(K.abs(inputs), axis=channel_axis, keepdims=True) \n",
        "        ones = K.ones(self.kernel_size + (1, 1))\n",
        "        inputs_a = K.conv2d(mask, ones, strides=self.strides,\n",
        "                      padding=self.padding,\n",
        "                      data_format=self.data_format,\n",
        "                      dilation_rate=self.dilation_rate) # nb_sample, 1, new_nb_row, new_nb_col\n",
        "        if self.data_format == 'channels_first':\n",
        "            outputs = outputs * K.stop_gradient(inputs_a) * K.expand_dims(K.expand_dims(K.expand_dims(kernel_a, 0), -1), -1)\n",
        "        else:\n",
        "            outputs = outputs * K.stop_gradient(inputs_a) * K.expand_dims(K.expand_dims(K.expand_dims(kernel_a, 0), 0), 0)\n",
        "                                \n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Eo3ZwLKlaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 20\n",
        "classes = 10\n",
        "\n",
        "H = 'Glorot'\n",
        "kernel_lr_multiplier = 'Glorot'\n",
        "\n",
        "# network\n",
        "num_unit = 512\n",
        "num_hidden = 3\n",
        "use_bias = False\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 1e-3\n",
        "lr_end = 1e-4\n",
        "lr_decay = (lr_end / lr_start)**(1. / epochs)\n",
        "\n",
        "# BN\n",
        "epsilon = 1e-6\n",
        "momentum = 0.9\n",
        "\n",
        "# dropout\n",
        "drop_in = 0 #0.2\n",
        "drop_hidden = 0# 0.5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOgIIMuTKoNZ",
        "colab_type": "code",
        "outputId": "66fc4f39-c308-4c4d-ab01-cb19ee672c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, classes) * 2 - 1 # -1 or 1 for hinge loss\n",
        "Y_test = np_utils.to_categorical(y_test, classes) * 2 - 1\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AhIIQA7Kp7T",
        "colab_type": "code",
        "outputId": "1ebc13a8-4eb8-4135-d72b-271eab1651ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dropout(0, input_shape=(784,), name='drop0'))\n",
        "\n",
        "for i in range(3):\n",
        "    model.add(XnorDense(512, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias))\n",
        "    model.add(BatchNormalization(epsilon=epsilon, momentum=momentum))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "model.add(XnorDense(10, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias,name='dense'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "opt = Adam(lr=lr_start) \n",
        "model.compile(loss='squared_hinge', optimizer=opt, metrics=['acc'])\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lambda e: lr_start * lr_decay ** e)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "drop0 (Dropout)              (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "xnor_dense_2 (XnorDense)     (None, 512)               401408    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "xnor_dense_4 (XnorDense)     (None, 512)               262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "xnor_dense_6 (XnorDense)     (None, 512)               262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (XnorDense)            (None, 10)                5120      \n",
            "_________________________________________________________________\n",
            "bn (BatchNormalization)      (None, 10)                40        \n",
            "=================================================================\n",
            "Total params: 937,000\n",
            "Trainable params: 933,908\n",
            "Non-trainable params: 3,092\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvmY2NxEKt0L",
        "colab_type": "code",
        "outputId": "c33934d0-2961-44bc-dd9c-988d2c29945c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, Y_train,batch_size=batch_size, epochs=75,verbose=1, validation_data=(X_test, Y_test),callbacks=[lr_scheduler])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/75\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.4722 - acc: 0.7722 - val_loss: 0.2122 - val_acc: 0.8121\n",
            "Epoch 2/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.1509 - acc: 0.8290 - val_loss: 0.1151 - val_acc: 0.8284\n",
            "Epoch 3/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.1074 - acc: 0.8413 - val_loss: 0.1025 - val_acc: 0.8387\n",
            "Epoch 4/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0992 - acc: 0.8476 - val_loss: 0.0995 - val_acc: 0.8391\n",
            "Epoch 5/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0930 - acc: 0.8542 - val_loss: 0.0956 - val_acc: 0.8420\n",
            "Epoch 6/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0895 - acc: 0.8587 - val_loss: 0.0953 - val_acc: 0.8463\n",
            "Epoch 7/75\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0873 - acc: 0.8611 - val_loss: 0.0916 - val_acc: 0.8501\n",
            "Epoch 8/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0839 - acc: 0.8668 - val_loss: 0.0918 - val_acc: 0.8514\n",
            "Epoch 9/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0821 - acc: 0.8704 - val_loss: 0.0918 - val_acc: 0.8496\n",
            "Epoch 10/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0799 - acc: 0.8737 - val_loss: 0.0864 - val_acc: 0.8587\n",
            "Epoch 11/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0766 - acc: 0.8783 - val_loss: 0.0870 - val_acc: 0.8596\n",
            "Epoch 12/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0751 - acc: 0.8807 - val_loss: 0.0877 - val_acc: 0.8540\n",
            "Epoch 13/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0735 - acc: 0.8840 - val_loss: 0.0840 - val_acc: 0.8584\n",
            "Epoch 14/75\n",
            "60000/60000 [==============================] - 7s 108us/step - loss: 0.0714 - acc: 0.8875 - val_loss: 0.0852 - val_acc: 0.8613\n",
            "Epoch 15/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0696 - acc: 0.8884 - val_loss: 0.0832 - val_acc: 0.8618\n",
            "Epoch 16/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0678 - acc: 0.8932 - val_loss: 0.0830 - val_acc: 0.8622\n",
            "Epoch 17/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0668 - acc: 0.8952 - val_loss: 0.0827 - val_acc: 0.8647\n",
            "Epoch 18/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0653 - acc: 0.8987 - val_loss: 0.0813 - val_acc: 0.8639\n",
            "Epoch 19/75\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0639 - acc: 0.9009 - val_loss: 0.0827 - val_acc: 0.8674\n",
            "Epoch 20/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0630 - acc: 0.9016 - val_loss: 0.0822 - val_acc: 0.8672\n",
            "Epoch 21/75\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0619 - acc: 0.9039 - val_loss: 0.0825 - val_acc: 0.8664\n",
            "Epoch 22/75\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0612 - acc: 0.9061 - val_loss: 0.0841 - val_acc: 0.8633\n",
            "Epoch 23/75\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0600 - acc: 0.9064 - val_loss: 0.0806 - val_acc: 0.8669\n",
            "Epoch 24/75\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0592 - acc: 0.9092 - val_loss: 0.0831 - val_acc: 0.8649\n",
            "Epoch 25/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0587 - acc: 0.9092 - val_loss: 0.0804 - val_acc: 0.8681\n",
            "Epoch 26/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0575 - acc: 0.9108 - val_loss: 0.0826 - val_acc: 0.8668\n",
            "Epoch 27/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0572 - acc: 0.9124 - val_loss: 0.0811 - val_acc: 0.8689\n",
            "Epoch 28/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0568 - acc: 0.9134 - val_loss: 0.0815 - val_acc: 0.8684\n",
            "Epoch 29/75\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0561 - acc: 0.9145 - val_loss: 0.0825 - val_acc: 0.8665\n",
            "Epoch 30/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0554 - acc: 0.9166 - val_loss: 0.0808 - val_acc: 0.8703\n",
            "Epoch 31/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0550 - acc: 0.9155 - val_loss: 0.0837 - val_acc: 0.8652\n",
            "Epoch 32/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0546 - acc: 0.9176 - val_loss: 0.0814 - val_acc: 0.8685\n",
            "Epoch 33/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0543 - acc: 0.9164 - val_loss: 0.0830 - val_acc: 0.8705\n",
            "Epoch 34/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0537 - acc: 0.9187 - val_loss: 0.0818 - val_acc: 0.8663\n",
            "Epoch 35/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0533 - acc: 0.9186 - val_loss: 0.0829 - val_acc: 0.8672\n",
            "Epoch 36/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0530 - acc: 0.9207 - val_loss: 0.0811 - val_acc: 0.8674\n",
            "Epoch 37/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0524 - acc: 0.9208 - val_loss: 0.0822 - val_acc: 0.8668\n",
            "Epoch 38/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0527 - acc: 0.9203 - val_loss: 0.0815 - val_acc: 0.8686\n",
            "Epoch 39/75\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0522 - acc: 0.9211 - val_loss: 0.0825 - val_acc: 0.8686\n",
            "Epoch 40/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0516 - acc: 0.9223 - val_loss: 0.0820 - val_acc: 0.8684\n",
            "Epoch 41/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0514 - acc: 0.9217 - val_loss: 0.0820 - val_acc: 0.8709\n",
            "Epoch 42/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0513 - acc: 0.9230 - val_loss: 0.0814 - val_acc: 0.8702\n",
            "Epoch 43/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0518 - acc: 0.9223 - val_loss: 0.0818 - val_acc: 0.8727\n",
            "Epoch 44/75\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0507 - acc: 0.9243 - val_loss: 0.0826 - val_acc: 0.8708\n",
            "Epoch 45/75\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.0512 - acc: 0.9236 - val_loss: 0.0824 - val_acc: 0.8690\n",
            "Epoch 46/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0514 - acc: 0.9229 - val_loss: 0.0820 - val_acc: 0.8695\n",
            "Epoch 47/75\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0513 - acc: 0.9235 - val_loss: 0.0810 - val_acc: 0.8711\n",
            "Epoch 48/75\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0508 - acc: 0.9243 - val_loss: 0.0832 - val_acc: 0.8685\n",
            "Epoch 49/75\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0505 - acc: 0.9238 - val_loss: 0.0829 - val_acc: 0.8676\n",
            "Epoch 50/75\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0502 - acc: 0.9249 - val_loss: 0.0821 - val_acc: 0.8694\n",
            "Epoch 51/75\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0506 - acc: 0.9239 - val_loss: 0.0815 - val_acc: 0.8713\n",
            "Epoch 52/75\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0505 - acc: 0.9240 - val_loss: 0.0806 - val_acc: 0.8714\n",
            "Epoch 53/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0500 - acc: 0.9251 - val_loss: 0.0814 - val_acc: 0.8694\n",
            "Epoch 54/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0500 - acc: 0.9244 - val_loss: 0.0817 - val_acc: 0.8700\n",
            "Epoch 55/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0502 - acc: 0.9249 - val_loss: 0.0821 - val_acc: 0.8700\n",
            "Epoch 56/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0501 - acc: 0.9251 - val_loss: 0.0834 - val_acc: 0.8684\n",
            "Epoch 57/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0500 - acc: 0.9250 - val_loss: 0.0816 - val_acc: 0.8692\n",
            "Epoch 58/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0498 - acc: 0.9252 - val_loss: 0.0820 - val_acc: 0.8698\n",
            "Epoch 59/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0493 - acc: 0.9260 - val_loss: 0.0818 - val_acc: 0.8704\n",
            "Epoch 60/75\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0496 - acc: 0.9257 - val_loss: 0.0821 - val_acc: 0.8717\n",
            "Epoch 61/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0495 - acc: 0.9256 - val_loss: 0.0809 - val_acc: 0.8700\n",
            "Epoch 62/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0500 - acc: 0.9251 - val_loss: 0.0809 - val_acc: 0.8703\n",
            "Epoch 63/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0497 - acc: 0.9259 - val_loss: 0.0818 - val_acc: 0.8710\n",
            "Epoch 64/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0493 - acc: 0.9272 - val_loss: 0.0812 - val_acc: 0.8709\n",
            "Epoch 65/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0499 - acc: 0.9251 - val_loss: 0.0818 - val_acc: 0.8708\n",
            "Epoch 66/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0500 - acc: 0.9262 - val_loss: 0.0821 - val_acc: 0.8697\n",
            "Epoch 67/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0497 - acc: 0.9260 - val_loss: 0.0814 - val_acc: 0.8717\n",
            "Epoch 68/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0494 - acc: 0.9266 - val_loss: 0.0823 - val_acc: 0.8703\n",
            "Epoch 69/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0493 - acc: 0.9266 - val_loss: 0.0820 - val_acc: 0.8694\n",
            "Epoch 70/75\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0495 - acc: 0.9265 - val_loss: 0.0808 - val_acc: 0.8717\n",
            "Epoch 71/75\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0491 - acc: 0.9275 - val_loss: 0.0834 - val_acc: 0.8692\n",
            "Epoch 72/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0493 - acc: 0.9262 - val_loss: 0.0825 - val_acc: 0.8690\n",
            "Epoch 73/75\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0492 - acc: 0.9261 - val_loss: 0.0828 - val_acc: 0.8667\n",
            "Epoch 74/75\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.0497 - acc: 0.9258 - val_loss: 0.0817 - val_acc: 0.8703\n",
            "Epoch 75/75\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0492 - acc: 0.9267 - val_loss: 0.0824 - val_acc: 0.8668\n",
            "Test score: 0.08236757522821427\n",
            "Test accuracy: 0.8668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7RocT0UoovI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
        "\n",
        "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
        "    confidence1 = []\n",
        "    confidence2 = []\n",
        "    acc1 = 0\n",
        "    acc2 = 0\n",
        "    for num in range(confidence_mtx1.shape[0]):\n",
        "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
        "        if np.argmax(confidence_mtx1[num,:]) == np.argmax(label_vec1[num]):\n",
        "            acc1 += 1\n",
        "\n",
        "    for num in range(confidence_mtx2.shape[0]):\n",
        "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
        "        if np.argmax(confidence_mtx2[num,:]) == np.argmax(label_vec2[num]):\n",
        "            acc2 += 1\n",
        "    confidence1 = np.array(confidence1)\n",
        "    confidence2 = np.array(confidence2)\n",
        "\n",
        "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
        "\n",
        "\n",
        "    #sort_confidence = np.sort(confidence1)\n",
        "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
        "    max_accuracy = 0.5\n",
        "    best_precision = 0.5\n",
        "    best_recall = 0.5\n",
        "    for num in range(len(sort_confidence)):\n",
        "        delta = sort_confidence[num]\n",
        "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
        "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
        "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
        "        if accuracy_now > max_accuracy:\n",
        "            max_accuracy = accuracy_now\n",
        "            best_precision = ratio1/(ratio1+ratio2)\n",
        "            best_recall = ratio1\n",
        "    print('maximum inference accuracy is:', max_accuracy)\n",
        "    print('precision is:', best_precision)\n",
        "    print('recall is:', best_recall)\n",
        "    return max_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXbmH79zopZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c2aebc3d-1c61-4cd2-8a73-98491cd693dc"
      },
      "source": [
        "output_train=model.predict(X_train)\n",
        "output_test=model.predict(X_test)\n",
        "Y_test = Y_test.astype('int')\n",
        "Y_train = Y_train.astype('int')\n",
        "\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, Y_train, Y_test)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy for training and test- (0.935, 0.8668)\n",
            "maximum inference accuracy is: 0.5174333333333333\n",
            "precision is: 0.5053590458234282\n",
            "recall is: 1.6439666666666666\n",
            "Maximum Accuracy: 0.5174333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}