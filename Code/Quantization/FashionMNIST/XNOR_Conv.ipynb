{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XNOR_Conv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlDrOwObKYlU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "e5519035-1a6e-4ed4-8d0e-7459d2c3f2c0"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization, MaxPooling2D\n",
        "from keras.layers import InputSpec, Layer, Dense, Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "from keras import constraints\n",
        "from keras import initializers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5pKtQLPJ_Gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xnorize(W, H=1., axis=None, keepdims=False):\n",
        "    Wb = binarize(W, H)\n",
        "    Wa = _mean_abs(W, axis, keepdims)\n",
        "    \n",
        "    return Wa, Wb\n",
        "  \n",
        "  \n",
        "def _mean_abs(x, axis=None, keepdims=False):\n",
        "    return K.stop_gradient(K.mean(K.abs(x), axis=axis, keepdims=keepdims))\n",
        "  \n",
        "  \n",
        "def _hard_sigmoid(x):\n",
        "    '''Hard sigmoid different from the more conventional form (see definition of K.hard_sigmoid).\n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    x = (0.5 * x) + 0.5\n",
        "    return K.clip(x, 0, 1)\n",
        "\n",
        "def round_through(x):\n",
        "    '''Element-wise rounding to the closest integer with full gradient propagation.\n",
        "    A trick from [Sergey Ioffe](http://stackoverflow.com/a/36480182)\n",
        "    '''\n",
        "    rounded = K.round(x)\n",
        "    return x + K.stop_gradient(rounded - x)\n",
        "  \n",
        "  \n",
        "def binarize(W, H=1):\n",
        "    '''The weights' binarization function, \n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    # [-H, H] -> -H or H\n",
        "    Wb = H * binary_tanh(W / H)\n",
        "    return Wb\n",
        "  \n",
        "def binary_tanh(x):\n",
        "    '''Binary hard sigmoid for training binarized neural network.\n",
        "     The neurons' activations binarization function\n",
        "     It behaves like the sign function during forward propagation\n",
        "     And like:\n",
        "        hard_tanh(x) = 2 * _hard_sigmoid(x) - 1 \n",
        "        clear gradient when |x| > 1 during back propagation\n",
        "    # Reference:\n",
        "    - [BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, Courbariaux et al. 2016](http://arxiv.org/abs/1602.02830}\n",
        "    '''\n",
        "    return 2 * round_through(_hard_sigmoid(x)) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq7j149bKNi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Clip(constraints.Constraint):\n",
        "    def __init__(self, min_value, max_value=None):\n",
        "        self.min_value = min_value\n",
        "        self.max_value = max_value\n",
        "        if not self.max_value:\n",
        "            self.max_value = -self.min_value\n",
        "        if self.min_value > self.max_value:\n",
        "            self.min_value, self.max_value = self.max_value, self.min_value\n",
        "\n",
        "    def __call__(self, p):\n",
        "        return K.clip(p, self.min_value, self.max_value)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"min_value\": self.min_value,\n",
        "                \"max_value\": self.max_value}\n",
        "\n",
        "\n",
        "class BinaryDense(Dense):\n",
        "    ''' Binarized Dense layer\n",
        "    References: \n",
        "    \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\" [http://arxiv.org/abs/1602.02830]\n",
        "    '''\n",
        "    def __init__(self, units, H=1., kernel_lr_multiplier='Glorot', bias_lr_multiplier=None, **kwargs):\n",
        "        super(BinaryDense, self).__init__(units, **kwargs)\n",
        "        self.H = H\n",
        "        self.kernel_lr_multiplier = kernel_lr_multiplier\n",
        "        self.bias_lr_multiplier = bias_lr_multiplier\n",
        "        \n",
        "        super(BinaryDense, self).__init__(units, **kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[1]\n",
        "\n",
        "        if self.H == 'Glorot':\n",
        "            self.H = np.float32(np.sqrt(1.5 / (input_dim + self.units)))\n",
        "            #print('Glorot H: {}'.format(self.H))\n",
        "        if self.kernel_lr_multiplier == 'Glorot':\n",
        "            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5 / (input_dim + self.units)))\n",
        "            #print('Glorot learning rate multiplier: {}'.format(self.lr_multiplier))\n",
        "            \n",
        "        self.kernel_constraint = Clip(-self.H, self.H)\n",
        "        self.kernel_initializer = initializers.RandomUniform(-self.H, self.H)\n",
        "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='kernel',\n",
        "                                     regularizer=self.kernel_regularizer,\n",
        "                                     constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier, self.bias_lr_multiplier]\n",
        "            self.bias = self.add_weight(shape=(self.output_dim,),\n",
        "                                     initializer=self.bias_initializer,\n",
        "                                     name='bias',\n",
        "                                     regularizer=self.bias_regularizer,\n",
        "                                     constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier]\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        binary_kernel = binarize(self.kernel, H=self.H)\n",
        "        output = K.dot(inputs, binary_kernel)\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {'H': self.H,\n",
        "                  'W_lr_multiplier': self.W_lr_multiplier,\n",
        "                  'b_lr_multiplier': self.b_lr_multiplier}\n",
        "        base_config = super(BinaryDense, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class BinaryConv2D(Conv2D):\n",
        "    '''Binarized Convolution2D layer\n",
        "    References: \n",
        "    \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\" [http://arxiv.org/abs/1602.02830]\n",
        "    '''\n",
        "    def __init__(self, filters, kernel_lr_multiplier='Glorot', \n",
        "                 bias_lr_multiplier=None, H=1., **kwargs):\n",
        "        super(BinaryConv2D, self).__init__(filters, **kwargs)\n",
        "        self.H = H\n",
        "        self.kernel_lr_multiplier = kernel_lr_multiplier\n",
        "        self.bias_lr_multiplier = bias_lr_multiplier\n",
        "        \n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1 \n",
        "        if input_shape[channel_axis] is None:\n",
        "                raise ValueError('The channel dimension of the inputs '\n",
        "                                 'should be defined. Found `None`.')\n",
        "\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "            \n",
        "        base = self.kernel_size[0] * self.kernel_size[1]\n",
        "        if self.H == 'Glorot':\n",
        "            nb_input = int(input_dim * base)\n",
        "            nb_output = int(self.filters * base)\n",
        "            self.H = np.float32(np.sqrt(1.5 / (nb_input + nb_output)))\n",
        "            #print('Glorot H: {}'.format(self.H))\n",
        "            \n",
        "        if self.kernel_lr_multiplier == 'Glorot':\n",
        "            nb_input = int(input_dim * base)\n",
        "            nb_output = int(self.filters * base)\n",
        "            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5/ (nb_input + nb_output)))\n",
        "            #print('Glorot learning rate multiplier: {}'.format(self.lr_multiplier))\n",
        "\n",
        "        self.kernel_constraint = Clip(-self.H, self.H)\n",
        "        self.kernel_initializer = initializers.RandomUniform(-self.H, self.H)\n",
        "        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='kernel',\n",
        "                                 regularizer=self.kernel_regularizer,\n",
        "                                 constraint=self.kernel_constraint)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier, self.bias_lr_multiplier]\n",
        "            self.bias = self.add_weight((self.output_dim,),\n",
        "                                     initializer=self.bias_initializers,\n",
        "                                     name='bias',\n",
        "                                     regularizer=self.bias_regularizer,\n",
        "                                     constraint=self.bias_constraint)\n",
        "\n",
        "        else:\n",
        "            self.lr_multipliers = [self.kernel_lr_multiplier]\n",
        "            self.bias = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        binary_kernel = binarize(self.kernel, H=self.H) \n",
        "        outputs = K.conv2d(\n",
        "            inputs,\n",
        "            binary_kernel,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding,\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {'H': self.H,\n",
        "                  'kernel_lr_multiplier': self.kernel_lr_multiplier,\n",
        "                  'bias_lr_multiplier': self.bias_lr_multiplier}\n",
        "        base_config = super(BinaryConv2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVINO38NJwhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XnorDense(BinaryDense):\n",
        "    '''XNOR Dense layer\n",
        "    References: \n",
        "    - [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](http://arxiv.org/abs/1603.05279)\n",
        "    '''\n",
        "    def call(self, inputs, mask=None):\n",
        "        inputs_a, inputs_b = xnorize(inputs, 1., axis=1, keepdims=True) # (nb_sample, 1)\n",
        "        kernel_a, kernel_b = xnorize(self.kernel, self.H, axis=0, keepdims=True) # (1, units)\n",
        "        output = K.dot(inputs_b, kernel_b) * kernel_a * inputs_a\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class XnorConv2D(BinaryConv2D):\n",
        "    '''XNOR Conv2D layer\n",
        "    References: \n",
        "    - [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](http://arxiv.org/abs/1603.05279)\n",
        "    '''\n",
        "    def call(self, inputs):\n",
        "        _, kernel_b = xnorize(self.kernel, self.H)\n",
        "        _, inputs_b = xnorize(inputs)\n",
        "        outputs = K.conv2d(inputs_b, kernel_b, strides=self.strides,\n",
        "                           padding=self.padding,\n",
        "                           data_format=self.data_format,\n",
        "                           dilation_rate=self.dilation_rate)\n",
        "\n",
        "        # calculate Wa and xa\n",
        "        \n",
        "        # kernel_a\n",
        "        mask = K.reshape(self.kernel, (-1, self.filters)) # self.nb_row * self.nb_col * channels, filters \n",
        "        kernel_a = K.stop_gradient(K.mean(K.abs(mask), axis=0)) # filters\n",
        "        \n",
        "        # inputs_a\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1 \n",
        "        mask = K.mean(K.abs(inputs), axis=channel_axis, keepdims=True) \n",
        "        ones = K.ones(self.kernel_size + (1, 1))\n",
        "        inputs_a = K.conv2d(mask, ones, strides=self.strides,\n",
        "                      padding=self.padding,\n",
        "                      data_format=self.data_format,\n",
        "                      dilation_rate=self.dilation_rate) # nb_sample, 1, new_nb_row, new_nb_col\n",
        "        if self.data_format == 'channels_first':\n",
        "            outputs = outputs * K.stop_gradient(inputs_a) * K.expand_dims(K.expand_dims(K.expand_dims(kernel_a, 0), -1), -1)\n",
        "        else:\n",
        "            outputs = outputs * K.stop_gradient(inputs_a) * K.expand_dims(K.expand_dims(K.expand_dims(kernel_a, 0), 0), 0)\n",
        "                                \n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Eo3ZwLKlaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H = 1.\n",
        "kernel_lr_multiplier = 'Glorot'\n",
        "\n",
        "# nn\n",
        "batch_size = 50\n",
        "epochs = 20 \n",
        "nb_channel = 1\n",
        "img_rows = 28 \n",
        "img_cols = 28 \n",
        "nb_filters = 32 \n",
        "nb_conv = 3\n",
        "nb_pool = 2\n",
        "nb_hid = 128\n",
        "nb_classes = 10\n",
        "use_bias = False\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 1e-3\n",
        "lr_end = 1e-4\n",
        "lr_decay = (lr_end / lr_start)**(1. / epochs)\n",
        "\n",
        "# BN\n",
        "epsilon = 1e-6\n",
        "momentum = 0.9\n",
        "\n",
        "# dropout\n",
        "p1 = 0.25\n",
        "p2 = 0.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOgIIMuTKoNZ",
        "colab_type": "code",
        "outputId": "a3142a78-2a64-4b61-f74f-5983116163a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 1, 28, 28)\n",
        "X_test = X_test.reshape(10000, 1, 28, 28)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes) * 2 - 1 # -1 or 1 for hinge loss\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes) * 2 - 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AhIIQA7Kp7T",
        "colab_type": "code",
        "outputId": "d22b850f-e5c1-4f72-9ad9-de2c3756e2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "# conv1\n",
        "model.add(XnorConv2D(32, kernel_size=(3, 3), input_shape=(nb_channel, img_rows, img_cols),H=H, kernel_lr_multiplier=kernel_lr_multiplier, padding='same', use_bias=use_bias, name='conv1'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn1'))\n",
        "model.add(Activation('relu', name='act1'))\n",
        "# conv2\n",
        "model.add(XnorConv2D(64, kernel_size=(3, 3), H=H, kernel_lr_multiplier=kernel_lr_multiplier, padding='same', use_bias=use_bias, name='conv2'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn2'))\n",
        "model.add(Activation('relu', name='act2'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), name='pool4'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn4'))\n",
        "model.add(Activation('relu', name='act4'))\n",
        "model.add(Flatten())\n",
        "# dense1\n",
        "model.add(XnorDense(128, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias, name='dense5'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn5'))\n",
        "model.add(Activation('relu', name='act5'))\n",
        "# dense2\n",
        "model.add(XnorDense(nb_classes, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias, name='dense6'))\n",
        "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn6'))\n",
        "\n",
        "opt = Adam(lr=lr_start) \n",
        "model.compile(loss='squared_hinge', optimizer=opt, metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lambda e: lr_start * lr_decay ** e)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (XnorConv2D)           (None, 32, 28, 28)        288       \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 32, 28, 28)        128       \n",
            "_________________________________________________________________\n",
            "act1 (Activation)            (None, 32, 28, 28)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (XnorConv2D)           (None, 64, 28, 28)        18432     \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 64, 28, 28)        256       \n",
            "_________________________________________________________________\n",
            "act2 (Activation)            (None, 64, 28, 28)        0         \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 64, 14, 14)        0         \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 64, 14, 14)        256       \n",
            "_________________________________________________________________\n",
            "act4 (Activation)            (None, 64, 14, 14)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense5 (XnorDense)           (None, 128)               1605632   \n",
            "_________________________________________________________________\n",
            "bn5 (BatchNormalization)     (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "act5 (Activation)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense6 (XnorDense)           (None, 10)                1280      \n",
            "_________________________________________________________________\n",
            "bn6 (BatchNormalization)     (None, 10)                40        \n",
            "=================================================================\n",
            "Total params: 1,626,824\n",
            "Trainable params: 1,626,228\n",
            "Non-trainable params: 596\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvmY2NxEKt0L",
        "colab_type": "code",
        "outputId": "40e60ec8-ddd1-482e-88d9-951cdb998605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, Y_train,batch_size=batch_size, epochs=75,verbose=1, validation_data=(X_test, Y_test),callbacks=[lr_scheduler])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/75\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.4544 - acc: 0.7116 - val_loss: 0.1555 - val_acc: 0.7781\n",
            "Epoch 2/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.1461 - acc: 0.7820 - val_loss: 0.1259 - val_acc: 0.8102\n",
            "Epoch 3/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.1256 - acc: 0.8087 - val_loss: 0.1143 - val_acc: 0.8277\n",
            "Epoch 4/75\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.1154 - acc: 0.8258 - val_loss: 0.1033 - val_acc: 0.8402\n",
            "Epoch 5/75\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.1096 - acc: 0.8326 - val_loss: 0.1051 - val_acc: 0.8347\n",
            "Epoch 6/75\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.1053 - acc: 0.8397 - val_loss: 0.1053 - val_acc: 0.8297\n",
            "Epoch 7/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.1049 - acc: 0.8403 - val_loss: 0.0991 - val_acc: 0.8437\n",
            "Epoch 8/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.1014 - acc: 0.8455 - val_loss: 0.0968 - val_acc: 0.8488\n",
            "Epoch 9/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.1012 - acc: 0.8457 - val_loss: 0.0925 - val_acc: 0.8532\n",
            "Epoch 10/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.1005 - acc: 0.8458 - val_loss: 0.0933 - val_acc: 0.8496\n",
            "Epoch 11/75\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.0980 - acc: 0.8497 - val_loss: 0.0923 - val_acc: 0.8501\n",
            "Epoch 12/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0979 - acc: 0.8501 - val_loss: 0.0929 - val_acc: 0.8512\n",
            "Epoch 13/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0977 - acc: 0.8511 - val_loss: 0.0945 - val_acc: 0.8483\n",
            "Epoch 14/75\n",
            "60000/60000 [==============================] - 18s 308us/step - loss: 0.0972 - acc: 0.8527 - val_loss: 0.0934 - val_acc: 0.8513\n",
            "Epoch 15/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0966 - acc: 0.8523 - val_loss: 0.0934 - val_acc: 0.8519\n",
            "Epoch 16/75\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.0957 - acc: 0.8546 - val_loss: 0.0917 - val_acc: 0.8562\n",
            "Epoch 17/75\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.0959 - acc: 0.8545 - val_loss: 0.0910 - val_acc: 0.8523\n",
            "Epoch 18/75\n",
            "60000/60000 [==============================] - 18s 307us/step - loss: 0.0958 - acc: 0.8545 - val_loss: 0.0962 - val_acc: 0.8501\n",
            "Epoch 19/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0957 - acc: 0.8543 - val_loss: 0.0919 - val_acc: 0.8565\n",
            "Epoch 20/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0956 - acc: 0.8549 - val_loss: 0.0900 - val_acc: 0.8547\n",
            "Epoch 21/75\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.0946 - acc: 0.8570 - val_loss: 0.0928 - val_acc: 0.8545\n",
            "Epoch 22/75\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.0953 - acc: 0.8556 - val_loss: 0.0902 - val_acc: 0.8572\n",
            "Epoch 23/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0942 - acc: 0.8558 - val_loss: 0.0881 - val_acc: 0.8601\n",
            "Epoch 24/75\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.0943 - acc: 0.8573 - val_loss: 0.0920 - val_acc: 0.8566\n",
            "Epoch 25/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0948 - acc: 0.8561 - val_loss: 0.0895 - val_acc: 0.8549\n",
            "Epoch 26/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0945 - acc: 0.8578 - val_loss: 0.0889 - val_acc: 0.8578\n",
            "Epoch 27/75\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.0948 - acc: 0.8567 - val_loss: 0.0931 - val_acc: 0.8482\n",
            "Epoch 28/75\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.0941 - acc: 0.8570 - val_loss: 0.0911 - val_acc: 0.8549\n",
            "Epoch 29/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0943 - acc: 0.8571 - val_loss: 0.0949 - val_acc: 0.8463\n",
            "Epoch 30/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0944 - acc: 0.8561 - val_loss: 0.0913 - val_acc: 0.8555\n",
            "Epoch 31/75\n",
            "60000/60000 [==============================] - 18s 308us/step - loss: 0.0938 - acc: 0.8581 - val_loss: 0.0891 - val_acc: 0.8568\n",
            "Epoch 32/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0949 - acc: 0.8558 - val_loss: 0.0890 - val_acc: 0.8591\n",
            "Epoch 33/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0938 - acc: 0.8578 - val_loss: 0.0889 - val_acc: 0.8594\n",
            "Epoch 34/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0932 - acc: 0.8582 - val_loss: 0.0898 - val_acc: 0.8574\n",
            "Epoch 35/75\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.0940 - acc: 0.8573 - val_loss: 0.0898 - val_acc: 0.8590\n",
            "Epoch 36/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0946 - acc: 0.8571 - val_loss: 0.0916 - val_acc: 0.8586\n",
            "Epoch 37/75\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.0942 - acc: 0.8552 - val_loss: 0.0902 - val_acc: 0.8572\n",
            "Epoch 38/75\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0942 - acc: 0.8578 - val_loss: 0.0918 - val_acc: 0.8558\n",
            "Epoch 39/75\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.0946 - acc: 0.8552 - val_loss: 0.0903 - val_acc: 0.8586\n",
            "Epoch 40/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0931 - acc: 0.8581 - val_loss: 0.0903 - val_acc: 0.8565\n",
            "Epoch 41/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0938 - acc: 0.8575 - val_loss: 0.0908 - val_acc: 0.8562\n",
            "Epoch 42/75\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0944 - acc: 0.8568 - val_loss: 0.0906 - val_acc: 0.8575\n",
            "Epoch 43/75\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.0950 - acc: 0.8562 - val_loss: 0.0907 - val_acc: 0.8545\n",
            "Epoch 44/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0942 - acc: 0.8568 - val_loss: 0.0941 - val_acc: 0.8554\n",
            "Epoch 45/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0953 - acc: 0.8554 - val_loss: 0.0903 - val_acc: 0.8576\n",
            "Epoch 46/75\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0954 - acc: 0.8554 - val_loss: 0.0911 - val_acc: 0.8559\n",
            "Epoch 47/75\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.0952 - acc: 0.8572 - val_loss: 0.0892 - val_acc: 0.8578\n",
            "Epoch 48/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0942 - acc: 0.8574 - val_loss: 0.0913 - val_acc: 0.8543\n",
            "Epoch 49/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0952 - acc: 0.8560 - val_loss: 0.0929 - val_acc: 0.8562\n",
            "Epoch 50/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0941 - acc: 0.8569 - val_loss: 0.0893 - val_acc: 0.8558\n",
            "Epoch 51/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0950 - acc: 0.8557 - val_loss: 0.0906 - val_acc: 0.8541\n",
            "Epoch 52/75\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0947 - acc: 0.8561 - val_loss: 0.0905 - val_acc: 0.8562\n",
            "Epoch 53/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0944 - acc: 0.8573 - val_loss: 0.0898 - val_acc: 0.8580\n",
            "Epoch 54/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0949 - acc: 0.8561 - val_loss: 0.0909 - val_acc: 0.8552\n",
            "Epoch 55/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0950 - acc: 0.8562 - val_loss: 0.0910 - val_acc: 0.8575\n",
            "Epoch 56/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0947 - acc: 0.8559 - val_loss: 0.0905 - val_acc: 0.8542\n",
            "Epoch 57/75\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.0951 - acc: 0.8562 - val_loss: 0.0904 - val_acc: 0.8569\n",
            "Epoch 58/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0954 - acc: 0.8551 - val_loss: 0.0896 - val_acc: 0.8598\n",
            "Epoch 59/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0949 - acc: 0.8563 - val_loss: 0.0906 - val_acc: 0.8569\n",
            "Epoch 60/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0947 - acc: 0.8555 - val_loss: 0.0901 - val_acc: 0.8593\n",
            "Epoch 61/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0946 - acc: 0.8571 - val_loss: 0.0943 - val_acc: 0.8512\n",
            "Epoch 62/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0952 - acc: 0.8564 - val_loss: 0.0916 - val_acc: 0.8539\n",
            "Epoch 63/75\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.0944 - acc: 0.8578 - val_loss: 0.0912 - val_acc: 0.8538\n",
            "Epoch 64/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0950 - acc: 0.8562 - val_loss: 0.0912 - val_acc: 0.8561\n",
            "Epoch 65/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0950 - acc: 0.8564 - val_loss: 0.0945 - val_acc: 0.8558\n",
            "Epoch 66/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0947 - acc: 0.8552 - val_loss: 0.0913 - val_acc: 0.8549\n",
            "Epoch 67/75\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0946 - acc: 0.8571 - val_loss: 0.0922 - val_acc: 0.8548\n",
            "Epoch 68/75\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0956 - acc: 0.8553 - val_loss: 0.0914 - val_acc: 0.8562\n",
            "Epoch 69/75\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.0945 - acc: 0.8572 - val_loss: 0.0919 - val_acc: 0.8556\n",
            "Epoch 70/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0953 - acc: 0.8566 - val_loss: 0.0945 - val_acc: 0.8538\n",
            "Epoch 71/75\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0951 - acc: 0.8557 - val_loss: 0.0931 - val_acc: 0.8550\n",
            "Epoch 72/75\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0945 - acc: 0.8577 - val_loss: 0.0901 - val_acc: 0.8584\n",
            "Epoch 73/75\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.0944 - acc: 0.8566 - val_loss: 0.0903 - val_acc: 0.8568\n",
            "Epoch 74/75\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.0947 - acc: 0.8558 - val_loss: 0.0925 - val_acc: 0.8582\n",
            "Epoch 75/75\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0951 - acc: 0.8555 - val_loss: 0.0917 - val_acc: 0.8568\n",
            "Test score: 0.09174089158177376\n",
            "Test accuracy: 0.8568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxfRUH_IVa1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
        "\n",
        "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
        "    confidence1 = []\n",
        "    confidence2 = []\n",
        "    acc1 = 0\n",
        "    acc2 = 0\n",
        "    for num in range(confidence_mtx1.shape[0]):\n",
        "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
        "        if np.argmax(confidence_mtx1[num,:]) == np.argmax(label_vec1[num]):\n",
        "            acc1 += 1\n",
        "\n",
        "    for num in range(confidence_mtx2.shape[0]):\n",
        "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
        "        if np.argmax(confidence_mtx2[num,:]) == np.argmax(label_vec2[num]):\n",
        "            acc2 += 1\n",
        "    confidence1 = np.array(confidence1)\n",
        "    confidence2 = np.array(confidence2)\n",
        "\n",
        "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
        "\n",
        "\n",
        "    #sort_confidence = np.sort(confidence1)\n",
        "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
        "    max_accuracy = 0.5\n",
        "    best_precision = 0.5\n",
        "    best_recall = 0.5\n",
        "    for num in range(len(sort_confidence)):\n",
        "        delta = sort_confidence[num]\n",
        "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
        "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
        "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
        "        if accuracy_now > max_accuracy:\n",
        "            max_accuracy = accuracy_now\n",
        "            best_precision = ratio1/(ratio1+ratio2)\n",
        "            best_recall = ratio1\n",
        "    print('maximum inference accuracy is:', max_accuracy)\n",
        "    print('precision is:', best_precision)\n",
        "    print('recall is:', best_recall)\n",
        "    return max_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20HwDPaqVe5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8a120409-1d27-4593-db8e-f827b5c8686c"
      },
      "source": [
        "output_train=model.predict(X_train)\n",
        "output_test=model.predict(X_test)\n",
        "Y_test = Y_test.astype('int')\n",
        "Y_train = Y_train.astype('int')\n",
        "\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, Y_train, Y_test)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model accuracy for training and test- (0.87195, 0.8568)\n",
            "maximum inference accuracy is: 0.5105333333333331\n",
            "precision is: 0.5005588370530187\n",
            "recall is: 9.434866666666666\n",
            "Maximum Accuracy: 0.5105333333333331\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}