{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BinaryNet_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P8mK1gLddEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojn0VPRecnMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BinActive(torch.autograd.Function):\n",
        "    '''\n",
        "    Binarize the input activations and calculate the mean across channel dimension.\n",
        "    '''\n",
        "    def forward(self, input):\n",
        "        self.save_for_backward(input)\n",
        "        size = input.size()\n",
        "        mean = torch.mean(input.abs(), 1, keepdim=True)\n",
        "        input = input.sign()\n",
        "        return input, mean\n",
        "\n",
        "    def backward(self, grad_output, grad_output_mean):\n",
        "        input, = self.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input.ge(1)] = 0\n",
        "        grad_input[input.le(-1)] = 0\n",
        "        return grad_input\n",
        "\n",
        "class BinConv2d(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels,\n",
        "            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n",
        "        super(BinConv2d, self).__init__()\n",
        "        self.layer_type = 'BinConv2d'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dropout_ratio = 0\n",
        "        dropout=0\n",
        "        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
        "        self.bn.weight.data = self.bn.weight.data.zero_().add(1.0)\n",
        "        if dropout!=0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
        "                kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x, mean = BinActive()(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.xnor = nn.Sequential(\n",
        "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
        "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "                BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
        "                BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "                BinConv2d( 96, 192, kernel_size=5, stride=1, padding=2, dropout=0),\n",
        "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "                BinConv2d(192, 192, kernel_size=3, stride=1, padding=1, dropout=0),\n",
        "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
        "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                if hasattr(m.weight, 'data'):\n",
        "                    m.weight.data.clamp_(min=0.01)\n",
        "        x = self.xnor(x)\n",
        "        x = x.view(x.size(0), 10)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOxdhLbLdZms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import numpy\n",
        "\n",
        "class BinOp():\n",
        "    def __init__(self, model):\n",
        "        # count the number of Conv2d\n",
        "        count_Conv2d = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                count_Conv2d = count_Conv2d + 1\n",
        "\n",
        "        start_range = 1\n",
        "        end_range = count_Conv2d-2\n",
        "        self.bin_range = numpy.linspace(start_range,\n",
        "                end_range, end_range-start_range+1)\\\n",
        "                        .astype('int').tolist()\n",
        "        self.num_of_params = len(self.bin_range)\n",
        "        self.saved_params = []\n",
        "        self.target_params = []\n",
        "        self.target_modules = []\n",
        "        index = -1\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                index = index + 1\n",
        "                if index in self.bin_range:\n",
        "                    tmp = m.weight.data.clone()\n",
        "                    self.saved_params.append(tmp)\n",
        "                    self.target_modules.append(m.weight)\n",
        "\n",
        "    def binarization(self):\n",
        "        self.meancenterConvParams()\n",
        "        self.clampConvParams()\n",
        "        self.save_params()\n",
        "        self.binarizeConvParams()\n",
        "\n",
        "    def meancenterConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            s = self.target_modules[index].data.size()\n",
        "            negMean = self.target_modules[index].data.mean(1, keepdim=True).\\\n",
        "                    mul(-1).expand_as(self.target_modules[index].data)\n",
        "            self.target_modules[index].data = self.target_modules[index].data.add(negMean)\n",
        "\n",
        "    def clampConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data = \\\n",
        "                    self.target_modules[index].data.clamp(-1.0, 1.0)\n",
        "\n",
        "    def save_params(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.saved_params[index].copy_(self.target_modules[index].data)\n",
        "\n",
        "    def binarizeConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            n = self.target_modules[index].data[0].nelement()\n",
        "            s = self.target_modules[index].data.size()\n",
        "            m = self.target_modules[index].data.norm(1, 3, keepdim=True)\\\n",
        "                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n)\n",
        "            self.target_modules[index].data = \\\n",
        "                    self.target_modules[index].data.sign().mul(m.expand(s))\n",
        "\n",
        "    def restore(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data.copy_(self.saved_params[index])\n",
        "\n",
        "    def updateBinaryGradWeight(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            weight = self.target_modules[index].data\n",
        "            n = weight[0].nelement()\n",
        "            s = weight.size()\n",
        "            m = weight.norm(1, 3, keepdim=True)\\\n",
        "                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
        "            m[weight.lt(-1.0)] = 0 \n",
        "            m[weight.gt(1.0)] = 0\n",
        "            # m = m.add(1.0/n).mul(1.0-1.0/s[1]).mul(n)\n",
        "            # self.target_modules[index].grad.data = \\\n",
        "            #         self.target_modules[index].grad.data.mul(m)\n",
        "            m = m.mul(self.target_modules[index].grad.data)\n",
        "            m_add = weight.sign().mul(self.target_modules[index].grad.data)\n",
        "            m_add = m_add.sum(3, keepdim=True)\\\n",
        "                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
        "            m_add = m_add.mul(weight.sign())\n",
        "            self.target_modules[index].grad.data = m.add(m_add).mul(1.0-1.0/s[1]).mul(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em143EF9edj6",
        "colab_type": "code",
        "outputId": "65289935-d49d-4c43-8d70-9942233acef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/170498071 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:01, 87003802.61it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwrC2SJ9eOim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(trainloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            #loss = nn.NLLLoss(output,target)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if batch_idx % 100 == 0:\n",
        "                done = batch_idx * len(data)\n",
        "                percentage = 100. * batch_idx / len(trainloader)\n",
        "                print(f'Train Epoch: {epoch} [{done:5}/{len(trainloader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
        "\n",
        "        test(trainloader)\n",
        "        test(testloader)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            #test_loss += nn.NLLLoss(output, target).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(loader.dataset)\n",
        "        accuracy = 100. * correct / len(loader.dataset)\n",
        "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkoGwyFedx5Q",
        "colab_type": "code",
        "outputId": "d0cdf943-9586-4886-ff72-afd7446fdc79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model = Net()\n",
        "\n",
        "model.cuda()\n",
        "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
        "print(model)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3,weight_decay=0.0000)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define the binarization operator\n",
        "bin_op = BinOp(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataParallel(\n",
            "  (module): Net(\n",
            "    (xnor): Sequential(\n",
            "      (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "      (1): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): BinConv2d(\n",
            "        (bn): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): BinConv2d(\n",
            "        (bn): BatchNorm2d(160, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (6): BinConv2d(\n",
            "        (bn): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): BinConv2d(\n",
            "        (bn): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (8): BinConv2d(\n",
            "        (bn): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (9): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
            "      (10): BinConv2d(\n",
            "        (bn): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (11): BinConv2d(\n",
            "        (bn): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (12): BatchNorm2d(192, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
            "      (13): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (14): ReLU(inplace=True)\n",
            "      (15): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcfB-qK3diJC",
        "colab_type": "code",
        "outputId": "771e9768-62a7-4b53-8814-04ebcba33897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "train(125)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [    0/50000 (  0%)]  Loss: 2.305069\n",
            "Train Epoch: 0 [12800/50000 ( 26%)]  Loss: 1.577118\n",
            "Train Epoch: 0 [25600/50000 ( 51%)]  Loss: 1.457902\n",
            "Train Epoch: 0 [38400/50000 ( 77%)]  Loss: 1.290983\n",
            "Test set: Average loss: 0.0095, Accuracy: 28170/50000 (56.34%)\n",
            "Test set: Average loss: 0.0120, Accuracy: 5688/10000 (56.88%)\n",
            "Train Epoch: 1 [    0/50000 (  0%)]  Loss: 1.185524\n",
            "Train Epoch: 1 [12800/50000 ( 26%)]  Loss: 1.116135\n",
            "Train Epoch: 1 [25600/50000 ( 51%)]  Loss: 1.322220\n",
            "Train Epoch: 1 [38400/50000 ( 77%)]  Loss: 1.186560\n",
            "Test set: Average loss: 0.0079, Accuracy: 32241/50000 (64.48%)\n",
            "Test set: Average loss: 0.0103, Accuracy: 6399/10000 (63.99%)\n",
            "Train Epoch: 2 [    0/50000 (  0%)]  Loss: 1.078241\n",
            "Train Epoch: 2 [12800/50000 ( 26%)]  Loss: 0.916645\n",
            "Train Epoch: 2 [25600/50000 ( 51%)]  Loss: 0.973445\n",
            "Train Epoch: 2 [38400/50000 ( 77%)]  Loss: 1.017639\n",
            "Test set: Average loss: 0.0074, Accuracy: 33308/50000 (66.62%)\n",
            "Test set: Average loss: 0.0098, Accuracy: 6495/10000 (64.95%)\n",
            "Train Epoch: 3 [    0/50000 (  0%)]  Loss: 0.978020\n",
            "Train Epoch: 3 [12800/50000 ( 26%)]  Loss: 1.053841\n",
            "Train Epoch: 3 [25600/50000 ( 51%)]  Loss: 0.959938\n",
            "Train Epoch: 3 [38400/50000 ( 77%)]  Loss: 0.837347\n",
            "Test set: Average loss: 0.0072, Accuracy: 33380/50000 (66.76%)\n",
            "Test set: Average loss: 0.0099, Accuracy: 6529/10000 (65.29%)\n",
            "Train Epoch: 4 [    0/50000 (  0%)]  Loss: 0.921211\n",
            "Train Epoch: 4 [12800/50000 ( 26%)]  Loss: 0.942789\n",
            "Train Epoch: 4 [25600/50000 ( 51%)]  Loss: 1.048412\n",
            "Train Epoch: 4 [38400/50000 ( 77%)]  Loss: 0.950808\n",
            "Test set: Average loss: 0.0065, Accuracy: 35271/50000 (70.54%)\n",
            "Test set: Average loss: 0.0089, Accuracy: 6872/10000 (68.72%)\n",
            "Train Epoch: 5 [    0/50000 (  0%)]  Loss: 0.627226\n",
            "Train Epoch: 5 [12800/50000 ( 26%)]  Loss: 0.795334\n",
            "Train Epoch: 5 [25600/50000 ( 51%)]  Loss: 0.728615\n",
            "Train Epoch: 5 [38400/50000 ( 77%)]  Loss: 0.754875\n",
            "Test set: Average loss: 0.0063, Accuracy: 35865/50000 (71.73%)\n",
            "Test set: Average loss: 0.0088, Accuracy: 6936/10000 (69.36%)\n",
            "Train Epoch: 6 [    0/50000 (  0%)]  Loss: 0.682471\n",
            "Train Epoch: 6 [12800/50000 ( 26%)]  Loss: 0.836774\n",
            "Train Epoch: 6 [25600/50000 ( 51%)]  Loss: 0.765407\n",
            "Train Epoch: 6 [38400/50000 ( 77%)]  Loss: 0.871484\n",
            "Test set: Average loss: 0.0061, Accuracy: 36383/50000 (72.77%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7130/10000 (71.30%)\n",
            "Train Epoch: 7 [    0/50000 (  0%)]  Loss: 0.680840\n",
            "Train Epoch: 7 [12800/50000 ( 26%)]  Loss: 0.783405\n",
            "Train Epoch: 7 [25600/50000 ( 51%)]  Loss: 0.740386\n",
            "Train Epoch: 7 [38400/50000 ( 77%)]  Loss: 0.806283\n",
            "Test set: Average loss: 0.0068, Accuracy: 34520/50000 (69.04%)\n",
            "Test set: Average loss: 0.0095, Accuracy: 6745/10000 (67.45%)\n",
            "Train Epoch: 8 [    0/50000 (  0%)]  Loss: 0.960558\n",
            "Train Epoch: 8 [12800/50000 ( 26%)]  Loss: 0.644188\n",
            "Train Epoch: 8 [25600/50000 ( 51%)]  Loss: 0.775310\n",
            "Train Epoch: 8 [38400/50000 ( 77%)]  Loss: 0.676407\n",
            "Test set: Average loss: 0.0057, Accuracy: 37209/50000 (74.42%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7281/10000 (72.81%)\n",
            "Train Epoch: 9 [    0/50000 (  0%)]  Loss: 0.848231\n",
            "Train Epoch: 9 [12800/50000 ( 26%)]  Loss: 0.887468\n",
            "Train Epoch: 9 [25600/50000 ( 51%)]  Loss: 0.986677\n",
            "Train Epoch: 9 [38400/50000 ( 77%)]  Loss: 0.875173\n",
            "Test set: Average loss: 0.0055, Accuracy: 37510/50000 (75.02%)\n",
            "Test set: Average loss: 0.0079, Accuracy: 7314/10000 (73.14%)\n",
            "Train Epoch: 10 [    0/50000 (  0%)]  Loss: 0.793548\n",
            "Train Epoch: 10 [12800/50000 ( 26%)]  Loss: 0.829391\n",
            "Train Epoch: 10 [25600/50000 ( 51%)]  Loss: 0.678289\n",
            "Train Epoch: 10 [38400/50000 ( 77%)]  Loss: 0.720773\n",
            "Test set: Average loss: 0.0052, Accuracy: 38206/50000 (76.41%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7382/10000 (73.82%)\n",
            "Train Epoch: 11 [    0/50000 (  0%)]  Loss: 0.623370\n",
            "Train Epoch: 11 [12800/50000 ( 26%)]  Loss: 0.634258\n",
            "Train Epoch: 11 [25600/50000 ( 51%)]  Loss: 0.696201\n",
            "Train Epoch: 11 [38400/50000 ( 77%)]  Loss: 0.745888\n",
            "Test set: Average loss: 0.0052, Accuracy: 38284/50000 (76.57%)\n",
            "Test set: Average loss: 0.0073, Accuracy: 7462/10000 (74.62%)\n",
            "Train Epoch: 12 [    0/50000 (  0%)]  Loss: 0.552729\n",
            "Train Epoch: 12 [12800/50000 ( 26%)]  Loss: 0.552956\n",
            "Train Epoch: 12 [25600/50000 ( 51%)]  Loss: 0.643708\n",
            "Train Epoch: 12 [38400/50000 ( 77%)]  Loss: 0.644435\n",
            "Test set: Average loss: 0.0052, Accuracy: 38432/50000 (76.86%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7418/10000 (74.18%)\n",
            "Train Epoch: 13 [    0/50000 (  0%)]  Loss: 0.457401\n",
            "Train Epoch: 13 [12800/50000 ( 26%)]  Loss: 0.655061\n",
            "Train Epoch: 13 [25600/50000 ( 51%)]  Loss: 0.628963\n",
            "Train Epoch: 13 [38400/50000 ( 77%)]  Loss: 0.713577\n",
            "Test set: Average loss: 0.0050, Accuracy: 38787/50000 (77.57%)\n",
            "Test set: Average loss: 0.0072, Accuracy: 7555/10000 (75.55%)\n",
            "Train Epoch: 14 [    0/50000 (  0%)]  Loss: 0.651178\n",
            "Train Epoch: 14 [12800/50000 ( 26%)]  Loss: 0.589189\n",
            "Train Epoch: 14 [25600/50000 ( 51%)]  Loss: 0.716886\n",
            "Train Epoch: 14 [38400/50000 ( 77%)]  Loss: 0.693774\n",
            "Test set: Average loss: 0.0050, Accuracy: 38799/50000 (77.60%)\n",
            "Test set: Average loss: 0.0073, Accuracy: 7542/10000 (75.42%)\n",
            "Train Epoch: 15 [    0/50000 (  0%)]  Loss: 0.614004\n",
            "Train Epoch: 15 [12800/50000 ( 26%)]  Loss: 0.635128\n",
            "Train Epoch: 15 [25600/50000 ( 51%)]  Loss: 0.512648\n",
            "Train Epoch: 15 [38400/50000 ( 77%)]  Loss: 0.715980\n",
            "Test set: Average loss: 0.0048, Accuracy: 39259/50000 (78.52%)\n",
            "Test set: Average loss: 0.0069, Accuracy: 7628/10000 (76.28%)\n",
            "Train Epoch: 16 [    0/50000 (  0%)]  Loss: 0.669109\n",
            "Train Epoch: 16 [12800/50000 ( 26%)]  Loss: 0.672026\n",
            "Train Epoch: 16 [25600/50000 ( 51%)]  Loss: 0.738975\n",
            "Train Epoch: 16 [38400/50000 ( 77%)]  Loss: 0.715515\n",
            "Test set: Average loss: 0.0050, Accuracy: 38609/50000 (77.22%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7445/10000 (74.45%)\n",
            "Train Epoch: 17 [    0/50000 (  0%)]  Loss: 0.465084\n",
            "Train Epoch: 17 [12800/50000 ( 26%)]  Loss: 0.571294\n",
            "Train Epoch: 17 [25600/50000 ( 51%)]  Loss: 0.571250\n",
            "Train Epoch: 17 [38400/50000 ( 77%)]  Loss: 0.626241\n",
            "Test set: Average loss: 0.0048, Accuracy: 39164/50000 (78.33%)\n",
            "Test set: Average loss: 0.0071, Accuracy: 7569/10000 (75.69%)\n",
            "Train Epoch: 18 [    0/50000 (  0%)]  Loss: 0.627727\n",
            "Train Epoch: 18 [12800/50000 ( 26%)]  Loss: 0.572710\n",
            "Train Epoch: 18 [25600/50000 ( 51%)]  Loss: 0.602223\n",
            "Train Epoch: 18 [38400/50000 ( 77%)]  Loss: 0.647452\n",
            "Test set: Average loss: 0.0048, Accuracy: 39288/50000 (78.58%)\n",
            "Test set: Average loss: 0.0071, Accuracy: 7649/10000 (76.49%)\n",
            "Train Epoch: 19 [    0/50000 (  0%)]  Loss: 0.727022\n",
            "Train Epoch: 19 [12800/50000 ( 26%)]  Loss: 0.631640\n",
            "Train Epoch: 19 [25600/50000 ( 51%)]  Loss: 0.580658\n",
            "Train Epoch: 19 [38400/50000 ( 77%)]  Loss: 0.649154\n",
            "Test set: Average loss: 0.0046, Accuracy: 39837/50000 (79.67%)\n",
            "Test set: Average loss: 0.0064, Accuracy: 7799/10000 (77.99%)\n",
            "Train Epoch: 20 [    0/50000 (  0%)]  Loss: 0.559886\n",
            "Train Epoch: 20 [12800/50000 ( 26%)]  Loss: 0.621890\n",
            "Train Epoch: 20 [25600/50000 ( 51%)]  Loss: 0.710378\n",
            "Train Epoch: 20 [38400/50000 ( 77%)]  Loss: 0.636239\n",
            "Test set: Average loss: 0.0047, Accuracy: 39455/50000 (78.91%)\n",
            "Test set: Average loss: 0.0071, Accuracy: 7557/10000 (75.57%)\n",
            "Train Epoch: 21 [    0/50000 (  0%)]  Loss: 0.627673\n",
            "Train Epoch: 21 [12800/50000 ( 26%)]  Loss: 0.576128\n",
            "Train Epoch: 21 [25600/50000 ( 51%)]  Loss: 0.752022\n",
            "Train Epoch: 21 [38400/50000 ( 77%)]  Loss: 0.526963\n",
            "Test set: Average loss: 0.0046, Accuracy: 39775/50000 (79.55%)\n",
            "Test set: Average loss: 0.0063, Accuracy: 7810/10000 (78.10%)\n",
            "Train Epoch: 22 [    0/50000 (  0%)]  Loss: 0.667036\n",
            "Train Epoch: 22 [12800/50000 ( 26%)]  Loss: 0.573214\n",
            "Train Epoch: 22 [25600/50000 ( 51%)]  Loss: 0.663582\n",
            "Train Epoch: 22 [38400/50000 ( 77%)]  Loss: 0.626459\n",
            "Test set: Average loss: 0.0044, Accuracy: 40236/50000 (80.47%)\n",
            "Test set: Average loss: 0.0066, Accuracy: 7809/10000 (78.09%)\n",
            "Train Epoch: 23 [    0/50000 (  0%)]  Loss: 0.486723\n",
            "Train Epoch: 23 [12800/50000 ( 26%)]  Loss: 0.532918\n",
            "Train Epoch: 23 [25600/50000 ( 51%)]  Loss: 0.504793\n",
            "Train Epoch: 23 [38400/50000 ( 77%)]  Loss: 0.646008\n",
            "Test set: Average loss: 0.0044, Accuracy: 39890/50000 (79.78%)\n",
            "Test set: Average loss: 0.0068, Accuracy: 7699/10000 (76.99%)\n",
            "Train Epoch: 24 [    0/50000 (  0%)]  Loss: 0.525885\n",
            "Train Epoch: 24 [12800/50000 ( 26%)]  Loss: 0.581270\n",
            "Train Epoch: 24 [25600/50000 ( 51%)]  Loss: 0.584882\n",
            "Train Epoch: 24 [38400/50000 ( 77%)]  Loss: 0.494468\n",
            "Test set: Average loss: 0.0045, Accuracy: 39894/50000 (79.79%)\n",
            "Test set: Average loss: 0.0069, Accuracy: 7674/10000 (76.74%)\n",
            "Train Epoch: 25 [    0/50000 (  0%)]  Loss: 0.548129\n",
            "Train Epoch: 25 [12800/50000 ( 26%)]  Loss: 0.503742\n",
            "Train Epoch: 25 [25600/50000 ( 51%)]  Loss: 0.616280\n",
            "Train Epoch: 25 [38400/50000 ( 77%)]  Loss: 0.722733\n",
            "Test set: Average loss: 0.0043, Accuracy: 40272/50000 (80.54%)\n",
            "Test set: Average loss: 0.0065, Accuracy: 7807/10000 (78.07%)\n",
            "Train Epoch: 26 [    0/50000 (  0%)]  Loss: 0.463420\n",
            "Train Epoch: 26 [12800/50000 ( 26%)]  Loss: 0.699621\n",
            "Train Epoch: 26 [25600/50000 ( 51%)]  Loss: 0.667454\n",
            "Train Epoch: 26 [38400/50000 ( 77%)]  Loss: 0.567491\n",
            "Test set: Average loss: 0.0045, Accuracy: 39799/50000 (79.60%)\n",
            "Test set: Average loss: 0.0067, Accuracy: 7698/10000 (76.98%)\n",
            "Train Epoch: 27 [    0/50000 (  0%)]  Loss: 0.510101\n",
            "Train Epoch: 27 [12800/50000 ( 26%)]  Loss: 0.602429\n",
            "Train Epoch: 27 [25600/50000 ( 51%)]  Loss: 0.532909\n",
            "Train Epoch: 27 [38400/50000 ( 77%)]  Loss: 0.664215\n",
            "Test set: Average loss: 0.0043, Accuracy: 40220/50000 (80.44%)\n",
            "Test set: Average loss: 0.0063, Accuracy: 7807/10000 (78.07%)\n",
            "Train Epoch: 28 [    0/50000 (  0%)]  Loss: 0.598580\n",
            "Train Epoch: 28 [12800/50000 ( 26%)]  Loss: 0.491423\n",
            "Train Epoch: 28 [25600/50000 ( 51%)]  Loss: 0.541684\n",
            "Train Epoch: 28 [38400/50000 ( 77%)]  Loss: 0.713484\n",
            "Test set: Average loss: 0.0041, Accuracy: 40763/50000 (81.53%)\n",
            "Test set: Average loss: 0.0064, Accuracy: 7879/10000 (78.79%)\n",
            "Train Epoch: 29 [    0/50000 (  0%)]  Loss: 0.432755\n",
            "Train Epoch: 29 [12800/50000 ( 26%)]  Loss: 0.627024\n",
            "Train Epoch: 29 [25600/50000 ( 51%)]  Loss: 0.484635\n",
            "Train Epoch: 29 [38400/50000 ( 77%)]  Loss: 0.490413\n",
            "Test set: Average loss: 0.0041, Accuracy: 40868/50000 (81.74%)\n",
            "Test set: Average loss: 0.0065, Accuracy: 7843/10000 (78.43%)\n",
            "Train Epoch: 30 [    0/50000 (  0%)]  Loss: 0.317522\n",
            "Train Epoch: 30 [12800/50000 ( 26%)]  Loss: 0.491800\n",
            "Train Epoch: 30 [25600/50000 ( 51%)]  Loss: 0.529421\n",
            "Train Epoch: 30 [38400/50000 ( 77%)]  Loss: 0.537720\n",
            "Test set: Average loss: 0.0042, Accuracy: 40661/50000 (81.32%)\n",
            "Test set: Average loss: 0.0061, Accuracy: 7907/10000 (79.07%)\n",
            "Train Epoch: 31 [    0/50000 (  0%)]  Loss: 0.614192\n",
            "Train Epoch: 31 [12800/50000 ( 26%)]  Loss: 0.609853\n",
            "Train Epoch: 31 [25600/50000 ( 51%)]  Loss: 0.505930\n",
            "Train Epoch: 31 [38400/50000 ( 77%)]  Loss: 0.509935\n",
            "Test set: Average loss: 0.0042, Accuracy: 40679/50000 (81.36%)\n",
            "Test set: Average loss: 0.0062, Accuracy: 7939/10000 (79.39%)\n",
            "Train Epoch: 32 [    0/50000 (  0%)]  Loss: 0.529120\n",
            "Train Epoch: 32 [12800/50000 ( 26%)]  Loss: 0.544415\n",
            "Train Epoch: 32 [25600/50000 ( 51%)]  Loss: 0.578184\n",
            "Train Epoch: 32 [38400/50000 ( 77%)]  Loss: 0.618625\n",
            "Test set: Average loss: 0.0044, Accuracy: 40203/50000 (80.41%)\n",
            "Test set: Average loss: 0.0068, Accuracy: 7691/10000 (76.91%)\n",
            "Train Epoch: 33 [    0/50000 (  0%)]  Loss: 0.580042\n",
            "Train Epoch: 33 [12800/50000 ( 26%)]  Loss: 0.592858\n",
            "Train Epoch: 33 [25600/50000 ( 51%)]  Loss: 0.680926\n",
            "Train Epoch: 33 [38400/50000 ( 77%)]  Loss: 0.588727\n",
            "Test set: Average loss: 0.0042, Accuracy: 40571/50000 (81.14%)\n",
            "Test set: Average loss: 0.0064, Accuracy: 7846/10000 (78.46%)\n",
            "Train Epoch: 34 [    0/50000 (  0%)]  Loss: 0.529180\n",
            "Train Epoch: 34 [12800/50000 ( 26%)]  Loss: 0.539817\n",
            "Train Epoch: 34 [25600/50000 ( 51%)]  Loss: 0.526793\n",
            "Train Epoch: 34 [38400/50000 ( 77%)]  Loss: 0.533904\n",
            "Test set: Average loss: 0.0043, Accuracy: 40410/50000 (80.82%)\n",
            "Test set: Average loss: 0.0066, Accuracy: 7779/10000 (77.79%)\n",
            "Train Epoch: 35 [    0/50000 (  0%)]  Loss: 0.559863\n",
            "Train Epoch: 35 [12800/50000 ( 26%)]  Loss: 0.510126\n",
            "Train Epoch: 35 [25600/50000 ( 51%)]  Loss: 0.558073\n",
            "Train Epoch: 35 [38400/50000 ( 77%)]  Loss: 0.438928\n",
            "Test set: Average loss: 0.0044, Accuracy: 40179/50000 (80.36%)\n",
            "Test set: Average loss: 0.0068, Accuracy: 7716/10000 (77.16%)\n",
            "Train Epoch: 36 [    0/50000 (  0%)]  Loss: 0.580882\n",
            "Train Epoch: 36 [12800/50000 ( 26%)]  Loss: 0.814585\n",
            "Train Epoch: 36 [25600/50000 ( 51%)]  Loss: 0.614071\n",
            "Train Epoch: 36 [38400/50000 ( 77%)]  Loss: 0.506556\n",
            "Test set: Average loss: 0.0043, Accuracy: 40299/50000 (80.60%)\n",
            "Test set: Average loss: 0.0066, Accuracy: 7821/10000 (78.21%)\n",
            "Train Epoch: 37 [    0/50000 (  0%)]  Loss: 0.572351\n",
            "Train Epoch: 37 [12800/50000 ( 26%)]  Loss: 0.560606\n",
            "Train Epoch: 37 [25600/50000 ( 51%)]  Loss: 0.558115\n",
            "Train Epoch: 37 [38400/50000 ( 77%)]  Loss: 0.568141\n",
            "Test set: Average loss: 0.0041, Accuracy: 40873/50000 (81.75%)\n",
            "Test set: Average loss: 0.0062, Accuracy: 7916/10000 (79.16%)\n",
            "Train Epoch: 38 [    0/50000 (  0%)]  Loss: 0.500045\n",
            "Train Epoch: 38 [12800/50000 ( 26%)]  Loss: 0.419256\n",
            "Train Epoch: 38 [25600/50000 ( 51%)]  Loss: 0.648151\n",
            "Train Epoch: 38 [38400/50000 ( 77%)]  Loss: 0.547683\n",
            "Test set: Average loss: 0.0041, Accuracy: 41080/50000 (82.16%)\n",
            "Test set: Average loss: 0.0062, Accuracy: 7921/10000 (79.21%)\n",
            "Train Epoch: 39 [    0/50000 (  0%)]  Loss: 0.644830\n",
            "Train Epoch: 39 [12800/50000 ( 26%)]  Loss: 0.671019\n",
            "Train Epoch: 39 [25600/50000 ( 51%)]  Loss: 0.565277\n",
            "Train Epoch: 39 [38400/50000 ( 77%)]  Loss: 0.588843\n",
            "Test set: Average loss: 0.0043, Accuracy: 40567/50000 (81.13%)\n",
            "Test set: Average loss: 0.0063, Accuracy: 7895/10000 (78.95%)\n",
            "Train Epoch: 40 [    0/50000 (  0%)]  Loss: 0.618584\n",
            "Train Epoch: 40 [12800/50000 ( 26%)]  Loss: 0.504958\n",
            "Train Epoch: 40 [25600/50000 ( 51%)]  Loss: 0.618233\n",
            "Train Epoch: 40 [38400/50000 ( 77%)]  Loss: 0.464294\n",
            "Test set: Average loss: 0.0045, Accuracy: 39836/50000 (79.67%)\n",
            "Test set: Average loss: 0.0069, Accuracy: 7666/10000 (76.66%)\n",
            "Train Epoch: 41 [    0/50000 (  0%)]  Loss: 0.424421\n",
            "Train Epoch: 41 [12800/50000 ( 26%)]  Loss: 0.372280\n",
            "Train Epoch: 41 [25600/50000 ( 51%)]  Loss: 0.543102\n",
            "Train Epoch: 41 [38400/50000 ( 77%)]  Loss: 0.477554\n",
            "Test set: Average loss: 0.0040, Accuracy: 40998/50000 (82.00%)\n",
            "Test set: Average loss: 0.0062, Accuracy: 7926/10000 (79.26%)\n",
            "Train Epoch: 42 [    0/50000 (  0%)]  Loss: 0.442192\n",
            "Train Epoch: 42 [12800/50000 ( 26%)]  Loss: 0.519702\n",
            "Train Epoch: 42 [25600/50000 ( 51%)]  Loss: 0.572127\n",
            "Train Epoch: 42 [38400/50000 ( 77%)]  Loss: 0.430997\n",
            "Test set: Average loss: 0.0039, Accuracy: 41273/50000 (82.55%)\n",
            "Test set: Average loss: 0.0061, Accuracy: 7956/10000 (79.56%)\n",
            "Train Epoch: 43 [    0/50000 (  0%)]  Loss: 0.491807\n",
            "Train Epoch: 43 [12800/50000 ( 26%)]  Loss: 0.579544\n",
            "Train Epoch: 43 [25600/50000 ( 51%)]  Loss: 0.661665\n",
            "Train Epoch: 43 [38400/50000 ( 77%)]  Loss: 0.508199\n",
            "Test set: Average loss: 0.0040, Accuracy: 40867/50000 (81.73%)\n",
            "Test set: Average loss: 0.0063, Accuracy: 7894/10000 (78.94%)\n",
            "Train Epoch: 44 [    0/50000 (  0%)]  Loss: 0.490489\n",
            "Train Epoch: 44 [12800/50000 ( 26%)]  Loss: 0.399031\n",
            "Train Epoch: 44 [25600/50000 ( 51%)]  Loss: 0.528106\n",
            "Train Epoch: 44 [38400/50000 ( 77%)]  Loss: 0.609727\n",
            "Test set: Average loss: 0.0039, Accuracy: 41207/50000 (82.41%)\n",
            "Test set: Average loss: 0.0061, Accuracy: 7963/10000 (79.63%)\n",
            "Train Epoch: 45 [    0/50000 (  0%)]  Loss: 0.539568\n",
            "Train Epoch: 45 [12800/50000 ( 26%)]  Loss: 0.487204\n",
            "Train Epoch: 45 [25600/50000 ( 51%)]  Loss: 0.534934\n",
            "Train Epoch: 45 [38400/50000 ( 77%)]  Loss: 0.603196\n",
            "Test set: Average loss: 0.0041, Accuracy: 40873/50000 (81.75%)\n",
            "Test set: Average loss: 0.0060, Accuracy: 7921/10000 (79.21%)\n",
            "Train Epoch: 46 [    0/50000 (  0%)]  Loss: 0.439535\n",
            "Train Epoch: 46 [12800/50000 ( 26%)]  Loss: 0.618330\n",
            "Train Epoch: 46 [25600/50000 ( 51%)]  Loss: 0.485883\n",
            "Train Epoch: 46 [38400/50000 ( 77%)]  Loss: 0.496170\n",
            "Test set: Average loss: 0.0039, Accuracy: 41357/50000 (82.71%)\n",
            "Test set: Average loss: 0.0065, Accuracy: 7867/10000 (78.67%)\n",
            "Train Epoch: 47 [    0/50000 (  0%)]  Loss: 0.516643\n",
            "Train Epoch: 47 [12800/50000 ( 26%)]  Loss: 0.722760\n",
            "Train Epoch: 47 [25600/50000 ( 51%)]  Loss: 0.621460\n",
            "Train Epoch: 47 [38400/50000 ( 77%)]  Loss: 0.488359\n",
            "Test set: Average loss: 0.0043, Accuracy: 40280/50000 (80.56%)\n",
            "Test set: Average loss: 0.0068, Accuracy: 7749/10000 (77.49%)\n",
            "Train Epoch: 48 [    0/50000 (  0%)]  Loss: 0.536204\n",
            "Train Epoch: 48 [12800/50000 ( 26%)]  Loss: 0.587422\n",
            "Train Epoch: 48 [25600/50000 ( 51%)]  Loss: 0.411457\n",
            "Train Epoch: 48 [38400/50000 ( 77%)]  Loss: 0.458600\n",
            "Test set: Average loss: 0.0039, Accuracy: 41428/50000 (82.86%)\n",
            "Test set: Average loss: 0.0059, Accuracy: 7983/10000 (79.83%)\n",
            "Train Epoch: 49 [    0/50000 (  0%)]  Loss: 0.521039\n",
            "Train Epoch: 49 [12800/50000 ( 26%)]  Loss: 0.515190\n",
            "Train Epoch: 49 [25600/50000 ( 51%)]  Loss: 0.374095\n",
            "Train Epoch: 49 [38400/50000 ( 77%)]  Loss: 0.510307\n",
            "Test set: Average loss: 0.0041, Accuracy: 40797/50000 (81.59%)\n",
            "Test set: Average loss: 0.0066, Accuracy: 7895/10000 (78.95%)\n",
            "Train Epoch: 50 [    0/50000 (  0%)]  Loss: 0.586690\n",
            "Train Epoch: 50 [12800/50000 ( 26%)]  Loss: 0.399940\n",
            "Train Epoch: 50 [25600/50000 ( 51%)]  Loss: 0.581499\n",
            "Train Epoch: 50 [38400/50000 ( 77%)]  Loss: 0.597529\n",
            "Test set: Average loss: 0.0041, Accuracy: 40914/50000 (81.83%)\n",
            "Test set: Average loss: 0.0062, Accuracy: 7910/10000 (79.10%)\n",
            "Train Epoch: 51 [    0/50000 (  0%)]  Loss: 0.364325\n",
            "Train Epoch: 51 [12800/50000 ( 26%)]  Loss: 0.582785\n",
            "Train Epoch: 51 [25600/50000 ( 51%)]  Loss: 0.529083\n",
            "Train Epoch: 51 [38400/50000 ( 77%)]  Loss: 0.522893\n",
            "Test set: Average loss: 0.0043, Accuracy: 40375/50000 (80.75%)\n",
            "Test set: Average loss: 0.0066, Accuracy: 7729/10000 (77.29%)\n",
            "Train Epoch: 52 [    0/50000 (  0%)]  Loss: 0.420533\n",
            "Train Epoch: 52 [12800/50000 ( 26%)]  Loss: 0.557169\n",
            "Train Epoch: 52 [25600/50000 ( 51%)]  Loss: 0.339168\n",
            "Train Epoch: 52 [38400/50000 ( 77%)]  Loss: 0.381090\n",
            "Test set: Average loss: 0.0043, Accuracy: 40339/50000 (80.68%)\n",
            "Test set: Average loss: 0.0066, Accuracy: 7822/10000 (78.22%)\n",
            "Train Epoch: 53 [    0/50000 (  0%)]  Loss: 0.591158\n",
            "Train Epoch: 53 [12800/50000 ( 26%)]  Loss: 0.422971\n",
            "Train Epoch: 53 [25600/50000 ( 51%)]  Loss: 0.459271\n",
            "Train Epoch: 53 [38400/50000 ( 77%)]  Loss: 0.672000\n",
            "Test set: Average loss: 0.0038, Accuracy: 41449/50000 (82.90%)\n",
            "Test set: Average loss: 0.0060, Accuracy: 7933/10000 (79.33%)\n",
            "Train Epoch: 54 [    0/50000 (  0%)]  Loss: 0.350596\n",
            "Train Epoch: 54 [12800/50000 ( 26%)]  Loss: 0.440618\n",
            "Train Epoch: 54 [25600/50000 ( 51%)]  Loss: 0.609991\n",
            "Train Epoch: 54 [38400/50000 ( 77%)]  Loss: 0.484254\n",
            "Test set: Average loss: 0.0038, Accuracy: 41511/50000 (83.02%)\n",
            "Test set: Average loss: 0.0061, Accuracy: 7972/10000 (79.72%)\n",
            "Train Epoch: 55 [    0/50000 (  0%)]  Loss: 0.636636\n",
            "Train Epoch: 55 [12800/50000 ( 26%)]  Loss: 0.506720\n",
            "Train Epoch: 55 [25600/50000 ( 51%)]  Loss: 0.456863\n",
            "Train Epoch: 55 [38400/50000 ( 77%)]  Loss: 0.339020\n",
            "Test set: Average loss: 0.0039, Accuracy: 41250/50000 (82.50%)\n",
            "Test set: Average loss: 0.0063, Accuracy: 7913/10000 (79.13%)\n",
            "Train Epoch: 56 [    0/50000 (  0%)]  Loss: 0.548099\n",
            "Train Epoch: 56 [12800/50000 ( 26%)]  Loss: 0.565715\n",
            "Train Epoch: 56 [25600/50000 ( 51%)]  Loss: 0.642147\n",
            "Train Epoch: 56 [38400/50000 ( 77%)]  Loss: 0.696763\n",
            "Test set: Average loss: 0.0037, Accuracy: 41704/50000 (83.41%)\n",
            "Test set: Average loss: 0.0060, Accuracy: 8003/10000 (80.03%)\n",
            "Train Epoch: 57 [    0/50000 (  0%)]  Loss: 0.545729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b8d503692d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-93b566bdddca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m#loss = nn.NLLLoss(output,target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXDAzxwFewV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_by_row(logits, T = 1.0):\n",
        "    mx = np.max(logits, axis=-1, keepdims=True)\n",
        "    exp = np.exp((logits - mx)/T)\n",
        "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
        "    return exp/denominator\n",
        "\n",
        "def classifier_performance(model, train_loader, test_loader):\n",
        "\n",
        "    output_train_benign = []\n",
        "    train_label = []\n",
        "    for num, data in enumerate(train_loader):\n",
        "        images,labels = data\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        train_label.append(labels.numpy())\n",
        "        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    train_label = np.concatenate(train_label)\n",
        "    output_train_benign=np.concatenate(output_train_benign)\n",
        "\n",
        "    test_label = []\n",
        "    output_test_benign = []\n",
        "\n",
        "    for num, data in enumerate(test_loader):\n",
        "        images,labels = data\n",
        "\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        test_label.append(labels.numpy())\n",
        "        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    test_label = np.concatenate(test_label)\n",
        "    output_test_benign=np.concatenate(output_test_benign)\n",
        "\n",
        "\n",
        "    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n",
        "    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n",
        "\n",
        "    print('Accuracy: ', (train_acc1, test_acc1))\n",
        "\n",
        "    return output_train_benign, output_test_benign, train_label, test_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
        "    \n",
        "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
        "    confidence1 = []\n",
        "    confidence2 = []\n",
        "    acc1 = 0\n",
        "    acc2 = 0\n",
        "    for num in range(confidence_mtx1.shape[0]):\n",
        "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
        "        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n",
        "            acc1 += 1\n",
        "            \n",
        "    for num in range(confidence_mtx2.shape[0]):\n",
        "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
        "        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n",
        "            acc2 += 1\n",
        "    confidence1 = np.array(confidence1)\n",
        "    confidence2 = np.array(confidence2)\n",
        "    \n",
        "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
        "    \n",
        "    \n",
        "    #sort_confidence = np.sort(confidence1)\n",
        "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
        "    max_accuracy = 0.5\n",
        "    best_precision = 0.5\n",
        "    best_recall = 0.5\n",
        "    for num in range(len(sort_confidence)):\n",
        "        delta = sort_confidence[num]\n",
        "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
        "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
        "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
        "        if accuracy_now > max_accuracy:\n",
        "            max_accuracy = accuracy_now\n",
        "            best_precision = ratio1/(ratio1+ratio2)\n",
        "            best_recall = ratio1\n",
        "    print('membership inference accuracy is:', max_accuracy)\n",
        "    return max_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-O0GyDrezes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "188b82bf-a22b-4f77-fb3a-a1daf6c4f886"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import os\n",
        "import numpy as np\n",
        "import math \n",
        "import scipy\n",
        "import sys  \n",
        "\n",
        "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  (0.81934, 0.7874)\n",
            "model accuracy for training and test- (0.81934, 0.7874)\n",
            "membership inference accuracy is: 0.51766\n",
            "Maximum Accuracy: 0.51766\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}