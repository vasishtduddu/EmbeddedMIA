{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary_VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P8mK1gLddEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from collections import OrderedDict\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojn0VPRecnMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BinActive(torch.autograd.Function):\n",
        "    '''\n",
        "    Binarize the input activations and calculate the mean across channel dimension.\n",
        "    '''\n",
        "    def forward(self, input):\n",
        "        self.save_for_backward(input)\n",
        "        size = input.size()\n",
        "        mean = torch.mean(input.abs(), 1, keepdim=True)\n",
        "        input = input.sign()\n",
        "        return input, mean\n",
        "\n",
        "    def backward(self, grad_output, grad_output_mean):\n",
        "        input, = self.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input.ge(1)] = 0\n",
        "        grad_input[input.le(-1)] = 0\n",
        "        return grad_input\n",
        "\n",
        "class BinConv2d(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels,\n",
        "            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n",
        "        super(BinConv2d, self).__init__()\n",
        "        self.layer_type = 'BinConv2d'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dropout_ratio = 0\n",
        "        dropout=0\n",
        "        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
        "        self.bn.weight.data = self.bn.weight.data.zero_().add(1.0)\n",
        "        if dropout!=0:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
        "                kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x, mean = BinActive()(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': ['M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "class Bin_VGG_train(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(Bin_VGG_train, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1)),\n",
        "            ('bn0', nn.BatchNorm2d(64)),\n",
        "            ('relu0', nn.ReLU(inplace=True))\n",
        "            ])\n",
        "        in_channels = 64\n",
        "        cnt = 1\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers['pool'+str(cnt)] = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                cnt += 1\n",
        "            else:\n",
        "                layers['conv'+str(cnt)] = BinConv2d(input_channels=in_channels, output_channels=x, stride=1, kernel_size=3, padding=1)\n",
        "                cnt += 1\n",
        "                layers['bn'+str(cnt)] = nn.BatchNorm2d(x)\n",
        "                cnt += 1\n",
        "                layers['relu'+str(cnt)] = nn.ReLU(inplace=True)\n",
        "                cnt += 1\n",
        "                in_channels = x\n",
        "        layers['pool'+str(cnt)] = nn.AvgPool2d(kernel_size=1, stride=1)\n",
        "        return nn.Sequential(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOxdhLbLdZms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import numpy\n",
        "\n",
        "class BinOp():\n",
        "    def __init__(self, model):\n",
        "        # count the number of Conv2d\n",
        "        count_Conv2d = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                count_Conv2d = count_Conv2d + 1\n",
        "\n",
        "        start_range = 1\n",
        "        end_range = count_Conv2d-2\n",
        "        self.bin_range = numpy.linspace(start_range,\n",
        "                end_range, end_range-start_range+1)\\\n",
        "                        .astype('int').tolist()\n",
        "        self.num_of_params = len(self.bin_range)\n",
        "        self.saved_params = []\n",
        "        self.target_params = []\n",
        "        self.target_modules = []\n",
        "        index = -1\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                index = index + 1\n",
        "                if index in self.bin_range:\n",
        "                    tmp = m.weight.data.clone()\n",
        "                    self.saved_params.append(tmp)\n",
        "                    self.target_modules.append(m.weight)\n",
        "\n",
        "    def binarization(self):\n",
        "        self.meancenterConvParams()\n",
        "        self.clampConvParams()\n",
        "        self.save_params()\n",
        "        self.binarizeConvParams()\n",
        "\n",
        "    def meancenterConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            s = self.target_modules[index].data.size()\n",
        "            negMean = self.target_modules[index].data.mean(1, keepdim=True).\\\n",
        "                    mul(-1).expand_as(self.target_modules[index].data)\n",
        "            self.target_modules[index].data = self.target_modules[index].data.add(negMean)\n",
        "\n",
        "    def clampConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data = \\\n",
        "                    self.target_modules[index].data.clamp(-1.0, 1.0)\n",
        "\n",
        "    def save_params(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.saved_params[index].copy_(self.target_modules[index].data)\n",
        "\n",
        "    def binarizeConvParams(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            n = self.target_modules[index].data[0].nelement()\n",
        "            s = self.target_modules[index].data.size()\n",
        "            m = self.target_modules[index].data.norm(1, 3, keepdim=True)\\\n",
        "                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n)\n",
        "            self.target_modules[index].data = \\\n",
        "                    self.target_modules[index].data.sign().mul(m.expand(s))\n",
        "\n",
        "    def restore(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data.copy_(self.saved_params[index])\n",
        "\n",
        "    def updateBinaryGradWeight(self):\n",
        "        for index in range(self.num_of_params):\n",
        "            weight = self.target_modules[index].data\n",
        "            n = weight[0].nelement()\n",
        "            s = weight.size()\n",
        "            m = weight.norm(1, 3, keepdim=True)\\\n",
        "                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
        "            m[weight.lt(-1.0)] = 0 \n",
        "            m[weight.gt(1.0)] = 0\n",
        "            # m = m.add(1.0/n).mul(1.0-1.0/s[1]).mul(n)\n",
        "            # self.target_modules[index].grad.data = \\\n",
        "            #         self.target_modules[index].grad.data.mul(m)\n",
        "            m = m.mul(self.target_modules[index].grad.data)\n",
        "            m_add = weight.sign().mul(self.target_modules[index].grad.data)\n",
        "            m_add = m_add.sum(3, keepdim=True)\\\n",
        "                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
        "            m_add = m_add.mul(weight.sign())\n",
        "            self.target_modules[index].grad.data = m.add(m_add).mul(1.0-1.0/s[1]).mul(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em143EF9edj6",
        "colab_type": "code",
        "outputId": "81181888-2bf2-4887-e117-595add982093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:07, 24068524.09it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwrC2SJ9eOim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(trainloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            #loss = nn.NLLLoss(output,target)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if batch_idx % 100 == 0:\n",
        "                done = batch_idx * len(data)\n",
        "                percentage = 100. * batch_idx / len(trainloader)\n",
        "                print(f'Train Epoch: {epoch} [{done:5}/{len(trainloader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
        "\n",
        "        test(trainloader)\n",
        "        test(testloader)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            #test_loss += nn.NLLLoss(output, target).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(loader.dataset)\n",
        "        accuracy = 100. * correct / len(loader.dataset)\n",
        "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkoGwyFedx5Q",
        "colab_type": "code",
        "outputId": "e3da1e20-652e-4a4d-b3c6-499c9864d3e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Bin_VGG_train('VGG16')\n",
        "\n",
        "model.cuda()\n",
        "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
        "print(model)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4,weight_decay=0.0000)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define the binarization operator\n",
        "bin_op = BinOp(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataParallel(\n",
            "  (module): Bin_VGG_train(\n",
            "    (features): Sequential(\n",
            "      (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu0): ReLU(inplace=True)\n",
            "      (conv1): BinConv2d(\n",
            "        (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu3): ReLU(inplace=True)\n",
            "      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (conv5): BinConv2d(\n",
            "        (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu7): ReLU(inplace=True)\n",
            "      (conv8): BinConv2d(\n",
            "        (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu10): ReLU(inplace=True)\n",
            "      (pool11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (conv12): BinConv2d(\n",
            "        (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu14): ReLU(inplace=True)\n",
            "      (conv15): BinConv2d(\n",
            "        (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu17): ReLU(inplace=True)\n",
            "      (conv18): BinConv2d(\n",
            "        (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu20): ReLU(inplace=True)\n",
            "      (pool21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (conv22): BinConv2d(\n",
            "        (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu24): ReLU(inplace=True)\n",
            "      (conv25): BinConv2d(\n",
            "        (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu27): ReLU(inplace=True)\n",
            "      (conv28): BinConv2d(\n",
            "        (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu30): ReLU(inplace=True)\n",
            "      (pool31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (conv32): BinConv2d(\n",
            "        (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu34): ReLU(inplace=True)\n",
            "      (conv35): BinConv2d(\n",
            "        (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn36): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu37): ReLU(inplace=True)\n",
            "      (conv38): BinConv2d(\n",
            "        (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn39): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu40): ReLU(inplace=True)\n",
            "      (pool41): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (pool42): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "    )\n",
            "    (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcfB-qK3diJC",
        "colab_type": "code",
        "outputId": "e7ca5665-9b86-43e3-deba-a77f5879c195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "train(125)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [    0/50000 (  0%)]  Loss: 2.673512\n",
            "Train Epoch: 0 [12800/50000 ( 26%)]  Loss: 2.332272\n",
            "Train Epoch: 0 [25600/50000 ( 51%)]  Loss: 2.182087\n",
            "Train Epoch: 0 [38400/50000 ( 77%)]  Loss: 2.057146\n",
            "Test set: Average loss: 0.0159, Accuracy: 11644/50000 (23.29%)\n",
            "Test set: Average loss: 0.0201, Accuracy: 2417/10000 (24.17%)\n",
            "Train Epoch: 1 [    0/50000 (  0%)]  Loss: 1.937038\n",
            "Train Epoch: 1 [12800/50000 ( 26%)]  Loss: 2.102765\n",
            "Train Epoch: 1 [25600/50000 ( 51%)]  Loss: 2.020616\n",
            "Train Epoch: 1 [38400/50000 ( 77%)]  Loss: 1.864042\n",
            "Test set: Average loss: 0.0151, Accuracy: 13290/50000 (26.58%)\n",
            "Test set: Average loss: 0.0189, Accuracy: 2858/10000 (28.58%)\n",
            "Train Epoch: 2 [    0/50000 (  0%)]  Loss: 1.986360\n",
            "Train Epoch: 2 [12800/50000 ( 26%)]  Loss: 1.852508\n",
            "Train Epoch: 2 [25600/50000 ( 51%)]  Loss: 1.851635\n",
            "Train Epoch: 2 [38400/50000 ( 77%)]  Loss: 1.776468\n",
            "Test set: Average loss: 0.0146, Accuracy: 14942/50000 (29.88%)\n",
            "Test set: Average loss: 0.0186, Accuracy: 3106/10000 (31.06%)\n",
            "Train Epoch: 3 [    0/50000 (  0%)]  Loss: 1.964351\n",
            "Train Epoch: 3 [12800/50000 ( 26%)]  Loss: 1.857867\n",
            "Train Epoch: 3 [25600/50000 ( 51%)]  Loss: 1.822279\n",
            "Train Epoch: 3 [38400/50000 ( 77%)]  Loss: 1.859229\n",
            "Test set: Average loss: 0.0140, Accuracy: 16081/50000 (32.16%)\n",
            "Test set: Average loss: 0.0174, Accuracy: 3498/10000 (34.98%)\n",
            "Train Epoch: 4 [    0/50000 (  0%)]  Loss: 1.908594\n",
            "Train Epoch: 4 [12800/50000 ( 26%)]  Loss: 1.749864\n",
            "Train Epoch: 4 [25600/50000 ( 51%)]  Loss: 1.739115\n",
            "Train Epoch: 4 [38400/50000 ( 77%)]  Loss: 1.658898\n",
            "Test set: Average loss: 0.0137, Accuracy: 17398/50000 (34.80%)\n",
            "Test set: Average loss: 0.0171, Accuracy: 3596/10000 (35.96%)\n",
            "Train Epoch: 5 [    0/50000 (  0%)]  Loss: 1.792735\n",
            "Train Epoch: 5 [12800/50000 ( 26%)]  Loss: 1.815274\n",
            "Train Epoch: 5 [25600/50000 ( 51%)]  Loss: 1.589675\n",
            "Train Epoch: 5 [38400/50000 ( 77%)]  Loss: 1.630271\n",
            "Test set: Average loss: 0.0135, Accuracy: 17582/50000 (35.16%)\n",
            "Test set: Average loss: 0.0169, Accuracy: 3713/10000 (37.13%)\n",
            "Train Epoch: 6 [    0/50000 (  0%)]  Loss: 1.733267\n",
            "Train Epoch: 6 [12800/50000 ( 26%)]  Loss: 1.806474\n",
            "Train Epoch: 6 [25600/50000 ( 51%)]  Loss: 1.774948\n",
            "Train Epoch: 6 [38400/50000 ( 77%)]  Loss: 1.649726\n",
            "Test set: Average loss: 0.0131, Accuracy: 18772/50000 (37.54%)\n",
            "Test set: Average loss: 0.0161, Accuracy: 3953/10000 (39.53%)\n",
            "Train Epoch: 7 [    0/50000 (  0%)]  Loss: 1.641285\n",
            "Train Epoch: 7 [12800/50000 ( 26%)]  Loss: 1.693873\n",
            "Train Epoch: 7 [25600/50000 ( 51%)]  Loss: 1.617424\n",
            "Train Epoch: 7 [38400/50000 ( 77%)]  Loss: 1.702129\n",
            "Test set: Average loss: 0.0127, Accuracy: 19936/50000 (39.87%)\n",
            "Test set: Average loss: 0.0157, Accuracy: 4154/10000 (41.54%)\n",
            "Train Epoch: 8 [    0/50000 (  0%)]  Loss: 1.608279\n",
            "Train Epoch: 8 [12800/50000 ( 26%)]  Loss: 1.620592\n",
            "Train Epoch: 8 [25600/50000 ( 51%)]  Loss: 1.829921\n",
            "Train Epoch: 8 [38400/50000 ( 77%)]  Loss: 1.666194\n",
            "Test set: Average loss: 0.0125, Accuracy: 20286/50000 (40.57%)\n",
            "Test set: Average loss: 0.0155, Accuracy: 4188/10000 (41.88%)\n",
            "Train Epoch: 9 [    0/50000 (  0%)]  Loss: 1.680061\n",
            "Train Epoch: 9 [12800/50000 ( 26%)]  Loss: 1.623079\n",
            "Train Epoch: 9 [25600/50000 ( 51%)]  Loss: 1.693631\n",
            "Train Epoch: 9 [38400/50000 ( 77%)]  Loss: 1.462362\n",
            "Test set: Average loss: 0.0121, Accuracy: 21194/50000 (42.39%)\n",
            "Test set: Average loss: 0.0150, Accuracy: 4406/10000 (44.06%)\n",
            "Train Epoch: 10 [    0/50000 (  0%)]  Loss: 1.549116\n",
            "Train Epoch: 10 [12800/50000 ( 26%)]  Loss: 1.606901\n",
            "Train Epoch: 10 [25600/50000 ( 51%)]  Loss: 1.637472\n",
            "Train Epoch: 10 [38400/50000 ( 77%)]  Loss: 1.508518\n",
            "Test set: Average loss: 0.0118, Accuracy: 22085/50000 (44.17%)\n",
            "Test set: Average loss: 0.0147, Accuracy: 4558/10000 (45.58%)\n",
            "Train Epoch: 11 [    0/50000 (  0%)]  Loss: 1.482695\n",
            "Train Epoch: 11 [12800/50000 ( 26%)]  Loss: 1.472624\n",
            "Train Epoch: 11 [25600/50000 ( 51%)]  Loss: 1.546820\n",
            "Train Epoch: 11 [38400/50000 ( 77%)]  Loss: 1.507731\n",
            "Test set: Average loss: 0.0116, Accuracy: 22370/50000 (44.74%)\n",
            "Test set: Average loss: 0.0145, Accuracy: 4573/10000 (45.73%)\n",
            "Train Epoch: 12 [    0/50000 (  0%)]  Loss: 1.392403\n",
            "Train Epoch: 12 [12800/50000 ( 26%)]  Loss: 1.367141\n",
            "Train Epoch: 12 [25600/50000 ( 51%)]  Loss: 1.559443\n",
            "Train Epoch: 12 [38400/50000 ( 77%)]  Loss: 1.601758\n",
            "Test set: Average loss: 0.0114, Accuracy: 22895/50000 (45.79%)\n",
            "Test set: Average loss: 0.0143, Accuracy: 4769/10000 (47.69%)\n",
            "Train Epoch: 13 [    0/50000 (  0%)]  Loss: 1.598973\n",
            "Train Epoch: 13 [12800/50000 ( 26%)]  Loss: 1.406206\n",
            "Train Epoch: 13 [25600/50000 ( 51%)]  Loss: 1.589744\n",
            "Train Epoch: 13 [38400/50000 ( 77%)]  Loss: 1.383492\n",
            "Test set: Average loss: 0.0112, Accuracy: 23733/50000 (47.47%)\n",
            "Test set: Average loss: 0.0139, Accuracy: 4863/10000 (48.63%)\n",
            "Train Epoch: 14 [    0/50000 (  0%)]  Loss: 1.371225\n",
            "Train Epoch: 14 [12800/50000 ( 26%)]  Loss: 1.621986\n",
            "Train Epoch: 14 [25600/50000 ( 51%)]  Loss: 1.389303\n",
            "Train Epoch: 14 [38400/50000 ( 77%)]  Loss: 1.611194\n",
            "Test set: Average loss: 0.0110, Accuracy: 24108/50000 (48.22%)\n",
            "Test set: Average loss: 0.0137, Accuracy: 4950/10000 (49.50%)\n",
            "Train Epoch: 15 [    0/50000 (  0%)]  Loss: 1.363990\n",
            "Train Epoch: 15 [12800/50000 ( 26%)]  Loss: 1.350091\n",
            "Train Epoch: 15 [25600/50000 ( 51%)]  Loss: 1.600393\n",
            "Train Epoch: 15 [38400/50000 ( 77%)]  Loss: 1.594372\n",
            "Test set: Average loss: 0.0108, Accuracy: 24536/50000 (49.07%)\n",
            "Test set: Average loss: 0.0136, Accuracy: 4970/10000 (49.70%)\n",
            "Train Epoch: 16 [    0/50000 (  0%)]  Loss: 1.342798\n",
            "Train Epoch: 16 [12800/50000 ( 26%)]  Loss: 1.260556\n",
            "Train Epoch: 16 [25600/50000 ( 51%)]  Loss: 1.414212\n",
            "Train Epoch: 16 [38400/50000 ( 77%)]  Loss: 1.324746\n",
            "Test set: Average loss: 0.0107, Accuracy: 24841/50000 (49.68%)\n",
            "Test set: Average loss: 0.0136, Accuracy: 4990/10000 (49.90%)\n",
            "Train Epoch: 17 [    0/50000 (  0%)]  Loss: 1.302522\n",
            "Train Epoch: 17 [12800/50000 ( 26%)]  Loss: 1.392728\n",
            "Train Epoch: 17 [25600/50000 ( 51%)]  Loss: 1.243499\n",
            "Train Epoch: 17 [38400/50000 ( 77%)]  Loss: 1.413225\n",
            "Test set: Average loss: 0.0105, Accuracy: 25199/50000 (50.40%)\n",
            "Test set: Average loss: 0.0134, Accuracy: 5115/10000 (51.15%)\n",
            "Train Epoch: 18 [    0/50000 (  0%)]  Loss: 1.227484\n",
            "Train Epoch: 18 [12800/50000 ( 26%)]  Loss: 1.308692\n",
            "Train Epoch: 18 [25600/50000 ( 51%)]  Loss: 1.075410\n",
            "Train Epoch: 18 [38400/50000 ( 77%)]  Loss: 1.289312\n",
            "Test set: Average loss: 0.0105, Accuracy: 25253/50000 (50.51%)\n",
            "Test set: Average loss: 0.0134, Accuracy: 5096/10000 (50.96%)\n",
            "Train Epoch: 19 [    0/50000 (  0%)]  Loss: 1.333601\n",
            "Train Epoch: 19 [12800/50000 ( 26%)]  Loss: 1.216034\n",
            "Train Epoch: 19 [25600/50000 ( 51%)]  Loss: 1.179991\n",
            "Train Epoch: 19 [38400/50000 ( 77%)]  Loss: 1.386612\n",
            "Test set: Average loss: 0.0102, Accuracy: 26007/50000 (52.01%)\n",
            "Test set: Average loss: 0.0127, Accuracy: 5335/10000 (53.35%)\n",
            "Train Epoch: 20 [    0/50000 (  0%)]  Loss: 1.422026\n",
            "Train Epoch: 20 [12800/50000 ( 26%)]  Loss: 1.286210\n",
            "Train Epoch: 20 [25600/50000 ( 51%)]  Loss: 1.213079\n",
            "Train Epoch: 20 [38400/50000 ( 77%)]  Loss: 1.208143\n",
            "Test set: Average loss: 0.0099, Accuracy: 26874/50000 (53.75%)\n",
            "Test set: Average loss: 0.0124, Accuracy: 5482/10000 (54.82%)\n",
            "Train Epoch: 21 [    0/50000 (  0%)]  Loss: 1.216822\n",
            "Train Epoch: 21 [12800/50000 ( 26%)]  Loss: 1.352956\n",
            "Train Epoch: 21 [25600/50000 ( 51%)]  Loss: 1.334899\n",
            "Train Epoch: 21 [38400/50000 ( 77%)]  Loss: 1.335546\n",
            "Test set: Average loss: 0.0096, Accuracy: 27462/50000 (54.92%)\n",
            "Test set: Average loss: 0.0123, Accuracy: 5553/10000 (55.53%)\n",
            "Train Epoch: 22 [    0/50000 (  0%)]  Loss: 1.247933\n",
            "Train Epoch: 22 [12800/50000 ( 26%)]  Loss: 1.240600\n",
            "Train Epoch: 22 [25600/50000 ( 51%)]  Loss: 1.224621\n",
            "Train Epoch: 22 [38400/50000 ( 77%)]  Loss: 1.146689\n",
            "Test set: Average loss: 0.0097, Accuracy: 27172/50000 (54.34%)\n",
            "Test set: Average loss: 0.0122, Accuracy: 5510/10000 (55.10%)\n",
            "Train Epoch: 23 [    0/50000 (  0%)]  Loss: 1.301337\n",
            "Train Epoch: 23 [12800/50000 ( 26%)]  Loss: 1.274107\n",
            "Train Epoch: 23 [25600/50000 ( 51%)]  Loss: 1.232047\n",
            "Train Epoch: 23 [38400/50000 ( 77%)]  Loss: 1.225721\n",
            "Test set: Average loss: 0.0095, Accuracy: 27843/50000 (55.69%)\n",
            "Test set: Average loss: 0.0120, Accuracy: 5646/10000 (56.46%)\n",
            "Train Epoch: 24 [    0/50000 (  0%)]  Loss: 1.178102\n",
            "Train Epoch: 24 [12800/50000 ( 26%)]  Loss: 1.358682\n",
            "Train Epoch: 24 [25600/50000 ( 51%)]  Loss: 1.203003\n",
            "Train Epoch: 24 [38400/50000 ( 77%)]  Loss: 1.181812\n",
            "Test set: Average loss: 0.0093, Accuracy: 28166/50000 (56.33%)\n",
            "Test set: Average loss: 0.0117, Accuracy: 5697/10000 (56.97%)\n",
            "Train Epoch: 25 [    0/50000 (  0%)]  Loss: 1.367624\n",
            "Train Epoch: 25 [12800/50000 ( 26%)]  Loss: 1.278896\n",
            "Train Epoch: 25 [25600/50000 ( 51%)]  Loss: 1.158236\n",
            "Train Epoch: 25 [38400/50000 ( 77%)]  Loss: 1.184940\n",
            "Test set: Average loss: 0.0091, Accuracy: 28963/50000 (57.93%)\n",
            "Test set: Average loss: 0.0117, Accuracy: 5747/10000 (57.47%)\n",
            "Train Epoch: 26 [    0/50000 (  0%)]  Loss: 1.086497\n",
            "Train Epoch: 26 [12800/50000 ( 26%)]  Loss: 1.420780\n",
            "Train Epoch: 26 [25600/50000 ( 51%)]  Loss: 1.259971\n",
            "Train Epoch: 26 [38400/50000 ( 77%)]  Loss: 1.101558\n",
            "Test set: Average loss: 0.0089, Accuracy: 29148/50000 (58.30%)\n",
            "Test set: Average loss: 0.0115, Accuracy: 5774/10000 (57.74%)\n",
            "Train Epoch: 27 [    0/50000 (  0%)]  Loss: 1.151284\n",
            "Train Epoch: 27 [12800/50000 ( 26%)]  Loss: 1.266622\n",
            "Train Epoch: 27 [25600/50000 ( 51%)]  Loss: 1.034123\n",
            "Train Epoch: 27 [38400/50000 ( 77%)]  Loss: 1.212311\n",
            "Test set: Average loss: 0.0089, Accuracy: 29345/50000 (58.69%)\n",
            "Test set: Average loss: 0.0115, Accuracy: 5812/10000 (58.12%)\n",
            "Train Epoch: 28 [    0/50000 (  0%)]  Loss: 1.053203\n",
            "Train Epoch: 28 [12800/50000 ( 26%)]  Loss: 1.196193\n",
            "Train Epoch: 28 [25600/50000 ( 51%)]  Loss: 1.007130\n",
            "Train Epoch: 28 [38400/50000 ( 77%)]  Loss: 1.143022\n",
            "Test set: Average loss: 0.0086, Accuracy: 29983/50000 (59.97%)\n",
            "Test set: Average loss: 0.0111, Accuracy: 5955/10000 (59.55%)\n",
            "Train Epoch: 29 [    0/50000 (  0%)]  Loss: 1.217187\n",
            "Train Epoch: 29 [12800/50000 ( 26%)]  Loss: 0.992497\n",
            "Train Epoch: 29 [25600/50000 ( 51%)]  Loss: 1.041998\n",
            "Train Epoch: 29 [38400/50000 ( 77%)]  Loss: 0.935604\n",
            "Test set: Average loss: 0.0085, Accuracy: 30108/50000 (60.22%)\n",
            "Test set: Average loss: 0.0110, Accuracy: 6020/10000 (60.20%)\n",
            "Train Epoch: 30 [    0/50000 (  0%)]  Loss: 1.092988\n",
            "Train Epoch: 30 [12800/50000 ( 26%)]  Loss: 1.132894\n",
            "Train Epoch: 30 [25600/50000 ( 51%)]  Loss: 1.030924\n",
            "Train Epoch: 30 [38400/50000 ( 77%)]  Loss: 1.106751\n",
            "Test set: Average loss: 0.0086, Accuracy: 30128/50000 (60.26%)\n",
            "Test set: Average loss: 0.0112, Accuracy: 5993/10000 (59.93%)\n",
            "Train Epoch: 31 [    0/50000 (  0%)]  Loss: 1.391320\n",
            "Train Epoch: 31 [12800/50000 ( 26%)]  Loss: 1.144299\n",
            "Train Epoch: 31 [25600/50000 ( 51%)]  Loss: 1.092059\n",
            "Train Epoch: 31 [38400/50000 ( 77%)]  Loss: 1.009047\n",
            "Test set: Average loss: 0.0085, Accuracy: 30240/50000 (60.48%)\n",
            "Test set: Average loss: 0.0108, Accuracy: 6067/10000 (60.67%)\n",
            "Train Epoch: 32 [    0/50000 (  0%)]  Loss: 1.113497\n",
            "Train Epoch: 32 [12800/50000 ( 26%)]  Loss: 0.999864\n",
            "Train Epoch: 32 [25600/50000 ( 51%)]  Loss: 1.090148\n",
            "Train Epoch: 32 [38400/50000 ( 77%)]  Loss: 1.101184\n",
            "Test set: Average loss: 0.0082, Accuracy: 31022/50000 (62.04%)\n",
            "Test set: Average loss: 0.0106, Accuracy: 6172/10000 (61.72%)\n",
            "Train Epoch: 33 [    0/50000 (  0%)]  Loss: 1.034333\n",
            "Train Epoch: 33 [12800/50000 ( 26%)]  Loss: 1.103621\n",
            "Train Epoch: 33 [25600/50000 ( 51%)]  Loss: 1.010997\n",
            "Train Epoch: 33 [38400/50000 ( 77%)]  Loss: 1.035895\n",
            "Test set: Average loss: 0.0081, Accuracy: 31411/50000 (62.82%)\n",
            "Test set: Average loss: 0.0105, Accuracy: 6242/10000 (62.42%)\n",
            "Train Epoch: 34 [    0/50000 (  0%)]  Loss: 0.914014\n",
            "Train Epoch: 34 [12800/50000 ( 26%)]  Loss: 1.196260\n",
            "Train Epoch: 34 [25600/50000 ( 51%)]  Loss: 0.935224\n",
            "Train Epoch: 34 [38400/50000 ( 77%)]  Loss: 0.847322\n",
            "Test set: Average loss: 0.0080, Accuracy: 31620/50000 (63.24%)\n",
            "Test set: Average loss: 0.0104, Accuracy: 6257/10000 (62.57%)\n",
            "Train Epoch: 35 [    0/50000 (  0%)]  Loss: 1.125979\n",
            "Train Epoch: 35 [12800/50000 ( 26%)]  Loss: 0.896937\n",
            "Train Epoch: 35 [25600/50000 ( 51%)]  Loss: 1.113207\n",
            "Train Epoch: 35 [38400/50000 ( 77%)]  Loss: 1.074763\n",
            "Test set: Average loss: 0.0080, Accuracy: 31693/50000 (63.39%)\n",
            "Test set: Average loss: 0.0105, Accuracy: 6262/10000 (62.62%)\n",
            "Train Epoch: 36 [    0/50000 (  0%)]  Loss: 1.154118\n",
            "Train Epoch: 36 [12800/50000 ( 26%)]  Loss: 1.107948\n",
            "Train Epoch: 36 [25600/50000 ( 51%)]  Loss: 0.900030\n",
            "Train Epoch: 36 [38400/50000 ( 77%)]  Loss: 0.792250\n",
            "Test set: Average loss: 0.0078, Accuracy: 32131/50000 (64.26%)\n",
            "Test set: Average loss: 0.0101, Accuracy: 6400/10000 (64.00%)\n",
            "Train Epoch: 37 [    0/50000 (  0%)]  Loss: 0.939198\n",
            "Train Epoch: 37 [12800/50000 ( 26%)]  Loss: 0.922562\n",
            "Train Epoch: 37 [25600/50000 ( 51%)]  Loss: 1.098111\n",
            "Train Epoch: 37 [38400/50000 ( 77%)]  Loss: 1.029918\n",
            "Test set: Average loss: 0.0078, Accuracy: 31837/50000 (63.67%)\n",
            "Test set: Average loss: 0.0103, Accuracy: 6338/10000 (63.38%)\n",
            "Train Epoch: 38 [    0/50000 (  0%)]  Loss: 1.086030\n",
            "Train Epoch: 38 [12800/50000 ( 26%)]  Loss: 1.034007\n",
            "Train Epoch: 38 [25600/50000 ( 51%)]  Loss: 0.924479\n",
            "Train Epoch: 38 [38400/50000 ( 77%)]  Loss: 0.993408\n",
            "Test set: Average loss: 0.0076, Accuracy: 32465/50000 (64.93%)\n",
            "Test set: Average loss: 0.0099, Accuracy: 6416/10000 (64.16%)\n",
            "Train Epoch: 39 [    0/50000 (  0%)]  Loss: 0.984032\n",
            "Train Epoch: 39 [12800/50000 ( 26%)]  Loss: 0.765288\n",
            "Train Epoch: 39 [25600/50000 ( 51%)]  Loss: 0.846425\n",
            "Train Epoch: 39 [38400/50000 ( 77%)]  Loss: 0.963734\n",
            "Test set: Average loss: 0.0075, Accuracy: 32848/50000 (65.70%)\n",
            "Test set: Average loss: 0.0098, Accuracy: 6492/10000 (64.92%)\n",
            "Train Epoch: 40 [    0/50000 (  0%)]  Loss: 0.842517\n",
            "Train Epoch: 40 [12800/50000 ( 26%)]  Loss: 1.206553\n",
            "Train Epoch: 40 [25600/50000 ( 51%)]  Loss: 0.722765\n",
            "Train Epoch: 40 [38400/50000 ( 77%)]  Loss: 0.957747\n",
            "Test set: Average loss: 0.0075, Accuracy: 32674/50000 (65.35%)\n",
            "Test set: Average loss: 0.0099, Accuracy: 6460/10000 (64.60%)\n",
            "Train Epoch: 41 [    0/50000 (  0%)]  Loss: 0.975743\n",
            "Train Epoch: 41 [12800/50000 ( 26%)]  Loss: 0.942601\n",
            "Train Epoch: 41 [25600/50000 ( 51%)]  Loss: 1.083797\n",
            "Train Epoch: 41 [38400/50000 ( 77%)]  Loss: 0.941084\n",
            "Test set: Average loss: 0.0073, Accuracy: 33127/50000 (66.25%)\n",
            "Test set: Average loss: 0.0098, Accuracy: 6526/10000 (65.26%)\n",
            "Train Epoch: 42 [    0/50000 (  0%)]  Loss: 0.865483\n",
            "Train Epoch: 42 [12800/50000 ( 26%)]  Loss: 0.819256\n",
            "Train Epoch: 42 [25600/50000 ( 51%)]  Loss: 0.935782\n",
            "Train Epoch: 42 [38400/50000 ( 77%)]  Loss: 0.900445\n",
            "Test set: Average loss: 0.0072, Accuracy: 33473/50000 (66.95%)\n",
            "Test set: Average loss: 0.0095, Accuracy: 6629/10000 (66.29%)\n",
            "Train Epoch: 43 [    0/50000 (  0%)]  Loss: 0.819953\n",
            "Train Epoch: 43 [12800/50000 ( 26%)]  Loss: 0.911286\n",
            "Train Epoch: 43 [25600/50000 ( 51%)]  Loss: 1.043543\n",
            "Train Epoch: 43 [38400/50000 ( 77%)]  Loss: 0.889793\n",
            "Test set: Average loss: 0.0072, Accuracy: 33402/50000 (66.80%)\n",
            "Test set: Average loss: 0.0095, Accuracy: 6628/10000 (66.28%)\n",
            "Train Epoch: 44 [    0/50000 (  0%)]  Loss: 0.884977\n",
            "Train Epoch: 44 [12800/50000 ( 26%)]  Loss: 0.763115\n",
            "Train Epoch: 44 [25600/50000 ( 51%)]  Loss: 1.014544\n",
            "Train Epoch: 44 [38400/50000 ( 77%)]  Loss: 0.952741\n",
            "Test set: Average loss: 0.0072, Accuracy: 33366/50000 (66.73%)\n",
            "Test set: Average loss: 0.0095, Accuracy: 6552/10000 (65.52%)\n",
            "Train Epoch: 45 [    0/50000 (  0%)]  Loss: 0.924599\n",
            "Train Epoch: 45 [12800/50000 ( 26%)]  Loss: 0.868236\n",
            "Train Epoch: 45 [25600/50000 ( 51%)]  Loss: 0.808320\n",
            "Train Epoch: 45 [38400/50000 ( 77%)]  Loss: 0.960074\n",
            "Test set: Average loss: 0.0070, Accuracy: 33955/50000 (67.91%)\n",
            "Test set: Average loss: 0.0095, Accuracy: 6605/10000 (66.05%)\n",
            "Train Epoch: 46 [    0/50000 (  0%)]  Loss: 1.001275\n",
            "Train Epoch: 46 [12800/50000 ( 26%)]  Loss: 0.842948\n",
            "Train Epoch: 46 [25600/50000 ( 51%)]  Loss: 0.896099\n",
            "Train Epoch: 46 [38400/50000 ( 77%)]  Loss: 0.978222\n",
            "Test set: Average loss: 0.0071, Accuracy: 33627/50000 (67.25%)\n",
            "Test set: Average loss: 0.0096, Accuracy: 6612/10000 (66.12%)\n",
            "Train Epoch: 47 [    0/50000 (  0%)]  Loss: 0.908340\n",
            "Train Epoch: 47 [12800/50000 ( 26%)]  Loss: 0.930097\n",
            "Train Epoch: 47 [25600/50000 ( 51%)]  Loss: 0.906792\n",
            "Train Epoch: 47 [38400/50000 ( 77%)]  Loss: 0.866013\n",
            "Test set: Average loss: 0.0070, Accuracy: 33877/50000 (67.75%)\n",
            "Test set: Average loss: 0.0094, Accuracy: 6653/10000 (66.53%)\n",
            "Train Epoch: 48 [    0/50000 (  0%)]  Loss: 1.059336\n",
            "Train Epoch: 48 [12800/50000 ( 26%)]  Loss: 0.895386\n",
            "Train Epoch: 48 [25600/50000 ( 51%)]  Loss: 1.168765\n",
            "Train Epoch: 48 [38400/50000 ( 77%)]  Loss: 0.737241\n",
            "Test set: Average loss: 0.0069, Accuracy: 34146/50000 (68.29%)\n",
            "Test set: Average loss: 0.0091, Accuracy: 6764/10000 (67.64%)\n",
            "Train Epoch: 49 [    0/50000 (  0%)]  Loss: 0.918686\n",
            "Train Epoch: 49 [12800/50000 ( 26%)]  Loss: 0.874617\n",
            "Train Epoch: 49 [25600/50000 ( 51%)]  Loss: 0.799262\n",
            "Train Epoch: 49 [38400/50000 ( 77%)]  Loss: 0.973922\n",
            "Test set: Average loss: 0.0068, Accuracy: 34351/50000 (68.70%)\n",
            "Test set: Average loss: 0.0093, Accuracy: 6745/10000 (67.45%)\n",
            "Train Epoch: 50 [    0/50000 (  0%)]  Loss: 0.810578\n",
            "Train Epoch: 50 [12800/50000 ( 26%)]  Loss: 0.892427\n",
            "Train Epoch: 50 [25600/50000 ( 51%)]  Loss: 0.922932\n",
            "Train Epoch: 50 [38400/50000 ( 77%)]  Loss: 0.786872\n",
            "Test set: Average loss: 0.0068, Accuracy: 34452/50000 (68.90%)\n",
            "Test set: Average loss: 0.0092, Accuracy: 6697/10000 (66.97%)\n",
            "Train Epoch: 51 [    0/50000 (  0%)]  Loss: 0.788123\n",
            "Train Epoch: 51 [12800/50000 ( 26%)]  Loss: 0.781829\n",
            "Train Epoch: 51 [25600/50000 ( 51%)]  Loss: 0.809952\n",
            "Train Epoch: 51 [38400/50000 ( 77%)]  Loss: 1.008074\n",
            "Test set: Average loss: 0.0066, Accuracy: 34987/50000 (69.97%)\n",
            "Test set: Average loss: 0.0088, Accuracy: 6836/10000 (68.36%)\n",
            "Train Epoch: 52 [    0/50000 (  0%)]  Loss: 0.833812\n",
            "Train Epoch: 52 [12800/50000 ( 26%)]  Loss: 0.841868\n",
            "Train Epoch: 52 [25600/50000 ( 51%)]  Loss: 0.843695\n",
            "Train Epoch: 52 [38400/50000 ( 77%)]  Loss: 0.990689\n",
            "Test set: Average loss: 0.0066, Accuracy: 34894/50000 (69.79%)\n",
            "Test set: Average loss: 0.0089, Accuracy: 6806/10000 (68.06%)\n",
            "Train Epoch: 53 [    0/50000 (  0%)]  Loss: 1.139780\n",
            "Train Epoch: 53 [12800/50000 ( 26%)]  Loss: 0.905919\n",
            "Train Epoch: 53 [25600/50000 ( 51%)]  Loss: 0.792922\n",
            "Train Epoch: 53 [38400/50000 ( 77%)]  Loss: 1.076402\n",
            "Test set: Average loss: 0.0067, Accuracy: 34697/50000 (69.39%)\n",
            "Test set: Average loss: 0.0090, Accuracy: 6809/10000 (68.09%)\n",
            "Train Epoch: 54 [    0/50000 (  0%)]  Loss: 0.747418\n",
            "Train Epoch: 54 [12800/50000 ( 26%)]  Loss: 0.841525\n",
            "Train Epoch: 54 [25600/50000 ( 51%)]  Loss: 0.778431\n",
            "Train Epoch: 54 [38400/50000 ( 77%)]  Loss: 0.992141\n",
            "Test set: Average loss: 0.0066, Accuracy: 34968/50000 (69.94%)\n",
            "Test set: Average loss: 0.0089, Accuracy: 6786/10000 (67.86%)\n",
            "Train Epoch: 55 [    0/50000 (  0%)]  Loss: 0.953449\n",
            "Train Epoch: 55 [12800/50000 ( 26%)]  Loss: 0.855699\n",
            "Train Epoch: 55 [25600/50000 ( 51%)]  Loss: 0.846838\n",
            "Train Epoch: 55 [38400/50000 ( 77%)]  Loss: 0.719946\n",
            "Test set: Average loss: 0.0064, Accuracy: 35155/50000 (70.31%)\n",
            "Test set: Average loss: 0.0088, Accuracy: 6880/10000 (68.80%)\n",
            "Train Epoch: 56 [    0/50000 (  0%)]  Loss: 0.756909\n",
            "Train Epoch: 56 [12800/50000 ( 26%)]  Loss: 0.808928\n",
            "Train Epoch: 56 [25600/50000 ( 51%)]  Loss: 0.696631\n",
            "Train Epoch: 56 [38400/50000 ( 77%)]  Loss: 0.916420\n",
            "Test set: Average loss: 0.0065, Accuracy: 35036/50000 (70.07%)\n",
            "Test set: Average loss: 0.0090, Accuracy: 6794/10000 (67.94%)\n",
            "Train Epoch: 57 [    0/50000 (  0%)]  Loss: 0.870566\n",
            "Train Epoch: 57 [12800/50000 ( 26%)]  Loss: 0.642471\n",
            "Train Epoch: 57 [25600/50000 ( 51%)]  Loss: 0.798714\n",
            "Train Epoch: 57 [38400/50000 ( 77%)]  Loss: 0.718705\n",
            "Test set: Average loss: 0.0065, Accuracy: 35072/50000 (70.14%)\n",
            "Test set: Average loss: 0.0088, Accuracy: 6897/10000 (68.97%)\n",
            "Train Epoch: 58 [    0/50000 (  0%)]  Loss: 0.969437\n",
            "Train Epoch: 58 [12800/50000 ( 26%)]  Loss: 0.777383\n",
            "Train Epoch: 58 [25600/50000 ( 51%)]  Loss: 0.778349\n",
            "Train Epoch: 58 [38400/50000 ( 77%)]  Loss: 0.867642\n",
            "Test set: Average loss: 0.0064, Accuracy: 35361/50000 (70.72%)\n",
            "Test set: Average loss: 0.0087, Accuracy: 6955/10000 (69.55%)\n",
            "Train Epoch: 59 [    0/50000 (  0%)]  Loss: 0.779904\n",
            "Train Epoch: 59 [12800/50000 ( 26%)]  Loss: 0.674586\n",
            "Train Epoch: 59 [25600/50000 ( 51%)]  Loss: 0.753563\n",
            "Train Epoch: 59 [38400/50000 ( 77%)]  Loss: 0.951212\n",
            "Test set: Average loss: 0.0063, Accuracy: 35547/50000 (71.09%)\n",
            "Test set: Average loss: 0.0086, Accuracy: 6969/10000 (69.69%)\n",
            "Train Epoch: 60 [    0/50000 (  0%)]  Loss: 0.822766\n",
            "Train Epoch: 60 [12800/50000 ( 26%)]  Loss: 0.814429\n",
            "Train Epoch: 60 [25600/50000 ( 51%)]  Loss: 0.876597\n",
            "Train Epoch: 60 [38400/50000 ( 77%)]  Loss: 0.822210\n",
            "Test set: Average loss: 0.0063, Accuracy: 35386/50000 (70.77%)\n",
            "Test set: Average loss: 0.0088, Accuracy: 6932/10000 (69.32%)\n",
            "Train Epoch: 61 [    0/50000 (  0%)]  Loss: 0.981887\n",
            "Train Epoch: 61 [12800/50000 ( 26%)]  Loss: 0.802337\n",
            "Train Epoch: 61 [25600/50000 ( 51%)]  Loss: 0.730221\n",
            "Train Epoch: 61 [38400/50000 ( 77%)]  Loss: 0.750378\n",
            "Test set: Average loss: 0.0062, Accuracy: 35911/50000 (71.82%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7021/10000 (70.21%)\n",
            "Train Epoch: 62 [    0/50000 (  0%)]  Loss: 0.788700\n",
            "Train Epoch: 62 [12800/50000 ( 26%)]  Loss: 0.877453\n",
            "Train Epoch: 62 [25600/50000 ( 51%)]  Loss: 0.674212\n",
            "Train Epoch: 62 [38400/50000 ( 77%)]  Loss: 0.761544\n",
            "Test set: Average loss: 0.0062, Accuracy: 35689/50000 (71.38%)\n",
            "Test set: Average loss: 0.0086, Accuracy: 6971/10000 (69.71%)\n",
            "Train Epoch: 63 [    0/50000 (  0%)]  Loss: 0.677213\n",
            "Train Epoch: 63 [12800/50000 ( 26%)]  Loss: 0.829079\n",
            "Train Epoch: 63 [25600/50000 ( 51%)]  Loss: 1.009989\n",
            "Train Epoch: 63 [38400/50000 ( 77%)]  Loss: 0.790547\n",
            "Test set: Average loss: 0.0061, Accuracy: 36095/50000 (72.19%)\n",
            "Test set: Average loss: 0.0085, Accuracy: 7004/10000 (70.04%)\n",
            "Train Epoch: 64 [    0/50000 (  0%)]  Loss: 0.846094\n",
            "Train Epoch: 64 [12800/50000 ( 26%)]  Loss: 1.003696\n",
            "Train Epoch: 64 [25600/50000 ( 51%)]  Loss: 0.829890\n",
            "Train Epoch: 64 [38400/50000 ( 77%)]  Loss: 0.728306\n",
            "Test set: Average loss: 0.0060, Accuracy: 36227/50000 (72.45%)\n",
            "Test set: Average loss: 0.0085, Accuracy: 6983/10000 (69.83%)\n",
            "Train Epoch: 65 [    0/50000 (  0%)]  Loss: 0.913190\n",
            "Train Epoch: 65 [12800/50000 ( 26%)]  Loss: 0.689348\n",
            "Train Epoch: 65 [25600/50000 ( 51%)]  Loss: 0.767358\n",
            "Train Epoch: 65 [38400/50000 ( 77%)]  Loss: 0.751350\n",
            "Test set: Average loss: 0.0060, Accuracy: 36220/50000 (72.44%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7022/10000 (70.22%)\n",
            "Train Epoch: 66 [    0/50000 (  0%)]  Loss: 0.650678\n",
            "Train Epoch: 66 [12800/50000 ( 26%)]  Loss: 0.715467\n",
            "Train Epoch: 66 [25600/50000 ( 51%)]  Loss: 0.701874\n",
            "Train Epoch: 66 [38400/50000 ( 77%)]  Loss: 0.719962\n",
            "Test set: Average loss: 0.0059, Accuracy: 36429/50000 (72.86%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7073/10000 (70.73%)\n",
            "Train Epoch: 67 [    0/50000 (  0%)]  Loss: 0.600130\n",
            "Train Epoch: 67 [12800/50000 ( 26%)]  Loss: 0.659857\n",
            "Train Epoch: 67 [25600/50000 ( 51%)]  Loss: 0.744687\n",
            "Train Epoch: 67 [38400/50000 ( 77%)]  Loss: 0.711887\n",
            "Test set: Average loss: 0.0059, Accuracy: 36342/50000 (72.68%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7022/10000 (70.22%)\n",
            "Train Epoch: 68 [    0/50000 (  0%)]  Loss: 0.755040\n",
            "Train Epoch: 68 [12800/50000 ( 26%)]  Loss: 0.824139\n",
            "Train Epoch: 68 [25600/50000 ( 51%)]  Loss: 0.800633\n",
            "Train Epoch: 68 [38400/50000 ( 77%)]  Loss: 0.715227\n",
            "Test set: Average loss: 0.0060, Accuracy: 36286/50000 (72.57%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7037/10000 (70.37%)\n",
            "Train Epoch: 69 [    0/50000 (  0%)]  Loss: 0.811089\n",
            "Train Epoch: 69 [12800/50000 ( 26%)]  Loss: 0.634968\n",
            "Train Epoch: 69 [25600/50000 ( 51%)]  Loss: 0.747979\n",
            "Train Epoch: 69 [38400/50000 ( 77%)]  Loss: 0.695598\n",
            "Test set: Average loss: 0.0059, Accuracy: 36405/50000 (72.81%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7029/10000 (70.29%)\n",
            "Train Epoch: 70 [    0/50000 (  0%)]  Loss: 0.738456\n",
            "Train Epoch: 70 [12800/50000 ( 26%)]  Loss: 0.786498\n",
            "Train Epoch: 70 [25600/50000 ( 51%)]  Loss: 0.793684\n",
            "Train Epoch: 70 [38400/50000 ( 77%)]  Loss: 0.750555\n",
            "Test set: Average loss: 0.0059, Accuracy: 36548/50000 (73.10%)\n",
            "Test set: Average loss: 0.0083, Accuracy: 7080/10000 (70.80%)\n",
            "Train Epoch: 71 [    0/50000 (  0%)]  Loss: 0.715020\n",
            "Train Epoch: 71 [12800/50000 ( 26%)]  Loss: 0.592682\n",
            "Train Epoch: 71 [25600/50000 ( 51%)]  Loss: 0.828384\n",
            "Train Epoch: 71 [38400/50000 ( 77%)]  Loss: 0.771863\n",
            "Test set: Average loss: 0.0058, Accuracy: 36801/50000 (73.60%)\n",
            "Test set: Average loss: 0.0082, Accuracy: 7119/10000 (71.19%)\n",
            "Train Epoch: 72 [    0/50000 (  0%)]  Loss: 0.639313\n",
            "Train Epoch: 72 [12800/50000 ( 26%)]  Loss: 0.741048\n",
            "Train Epoch: 72 [25600/50000 ( 51%)]  Loss: 0.869652\n",
            "Train Epoch: 72 [38400/50000 ( 77%)]  Loss: 0.879826\n",
            "Test set: Average loss: 0.0057, Accuracy: 36819/50000 (73.64%)\n",
            "Test set: Average loss: 0.0082, Accuracy: 7122/10000 (71.22%)\n",
            "Train Epoch: 73 [    0/50000 (  0%)]  Loss: 0.568591\n",
            "Train Epoch: 73 [12800/50000 ( 26%)]  Loss: 0.833378\n",
            "Train Epoch: 73 [25600/50000 ( 51%)]  Loss: 0.685544\n",
            "Train Epoch: 73 [38400/50000 ( 77%)]  Loss: 0.672548\n",
            "Test set: Average loss: 0.0059, Accuracy: 36533/50000 (73.07%)\n",
            "Test set: Average loss: 0.0082, Accuracy: 7096/10000 (70.96%)\n",
            "Train Epoch: 74 [    0/50000 (  0%)]  Loss: 0.847215\n",
            "Train Epoch: 74 [12800/50000 ( 26%)]  Loss: 0.899312\n",
            "Train Epoch: 74 [25600/50000 ( 51%)]  Loss: 0.694371\n",
            "Train Epoch: 74 [38400/50000 ( 77%)]  Loss: 0.680936\n",
            "Test set: Average loss: 0.0057, Accuracy: 36905/50000 (73.81%)\n",
            "Test set: Average loss: 0.0081, Accuracy: 7133/10000 (71.33%)\n",
            "Train Epoch: 75 [    0/50000 (  0%)]  Loss: 0.686790\n",
            "Train Epoch: 75 [12800/50000 ( 26%)]  Loss: 0.840476\n",
            "Train Epoch: 75 [25600/50000 ( 51%)]  Loss: 0.692865\n",
            "Train Epoch: 75 [38400/50000 ( 77%)]  Loss: 0.727558\n",
            "Test set: Average loss: 0.0058, Accuracy: 36499/50000 (73.00%)\n",
            "Test set: Average loss: 0.0084, Accuracy: 7072/10000 (70.72%)\n",
            "Train Epoch: 76 [    0/50000 (  0%)]  Loss: 0.816125\n",
            "Train Epoch: 76 [12800/50000 ( 26%)]  Loss: 0.631453\n",
            "Train Epoch: 76 [25600/50000 ( 51%)]  Loss: 0.694661\n",
            "Train Epoch: 76 [38400/50000 ( 77%)]  Loss: 0.781163\n",
            "Test set: Average loss: 0.0056, Accuracy: 37235/50000 (74.47%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7204/10000 (72.04%)\n",
            "Train Epoch: 77 [    0/50000 (  0%)]  Loss: 0.820922\n",
            "Train Epoch: 77 [12800/50000 ( 26%)]  Loss: 0.603882\n",
            "Train Epoch: 77 [25600/50000 ( 51%)]  Loss: 0.887689\n",
            "Train Epoch: 77 [38400/50000 ( 77%)]  Loss: 0.861121\n",
            "Test set: Average loss: 0.0056, Accuracy: 37142/50000 (74.28%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7191/10000 (71.91%)\n",
            "Train Epoch: 78 [    0/50000 (  0%)]  Loss: 0.786352\n",
            "Train Epoch: 78 [12800/50000 ( 26%)]  Loss: 0.579184\n",
            "Train Epoch: 78 [25600/50000 ( 51%)]  Loss: 0.726210\n",
            "Train Epoch: 78 [38400/50000 ( 77%)]  Loss: 0.937454\n",
            "Test set: Average loss: 0.0055, Accuracy: 37328/50000 (74.66%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7198/10000 (71.98%)\n",
            "Train Epoch: 79 [    0/50000 (  0%)]  Loss: 0.782296\n",
            "Train Epoch: 79 [12800/50000 ( 26%)]  Loss: 0.630468\n",
            "Train Epoch: 79 [25600/50000 ( 51%)]  Loss: 0.866368\n",
            "Train Epoch: 79 [38400/50000 ( 77%)]  Loss: 0.625929\n",
            "Test set: Average loss: 0.0055, Accuracy: 37195/50000 (74.39%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7211/10000 (72.11%)\n",
            "Train Epoch: 80 [    0/50000 (  0%)]  Loss: 0.707117\n",
            "Train Epoch: 80 [12800/50000 ( 26%)]  Loss: 0.703387\n",
            "Train Epoch: 80 [25600/50000 ( 51%)]  Loss: 0.698016\n",
            "Train Epoch: 80 [38400/50000 ( 77%)]  Loss: 0.569855\n",
            "Test set: Average loss: 0.0055, Accuracy: 37437/50000 (74.87%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7186/10000 (71.86%)\n",
            "Train Epoch: 81 [    0/50000 (  0%)]  Loss: 0.824515\n",
            "Train Epoch: 81 [12800/50000 ( 26%)]  Loss: 0.582184\n",
            "Train Epoch: 81 [25600/50000 ( 51%)]  Loss: 0.668021\n",
            "Train Epoch: 81 [38400/50000 ( 77%)]  Loss: 0.624704\n",
            "Test set: Average loss: 0.0055, Accuracy: 37304/50000 (74.61%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7172/10000 (71.72%)\n",
            "Train Epoch: 82 [    0/50000 (  0%)]  Loss: 0.845340\n",
            "Train Epoch: 82 [12800/50000 ( 26%)]  Loss: 0.669011\n",
            "Train Epoch: 82 [25600/50000 ( 51%)]  Loss: 0.850643\n",
            "Train Epoch: 82 [38400/50000 ( 77%)]  Loss: 0.693516\n",
            "Test set: Average loss: 0.0054, Accuracy: 37485/50000 (74.97%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7202/10000 (72.02%)\n",
            "Train Epoch: 83 [    0/50000 (  0%)]  Loss: 0.738805\n",
            "Train Epoch: 83 [12800/50000 ( 26%)]  Loss: 0.694247\n",
            "Train Epoch: 83 [25600/50000 ( 51%)]  Loss: 0.669558\n",
            "Train Epoch: 83 [38400/50000 ( 77%)]  Loss: 0.814142\n",
            "Test set: Average loss: 0.0055, Accuracy: 37411/50000 (74.82%)\n",
            "Test set: Average loss: 0.0081, Accuracy: 7180/10000 (71.80%)\n",
            "Train Epoch: 84 [    0/50000 (  0%)]  Loss: 0.785792\n",
            "Train Epoch: 84 [12800/50000 ( 26%)]  Loss: 0.609877\n",
            "Train Epoch: 84 [25600/50000 ( 51%)]  Loss: 0.654855\n",
            "Train Epoch: 84 [38400/50000 ( 77%)]  Loss: 0.648900\n",
            "Test set: Average loss: 0.0054, Accuracy: 37634/50000 (75.27%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7230/10000 (72.30%)\n",
            "Train Epoch: 85 [    0/50000 (  0%)]  Loss: 0.628423\n",
            "Train Epoch: 85 [12800/50000 ( 26%)]  Loss: 0.742697\n",
            "Train Epoch: 85 [25600/50000 ( 51%)]  Loss: 0.632932\n",
            "Train Epoch: 85 [38400/50000 ( 77%)]  Loss: 0.607505\n",
            "Test set: Average loss: 0.0053, Accuracy: 37713/50000 (75.43%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7213/10000 (72.13%)\n",
            "Train Epoch: 86 [    0/50000 (  0%)]  Loss: 0.841049\n",
            "Train Epoch: 86 [12800/50000 ( 26%)]  Loss: 0.856632\n",
            "Train Epoch: 86 [25600/50000 ( 51%)]  Loss: 0.689631\n",
            "Train Epoch: 86 [38400/50000 ( 77%)]  Loss: 0.561155\n",
            "Test set: Average loss: 0.0054, Accuracy: 37617/50000 (75.23%)\n",
            "Test set: Average loss: 0.0081, Accuracy: 7210/10000 (72.10%)\n",
            "Train Epoch: 87 [    0/50000 (  0%)]  Loss: 0.654141\n",
            "Train Epoch: 87 [12800/50000 ( 26%)]  Loss: 0.540192\n",
            "Train Epoch: 87 [25600/50000 ( 51%)]  Loss: 0.718795\n",
            "Train Epoch: 87 [38400/50000 ( 77%)]  Loss: 0.769458\n",
            "Test set: Average loss: 0.0053, Accuracy: 37653/50000 (75.31%)\n",
            "Test set: Average loss: 0.0079, Accuracy: 7264/10000 (72.64%)\n",
            "Train Epoch: 88 [    0/50000 (  0%)]  Loss: 0.702049\n",
            "Train Epoch: 88 [12800/50000 ( 26%)]  Loss: 0.757044\n",
            "Train Epoch: 88 [25600/50000 ( 51%)]  Loss: 0.905247\n",
            "Train Epoch: 88 [38400/50000 ( 77%)]  Loss: 0.590373\n",
            "Test set: Average loss: 0.0053, Accuracy: 37778/50000 (75.56%)\n",
            "Test set: Average loss: 0.0078, Accuracy: 7296/10000 (72.96%)\n",
            "Train Epoch: 89 [    0/50000 (  0%)]  Loss: 0.793552\n",
            "Train Epoch: 89 [12800/50000 ( 26%)]  Loss: 0.691118\n",
            "Train Epoch: 89 [25600/50000 ( 51%)]  Loss: 0.740410\n",
            "Train Epoch: 89 [38400/50000 ( 77%)]  Loss: 0.762243\n",
            "Test set: Average loss: 0.0055, Accuracy: 37421/50000 (74.84%)\n",
            "Test set: Average loss: 0.0081, Accuracy: 7205/10000 (72.05%)\n",
            "Train Epoch: 90 [    0/50000 (  0%)]  Loss: 0.754336\n",
            "Train Epoch: 90 [12800/50000 ( 26%)]  Loss: 0.675810\n",
            "Train Epoch: 90 [25600/50000 ( 51%)]  Loss: 0.632930\n",
            "Train Epoch: 90 [38400/50000 ( 77%)]  Loss: 0.571102\n",
            "Test set: Average loss: 0.0052, Accuracy: 38103/50000 (76.21%)\n",
            "Test set: Average loss: 0.0077, Accuracy: 7354/10000 (73.54%)\n",
            "Train Epoch: 91 [    0/50000 (  0%)]  Loss: 0.617657\n",
            "Train Epoch: 91 [12800/50000 ( 26%)]  Loss: 0.646391\n",
            "Train Epoch: 91 [25600/50000 ( 51%)]  Loss: 0.835319\n",
            "Train Epoch: 91 [38400/50000 ( 77%)]  Loss: 0.704880\n",
            "Test set: Average loss: 0.0050, Accuracy: 38405/50000 (76.81%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7373/10000 (73.73%)\n",
            "Train Epoch: 92 [    0/50000 (  0%)]  Loss: 0.652154\n",
            "Train Epoch: 92 [12800/50000 ( 26%)]  Loss: 0.833919\n",
            "Train Epoch: 92 [25600/50000 ( 51%)]  Loss: 0.718855\n",
            "Train Epoch: 92 [38400/50000 ( 77%)]  Loss: 0.714146\n",
            "Test set: Average loss: 0.0052, Accuracy: 38092/50000 (76.18%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7291/10000 (72.91%)\n",
            "Train Epoch: 93 [    0/50000 (  0%)]  Loss: 0.719532\n",
            "Train Epoch: 93 [12800/50000 ( 26%)]  Loss: 0.564883\n",
            "Train Epoch: 93 [25600/50000 ( 51%)]  Loss: 0.792610\n",
            "Train Epoch: 93 [38400/50000 ( 77%)]  Loss: 0.722451\n",
            "Test set: Average loss: 0.0053, Accuracy: 37961/50000 (75.92%)\n",
            "Test set: Average loss: 0.0079, Accuracy: 7285/10000 (72.85%)\n",
            "Train Epoch: 94 [    0/50000 (  0%)]  Loss: 0.599631\n",
            "Train Epoch: 94 [12800/50000 ( 26%)]  Loss: 0.671679\n",
            "Train Epoch: 94 [25600/50000 ( 51%)]  Loss: 0.720968\n",
            "Train Epoch: 94 [38400/50000 ( 77%)]  Loss: 0.627942\n",
            "Test set: Average loss: 0.0051, Accuracy: 38251/50000 (76.50%)\n",
            "Test set: Average loss: 0.0079, Accuracy: 7271/10000 (72.71%)\n",
            "Train Epoch: 95 [    0/50000 (  0%)]  Loss: 0.498860\n",
            "Train Epoch: 95 [12800/50000 ( 26%)]  Loss: 0.782391\n",
            "Train Epoch: 95 [25600/50000 ( 51%)]  Loss: 0.807922\n",
            "Train Epoch: 95 [38400/50000 ( 77%)]  Loss: 0.603226\n",
            "Test set: Average loss: 0.0053, Accuracy: 37908/50000 (75.82%)\n",
            "Test set: Average loss: 0.0080, Accuracy: 7322/10000 (73.22%)\n",
            "Train Epoch: 96 [    0/50000 (  0%)]  Loss: 0.636114\n",
            "Train Epoch: 96 [12800/50000 ( 26%)]  Loss: 0.604124\n",
            "Train Epoch: 96 [25600/50000 ( 51%)]  Loss: 0.867454\n",
            "Train Epoch: 96 [38400/50000 ( 77%)]  Loss: 0.839027\n",
            "Test set: Average loss: 0.0051, Accuracy: 38300/50000 (76.60%)\n",
            "Test set: Average loss: 0.0078, Accuracy: 7324/10000 (73.24%)\n",
            "Train Epoch: 97 [    0/50000 (  0%)]  Loss: 0.644602\n",
            "Train Epoch: 97 [12800/50000 ( 26%)]  Loss: 0.659028\n",
            "Train Epoch: 97 [25600/50000 ( 51%)]  Loss: 0.814966\n",
            "Train Epoch: 97 [38400/50000 ( 77%)]  Loss: 0.773483\n",
            "Test set: Average loss: 0.0050, Accuracy: 38571/50000 (77.14%)\n",
            "Test set: Average loss: 0.0078, Accuracy: 7356/10000 (73.56%)\n",
            "Train Epoch: 98 [    0/50000 (  0%)]  Loss: 0.685732\n",
            "Train Epoch: 98 [12800/50000 ( 26%)]  Loss: 0.627003\n",
            "Train Epoch: 98 [25600/50000 ( 51%)]  Loss: 0.839767\n",
            "Train Epoch: 98 [38400/50000 ( 77%)]  Loss: 0.709126\n",
            "Test set: Average loss: 0.0049, Accuracy: 38604/50000 (77.21%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7408/10000 (74.08%)\n",
            "Train Epoch: 99 [    0/50000 (  0%)]  Loss: 0.580115\n",
            "Train Epoch: 99 [12800/50000 ( 26%)]  Loss: 0.515388\n",
            "Train Epoch: 99 [25600/50000 ( 51%)]  Loss: 0.597328\n",
            "Train Epoch: 99 [38400/50000 ( 77%)]  Loss: 0.562405\n",
            "Test set: Average loss: 0.0049, Accuracy: 38682/50000 (77.36%)\n",
            "Test set: Average loss: 0.0077, Accuracy: 7369/10000 (73.69%)\n",
            "Train Epoch: 100 [    0/50000 (  0%)]  Loss: 0.545965\n",
            "Train Epoch: 100 [12800/50000 ( 26%)]  Loss: 0.535453\n",
            "Train Epoch: 100 [25600/50000 ( 51%)]  Loss: 0.527318\n",
            "Train Epoch: 100 [38400/50000 ( 77%)]  Loss: 0.466942\n",
            "Test set: Average loss: 0.0049, Accuracy: 38632/50000 (77.26%)\n",
            "Test set: Average loss: 0.0078, Accuracy: 7274/10000 (72.74%)\n",
            "Train Epoch: 101 [    0/50000 (  0%)]  Loss: 0.602036\n",
            "Train Epoch: 101 [12800/50000 ( 26%)]  Loss: 0.628634\n",
            "Train Epoch: 101 [25600/50000 ( 51%)]  Loss: 0.585774\n",
            "Train Epoch: 101 [38400/50000 ( 77%)]  Loss: 0.711617\n",
            "Test set: Average loss: 0.0050, Accuracy: 38644/50000 (77.29%)\n",
            "Test set: Average loss: 0.0077, Accuracy: 7332/10000 (73.32%)\n",
            "Train Epoch: 102 [    0/50000 (  0%)]  Loss: 0.680773\n",
            "Train Epoch: 102 [12800/50000 ( 26%)]  Loss: 0.619130\n",
            "Train Epoch: 102 [25600/50000 ( 51%)]  Loss: 0.704774\n",
            "Train Epoch: 102 [38400/50000 ( 77%)]  Loss: 0.631480\n",
            "Test set: Average loss: 0.0049, Accuracy: 38762/50000 (77.52%)\n",
            "Test set: Average loss: 0.0077, Accuracy: 7397/10000 (73.97%)\n",
            "Train Epoch: 103 [    0/50000 (  0%)]  Loss: 0.457964\n",
            "Train Epoch: 103 [12800/50000 ( 26%)]  Loss: 0.497070\n",
            "Train Epoch: 103 [25600/50000 ( 51%)]  Loss: 0.610620\n",
            "Train Epoch: 103 [38400/50000 ( 77%)]  Loss: 0.625475\n",
            "Test set: Average loss: 0.0050, Accuracy: 38575/50000 (77.15%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7367/10000 (73.67%)\n",
            "Train Epoch: 104 [    0/50000 (  0%)]  Loss: 0.452773\n",
            "Train Epoch: 104 [12800/50000 ( 26%)]  Loss: 0.599532\n",
            "Train Epoch: 104 [25600/50000 ( 51%)]  Loss: 0.754923\n",
            "Train Epoch: 104 [38400/50000 ( 77%)]  Loss: 0.762786\n",
            "Test set: Average loss: 0.0048, Accuracy: 38995/50000 (77.99%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7405/10000 (74.05%)\n",
            "Train Epoch: 105 [    0/50000 (  0%)]  Loss: 0.570450\n",
            "Train Epoch: 105 [12800/50000 ( 26%)]  Loss: 0.810042\n",
            "Train Epoch: 105 [25600/50000 ( 51%)]  Loss: 0.760606\n",
            "Train Epoch: 105 [38400/50000 ( 77%)]  Loss: 0.717770\n",
            "Test set: Average loss: 0.0048, Accuracy: 38902/50000 (77.80%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7428/10000 (74.28%)\n",
            "Train Epoch: 106 [    0/50000 (  0%)]  Loss: 0.711546\n",
            "Train Epoch: 106 [12800/50000 ( 26%)]  Loss: 0.627914\n",
            "Train Epoch: 106 [25600/50000 ( 51%)]  Loss: 0.624779\n",
            "Train Epoch: 106 [38400/50000 ( 77%)]  Loss: 0.761772\n",
            "Test set: Average loss: 0.0048, Accuracy: 39054/50000 (78.11%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7446/10000 (74.46%)\n",
            "Train Epoch: 107 [    0/50000 (  0%)]  Loss: 0.659471\n",
            "Train Epoch: 107 [12800/50000 ( 26%)]  Loss: 0.569145\n",
            "Train Epoch: 107 [25600/50000 ( 51%)]  Loss: 0.516469\n",
            "Train Epoch: 107 [38400/50000 ( 77%)]  Loss: 0.738252\n",
            "Test set: Average loss: 0.0047, Accuracy: 39161/50000 (78.32%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7393/10000 (73.93%)\n",
            "Train Epoch: 108 [    0/50000 (  0%)]  Loss: 0.731747\n",
            "Train Epoch: 108 [12800/50000 ( 26%)]  Loss: 0.589211\n",
            "Train Epoch: 108 [25600/50000 ( 51%)]  Loss: 0.690396\n",
            "Train Epoch: 108 [38400/50000 ( 77%)]  Loss: 0.570429\n",
            "Test set: Average loss: 0.0048, Accuracy: 39014/50000 (78.03%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7393/10000 (73.93%)\n",
            "Train Epoch: 109 [    0/50000 (  0%)]  Loss: 0.581922\n",
            "Train Epoch: 109 [12800/50000 ( 26%)]  Loss: 0.777915\n",
            "Train Epoch: 109 [25600/50000 ( 51%)]  Loss: 0.781047\n",
            "Train Epoch: 109 [38400/50000 ( 77%)]  Loss: 0.509990\n",
            "Test set: Average loss: 0.0047, Accuracy: 39224/50000 (78.45%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7396/10000 (73.96%)\n",
            "Train Epoch: 110 [    0/50000 (  0%)]  Loss: 0.712038\n",
            "Train Epoch: 110 [12800/50000 ( 26%)]  Loss: 0.658850\n",
            "Train Epoch: 110 [25600/50000 ( 51%)]  Loss: 0.649656\n",
            "Train Epoch: 110 [38400/50000 ( 77%)]  Loss: 0.809582\n",
            "Test set: Average loss: 0.0048, Accuracy: 39025/50000 (78.05%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7457/10000 (74.57%)\n",
            "Train Epoch: 111 [    0/50000 (  0%)]  Loss: 0.583079\n",
            "Train Epoch: 111 [12800/50000 ( 26%)]  Loss: 0.589024\n",
            "Train Epoch: 111 [25600/50000 ( 51%)]  Loss: 0.666715\n",
            "Train Epoch: 111 [38400/50000 ( 77%)]  Loss: 0.525613\n",
            "Test set: Average loss: 0.0047, Accuracy: 39200/50000 (78.40%)\n",
            "Test set: Average loss: 0.0074, Accuracy: 7431/10000 (74.31%)\n",
            "Train Epoch: 112 [    0/50000 (  0%)]  Loss: 0.572041\n",
            "Train Epoch: 112 [12800/50000 ( 26%)]  Loss: 0.648541\n",
            "Train Epoch: 112 [25600/50000 ( 51%)]  Loss: 0.607381\n",
            "Train Epoch: 112 [38400/50000 ( 77%)]  Loss: 0.640195\n",
            "Test set: Average loss: 0.0047, Accuracy: 39146/50000 (78.29%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7400/10000 (74.00%)\n",
            "Train Epoch: 113 [    0/50000 (  0%)]  Loss: 0.525791\n",
            "Train Epoch: 113 [12800/50000 ( 26%)]  Loss: 0.721197\n",
            "Train Epoch: 113 [25600/50000 ( 51%)]  Loss: 0.493544\n",
            "Train Epoch: 113 [38400/50000 ( 77%)]  Loss: 0.525326\n",
            "Test set: Average loss: 0.0046, Accuracy: 39360/50000 (78.72%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7450/10000 (74.50%)\n",
            "Train Epoch: 114 [    0/50000 (  0%)]  Loss: 0.651524\n",
            "Train Epoch: 114 [12800/50000 ( 26%)]  Loss: 0.577247\n",
            "Train Epoch: 114 [25600/50000 ( 51%)]  Loss: 0.413216\n",
            "Train Epoch: 114 [38400/50000 ( 77%)]  Loss: 0.566035\n",
            "Test set: Average loss: 0.0045, Accuracy: 39531/50000 (79.06%)\n",
            "Test set: Average loss: 0.0073, Accuracy: 7497/10000 (74.97%)\n",
            "Train Epoch: 115 [    0/50000 (  0%)]  Loss: 0.528440\n",
            "Train Epoch: 115 [12800/50000 ( 26%)]  Loss: 0.454679\n",
            "Train Epoch: 115 [25600/50000 ( 51%)]  Loss: 0.505307\n",
            "Train Epoch: 115 [38400/50000 ( 77%)]  Loss: 0.644011\n",
            "Test set: Average loss: 0.0047, Accuracy: 39336/50000 (78.67%)\n",
            "Test set: Average loss: 0.0074, Accuracy: 7473/10000 (74.73%)\n",
            "Train Epoch: 116 [    0/50000 (  0%)]  Loss: 0.750213\n",
            "Train Epoch: 116 [12800/50000 ( 26%)]  Loss: 0.580098\n",
            "Train Epoch: 116 [25600/50000 ( 51%)]  Loss: 0.700989\n",
            "Train Epoch: 116 [38400/50000 ( 77%)]  Loss: 0.432934\n",
            "Test set: Average loss: 0.0047, Accuracy: 39240/50000 (78.48%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7471/10000 (74.71%)\n",
            "Train Epoch: 117 [    0/50000 (  0%)]  Loss: 0.547753\n",
            "Train Epoch: 117 [12800/50000 ( 26%)]  Loss: 0.570680\n",
            "Train Epoch: 117 [25600/50000 ( 51%)]  Loss: 0.608642\n",
            "Train Epoch: 117 [38400/50000 ( 77%)]  Loss: 0.763587\n",
            "Test set: Average loss: 0.0047, Accuracy: 39290/50000 (78.58%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7432/10000 (74.32%)\n",
            "Train Epoch: 118 [    0/50000 (  0%)]  Loss: 0.658640\n",
            "Train Epoch: 118 [12800/50000 ( 26%)]  Loss: 0.710470\n",
            "Train Epoch: 118 [25600/50000 ( 51%)]  Loss: 0.641095\n",
            "Train Epoch: 118 [38400/50000 ( 77%)]  Loss: 0.610103\n",
            "Test set: Average loss: 0.0045, Accuracy: 39687/50000 (79.37%)\n",
            "Test set: Average loss: 0.0074, Accuracy: 7496/10000 (74.96%)\n",
            "Train Epoch: 119 [    0/50000 (  0%)]  Loss: 0.568848\n",
            "Train Epoch: 119 [12800/50000 ( 26%)]  Loss: 0.572851\n",
            "Train Epoch: 119 [25600/50000 ( 51%)]  Loss: 0.619591\n",
            "Train Epoch: 119 [38400/50000 ( 77%)]  Loss: 0.745393\n",
            "Test set: Average loss: 0.0045, Accuracy: 39566/50000 (79.13%)\n",
            "Test set: Average loss: 0.0074, Accuracy: 7480/10000 (74.80%)\n",
            "Train Epoch: 120 [    0/50000 (  0%)]  Loss: 0.442509\n",
            "Train Epoch: 120 [12800/50000 ( 26%)]  Loss: 0.554519\n",
            "Train Epoch: 120 [25600/50000 ( 51%)]  Loss: 0.536726\n",
            "Train Epoch: 120 [38400/50000 ( 77%)]  Loss: 0.540332\n",
            "Test set: Average loss: 0.0047, Accuracy: 39266/50000 (78.53%)\n",
            "Test set: Average loss: 0.0076, Accuracy: 7439/10000 (74.39%)\n",
            "Train Epoch: 121 [    0/50000 (  0%)]  Loss: 0.687032\n",
            "Train Epoch: 121 [12800/50000 ( 26%)]  Loss: 0.718401\n",
            "Train Epoch: 121 [25600/50000 ( 51%)]  Loss: 0.566804\n",
            "Train Epoch: 121 [38400/50000 ( 77%)]  Loss: 0.662450\n",
            "Test set: Average loss: 0.0045, Accuracy: 39656/50000 (79.31%)\n",
            "Test set: Average loss: 0.0074, Accuracy: 7477/10000 (74.77%)\n",
            "Train Epoch: 122 [    0/50000 (  0%)]  Loss: 0.733512\n",
            "Train Epoch: 122 [12800/50000 ( 26%)]  Loss: 0.532898\n",
            "Train Epoch: 122 [25600/50000 ( 51%)]  Loss: 0.544686\n",
            "Train Epoch: 122 [38400/50000 ( 77%)]  Loss: 0.740139\n",
            "Test set: Average loss: 0.0047, Accuracy: 39222/50000 (78.44%)\n",
            "Test set: Average loss: 0.0075, Accuracy: 7419/10000 (74.19%)\n",
            "Train Epoch: 123 [    0/50000 (  0%)]  Loss: 0.571442\n",
            "Train Epoch: 123 [12800/50000 ( 26%)]  Loss: 0.600457\n",
            "Train Epoch: 123 [25600/50000 ( 51%)]  Loss: 0.576145\n",
            "Train Epoch: 123 [38400/50000 ( 77%)]  Loss: 0.497952\n",
            "Test set: Average loss: 0.0046, Accuracy: 39410/50000 (78.82%)\n",
            "Test set: Average loss: 0.0074, Accuracy: 7496/10000 (74.96%)\n",
            "Train Epoch: 124 [    0/50000 (  0%)]  Loss: 0.608788\n",
            "Train Epoch: 124 [12800/50000 ( 26%)]  Loss: 0.576820\n",
            "Train Epoch: 124 [25600/50000 ( 51%)]  Loss: 0.622539\n",
            "Train Epoch: 124 [38400/50000 ( 77%)]  Loss: 0.675217\n",
            "Test set: Average loss: 0.0043, Accuracy: 39940/50000 (79.88%)\n",
            "Test set: Average loss: 0.0074, Accuracy: 7464/10000 (74.64%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXDAzxwFewV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_by_row(logits, T = 1.0):\n",
        "    mx = np.max(logits, axis=-1, keepdims=True)\n",
        "    exp = np.exp((logits - mx)/T)\n",
        "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
        "    return exp/denominator\n",
        "\n",
        "def classifier_performance(model, train_loader, test_loader):\n",
        "\n",
        "    output_train_benign = []\n",
        "    train_label = []\n",
        "    for num, data in enumerate(train_loader):\n",
        "        images,labels = data\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        train_label.append(labels.numpy())\n",
        "        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    train_label = np.concatenate(train_label)\n",
        "    output_train_benign=np.concatenate(output_train_benign)\n",
        "\n",
        "    test_label = []\n",
        "    output_test_benign = []\n",
        "\n",
        "    for num, data in enumerate(test_loader):\n",
        "        images,labels = data\n",
        "\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        test_label.append(labels.numpy())\n",
        "        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    test_label = np.concatenate(test_label)\n",
        "    output_test_benign=np.concatenate(output_test_benign)\n",
        "\n",
        "\n",
        "    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n",
        "    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n",
        "\n",
        "    print('Accuracy: ', (train_acc1, test_acc1))\n",
        "\n",
        "    return output_train_benign, output_test_benign, train_label, test_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
        "    \n",
        "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
        "    confidence1 = []\n",
        "    confidence2 = []\n",
        "    acc1 = 0\n",
        "    acc2 = 0\n",
        "    for num in range(confidence_mtx1.shape[0]):\n",
        "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
        "        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n",
        "            acc1 += 1\n",
        "            \n",
        "    for num in range(confidence_mtx2.shape[0]):\n",
        "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
        "        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n",
        "            acc2 += 1\n",
        "    confidence1 = np.array(confidence1)\n",
        "    confidence2 = np.array(confidence2)\n",
        "    \n",
        "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
        "    \n",
        "    \n",
        "    #sort_confidence = np.sort(confidence1)\n",
        "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
        "    max_accuracy = 0.5\n",
        "    best_precision = 0.5\n",
        "    best_recall = 0.5\n",
        "    for num in range(len(sort_confidence)):\n",
        "        delta = sort_confidence[num]\n",
        "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
        "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
        "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
        "        if accuracy_now > max_accuracy:\n",
        "            max_accuracy = accuracy_now\n",
        "            best_precision = ratio1/(ratio1+ratio2)\n",
        "            best_recall = ratio1\n",
        "    print('membership inference accuracy is:', max_accuracy)\n",
        "    return max_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-O0GyDrezes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "870a8259-c898-4748-f736-ddbf99ae34c4"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import os\n",
        "import numpy as np\n",
        "import math \n",
        "import scipy\n",
        "import sys  \n",
        "\n",
        "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  (0.79672, 0.7464)\n",
            "model accuracy for training and test- (0.79672, 0.7464)\n",
            "membership inference accuracy is: 0.52657\n",
            "Maximum Accuracy: 0.52657\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}