{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vwIXk9gmF61"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DuXXLLblQu9"
   },
   "outputs": [],
   "source": [
    "class PruningModule(Module):\n",
    "    def prune_by_percentile(self, q=5.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "             The pruning percentile is based on all layer's parameters concatenated\n",
    "        Args:\n",
    "            q (float): percentile in float\n",
    "            **kwargs: may contain `cuda`\n",
    "        \"\"\"\n",
    "        # Calculate percentile value\n",
    "        alive_parameters = []\n",
    "        for name, p in self.named_parameters():\n",
    "            # We do not prune bias term\n",
    "            if 'bias' in name or 'mask' in name:\n",
    "                continue\n",
    "            tensor = p.data.cpu().numpy()\n",
    "            alive = tensor[np.nonzero(tensor)] # flattened array of nonzero values\n",
    "            alive_parameters.append(alive)\n",
    "\n",
    "        all_alives = np.concatenate(alive_parameters)\n",
    "        percentile_value = np.percentile(abs(all_alives), q)\n",
    "        print(f'Pruning with threshold : {percentile_value}')\n",
    "\n",
    "        # Prune the weights and mask\n",
    "        # Note that module here is the layer\n",
    "        # ex) fc1, fc2, fc3\n",
    "        for name, module in self.named_modules():\n",
    "            if name in ['fc1', 'fc2', 'fc3']:\n",
    "                module.prune(threshold=percentile_value)\n",
    "\n",
    "    def prune_by_std(self, s=0.25):\n",
    "        \"\"\"\n",
    "        Note that `s` is a quality parameter / sensitivity value according to the paper.\n",
    "        According to Song Han's previous paper (Learning both Weights and Connections for Efficient Neural Networks),\n",
    "        'The pruning threshold is chosen as a quality parameter multiplied by the standard deviation of a layerâ€™s weights'\n",
    "\n",
    "        I tried multiple values and empirically, 0.25 matches the paper's compression rate and number of parameters.\n",
    "        Note : In the paper, the authors used different sensitivity values for different layers.\n",
    "        \"\"\"\n",
    "        for name, module in self.named_modules():\n",
    "            if name in ['fc1', 'fc2', 'fc3']:\n",
    "                threshold = np.std(module.weight.data.cpu().numpy()) * s\n",
    "                print(f'Pruning with threshold : {threshold} for layer {name}')\n",
    "                module.prune(threshold)\n",
    "\n",
    "\n",
    "class MaskedLinear(Module):\n",
    "    r\"\"\"Applies a masked linear transformation to the incoming data: :math:`y = (A * M)x + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
    "          additional dimensions\n",
    "        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
    "          are the same shape as the input.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            (out_features x in_features)\n",
    "        bias:   the learnable bias of the module of shape (out_features)\n",
    "        mask: the unlearnable mask for the weight.\n",
    "            It has the same shape as weight (out_features x in_features)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        # Initialize the mask with 1\n",
    "        self.mask = Parameter(torch.ones([out_features, in_features]), requires_grad=False)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight * self.mask, self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'\n",
    "\n",
    "    def prune(self, threshold):\n",
    "        weight_dev = self.weight.device\n",
    "        mask_dev = self.mask.device\n",
    "        # Convert Tensors to numpy and calculate\n",
    "        tensor = self.weight.data.cpu().numpy()\n",
    "        mask = self.mask.data.cpu().numpy()\n",
    "        new_mask = np.where(abs(tensor) < threshold, 0, mask)\n",
    "        # Apply new weight and mask\n",
    "        self.weight.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "        self.mask.data = torch.from_numpy(new_mask).to(mask_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sc4-Wv_ZljJR"
   },
   "outputs": [],
   "source": [
    "def print_model_parameters(model, with_values=False):\n",
    "    print(f\"{'Param name':20} {'Shape':30} {'Type':15}\")\n",
    "    print('-'*70)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name:20} {str(param.shape):30} {str(param.dtype):15}')\n",
    "        if with_values:\n",
    "            print(param)\n",
    "\n",
    "\n",
    "def print_nonzeros(model):\n",
    "    nonzero = total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'mask' in name:\n",
    "            continue\n",
    "        tensor = p.data.cpu().numpy()\n",
    "        nz_count = np.count_nonzero(tensor)\n",
    "        total_params = np.prod(tensor.shape)\n",
    "        nonzero += nz_count\n",
    "        total += total_params\n",
    "        print(f'{name:20} | nonzeros = {nz_count:7} / {total_params:7} ({100 * nz_count / total_params:6.2f}%) | total_pruned = {total_params - nz_count :7} | shape = {tensor.shape}')\n",
    "    print(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzfvHlXYiNei"
   },
   "outputs": [],
   "source": [
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "def classifier_performance(model, train_loader, test_loader):\n",
    "\n",
    "    output_train_benign = []\n",
    "    train_label = []\n",
    "    for num, data in enumerate(train_loader):\n",
    "        images,labels = data\n",
    "        image_tensor= images.to(device)\n",
    "        img_variable = Variable(image_tensor, requires_grad=True)\n",
    "        output = model.forward(img_variable)\n",
    "\n",
    "        train_label.append(labels.numpy())\n",
    "        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
    "\n",
    "\n",
    "    train_label = np.concatenate(train_label)\n",
    "    output_train_benign=np.concatenate(output_train_benign)\n",
    "\n",
    "    test_label = []\n",
    "    output_test_benign = []\n",
    "\n",
    "    for num, data in enumerate(test_loader):\n",
    "        images,labels = data\n",
    "\n",
    "        image_tensor= images.to(device)\n",
    "        img_variable = Variable(image_tensor, requires_grad=True)\n",
    "\n",
    "        output = model.forward(img_variable)\n",
    "\n",
    "        test_label.append(labels.numpy())\n",
    "        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
    "\n",
    "\n",
    "    test_label = np.concatenate(test_label)\n",
    "    output_test_benign=np.concatenate(output_test_benign)\n",
    "\n",
    "\n",
    "    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n",
    "    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n",
    "\n",
    "    print('Accuracy: ', (train_acc1, test_acc1))\n",
    "\n",
    "    return output_train_benign, output_test_benign, train_label, test_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
    "    \n",
    "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
    "    confidence1 = []\n",
    "    confidence2 = []\n",
    "    acc1 = 0\n",
    "    acc2 = 0\n",
    "    for num in range(confidence_mtx1.shape[0]):\n",
    "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
    "        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n",
    "            acc1 += 1\n",
    "            \n",
    "    for num in range(confidence_mtx2.shape[0]):\n",
    "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
    "        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n",
    "            acc2 += 1\n",
    "    confidence1 = np.array(confidence1)\n",
    "    confidence2 = np.array(confidence2)\n",
    "    \n",
    "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
    "    \n",
    "    \n",
    "    #sort_confidence = np.sort(confidence1)\n",
    "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
    "    max_accuracy = 0.5\n",
    "    best_precision = 0.5\n",
    "    best_recall = 0.5\n",
    "    for num in range(len(sort_confidence)):\n",
    "        delta = sort_confidence[num]\n",
    "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
    "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
    "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
    "        if accuracy_now > max_accuracy:\n",
    "            max_accuracy = accuracy_now\n",
    "            best_precision = ratio1/(ratio1+ratio2)\n",
    "            best_recall = ratio1\n",
    "    print('membership inference accuracy is:', max_accuracy)\n",
    "    return max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Puh4c5iRhrZj",
    "outputId": "24a27d53-d6d5-4cd7-9457-636788b3dffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "colab='Colab Notebooks'\n",
    "path = F\"/content/gdrive/My Drive/{colab}/dataset_purchase\" \n",
    "path2 = F\"/content/gdrive/My Drive/{colab}/random_r_purchase100\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "K0rthX4bmLlr",
    "outputId": "714f2b78-d50a-44f4-daad-f86e44455e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "data_set =np.genfromtxt(path,delimiter=',')\n",
    "print(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoaIAg7lmPSZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X = data_set[:,1:].astype(np.float64)\n",
    "Y = (data_set[:,0]).astype(np.int32)-1\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.45)\n",
    "\n",
    "\n",
    "train_dataset = Data.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "test_dataset = Data.TensorDataset(torch.from_numpy(x_test).float(),  torch.from_numpy(y_test).long())\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlCAa9gvmR3g"
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "test_batch_size=1000\n",
    "epochs=50\n",
    "lr=1e-3\n",
    "seed=123\n",
    "log_interval=100\n",
    "\n",
    "\n",
    "class PurchaseClassifier(PruningModule):\n",
    "    def __init__(self, mask=False):\n",
    "        super(PurchaseClassifier, self).__init__()\n",
    "        linear = MaskedLinear if mask else nn.Linear\n",
    "        self.fc1=linear(600,1024)\n",
    "        self.fc2=linear(1024,512)\n",
    "        self.fc3=linear(512,256)\n",
    "        self.fc4=linear(256,128)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc5=nn.Linear(128,100)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc5(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Km2u_gy7kdPu"
   },
   "outputs": [],
   "source": [
    "def train(epochs, model):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(trainloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # zero-out all the gradients corresponding to the pruned connections\n",
    "            for name, p in model.named_parameters():\n",
    "                if 'mask' in name:\n",
    "                    continue\n",
    "                tensor = p.data.cpu().numpy()\n",
    "                grad_tensor = p.grad.data.cpu().numpy()\n",
    "                grad_tensor = np.where(tensor==0, 0, grad_tensor)\n",
    "                p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                done = batch_idx * len(data)\n",
    "                percentage = 100. * batch_idx / len(trainloader)\n",
    "                print(f'Train Epoch: {epoch} [{done:5}/{len(trainloader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
    "        test(trainloader, model)\n",
    "        test(testloader, model)\n",
    "\n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(loader.dataset)\n",
    "        accuracy = 100. * correct / len(loader.dataset)\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "pBG-w7MMgSal",
    "outputId": "15d56d5f-d913-4c9b-a436-ba8118179fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param name           Shape                          Type           \n",
      "----------------------------------------------------------------------\n",
      "fc1.weight           torch.Size([1024, 600])        torch.float32  \n",
      "fc1.mask             torch.Size([1024, 600])        torch.float32  \n",
      "fc1.bias             torch.Size([1024])             torch.float32  \n",
      "fc2.weight           torch.Size([512, 1024])        torch.float32  \n",
      "fc2.mask             torch.Size([512, 1024])        torch.float32  \n",
      "fc2.bias             torch.Size([512])              torch.float32  \n",
      "fc3.weight           torch.Size([256, 512])         torch.float32  \n",
      "fc3.mask             torch.Size([256, 512])         torch.float32  \n",
      "fc3.bias             torch.Size([256])              torch.float32  \n",
      "fc4.weight           torch.Size([128, 256])         torch.float32  \n",
      "fc4.mask             torch.Size([128, 256])         torch.float32  \n",
      "fc4.bias             torch.Size([128])              torch.float32  \n",
      "fc5.weight           torch.Size([100, 128])         torch.float32  \n",
      "fc5.bias             torch.Size([100])              torch.float32  \n"
     ]
    }
   ],
   "source": [
    "model = PurchaseClassifier(mask=True).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print_model_parameters(model)\n",
    "\n",
    "# NOTE : `weight_decay` term denotes L2 regularization loss term\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.000)\n",
    "initial_optimizer_state_dict = optimizer.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bL9WOnol8Sn"
   },
   "source": [
    "#Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wG8Rt7dxl7Yo",
    "outputId": "b75fcc09-a442-4456-8400-4e049d063844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial training ---\n",
      "Train Epoch: 0 [    0/108528 (  0%)]  Loss: 4.610348\n",
      "Train Epoch: 0 [12800/108528 ( 12%)]  Loss: 2.048452\n",
      "Train Epoch: 0 [25600/108528 ( 24%)]  Loss: 1.483622\n",
      "Train Epoch: 0 [38400/108528 ( 35%)]  Loss: 1.050490\n",
      "Train Epoch: 0 [51200/108528 ( 47%)]  Loss: 0.986742\n",
      "Train Epoch: 0 [64000/108528 ( 59%)]  Loss: 0.811211\n",
      "Train Epoch: 0 [76800/108528 ( 71%)]  Loss: 0.898746\n",
      "Train Epoch: 0 [89600/108528 ( 83%)]  Loss: 0.595600\n",
      "Train Epoch: 0 [102400/108528 ( 94%)]  Loss: 0.799103\n",
      "Test set: Average loss: 0.0049, Accuracy: 86018/108528 (79.26%)\n",
      "Test set: Average loss: 0.0007, Accuracy: 68571/88796 (77.22%)\n",
      "Train Epoch: 1 [    0/108528 (  0%)]  Loss: 0.545852\n",
      "Train Epoch: 1 [12800/108528 ( 12%)]  Loss: 0.466422\n",
      "Train Epoch: 1 [25600/108528 ( 24%)]  Loss: 0.444444\n",
      "Train Epoch: 1 [38400/108528 ( 35%)]  Loss: 0.422099\n",
      "Train Epoch: 1 [51200/108528 ( 47%)]  Loss: 0.579051\n",
      "Train Epoch: 1 [64000/108528 ( 59%)]  Loss: 0.502284\n",
      "Train Epoch: 1 [76800/108528 ( 71%)]  Loss: 0.438092\n",
      "Train Epoch: 1 [89600/108528 ( 83%)]  Loss: 0.444040\n",
      "Train Epoch: 1 [102400/108528 ( 94%)]  Loss: 0.589644\n",
      "Test set: Average loss: 0.0030, Accuracy: 94499/108528 (87.07%)\n",
      "Test set: Average loss: 0.0005, Accuracy: 74473/88796 (83.87%)\n",
      "Train Epoch: 2 [    0/108528 (  0%)]  Loss: 0.415299\n",
      "Train Epoch: 2 [12800/108528 ( 12%)]  Loss: 0.432835\n",
      "Train Epoch: 2 [25600/108528 ( 24%)]  Loss: 0.542115\n",
      "Train Epoch: 2 [38400/108528 ( 35%)]  Loss: 0.524260\n",
      "Train Epoch: 2 [51200/108528 ( 47%)]  Loss: 0.368721\n",
      "Train Epoch: 2 [64000/108528 ( 59%)]  Loss: 0.377680\n",
      "Train Epoch: 2 [76800/108528 ( 71%)]  Loss: 0.417903\n",
      "Train Epoch: 2 [89600/108528 ( 83%)]  Loss: 0.269953\n",
      "Train Epoch: 2 [102400/108528 ( 94%)]  Loss: 0.353322\n",
      "Test set: Average loss: 0.0023, Accuracy: 97669/108528 (89.99%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 76311/88796 (85.94%)\n",
      "Train Epoch: 3 [    0/108528 (  0%)]  Loss: 0.278684\n",
      "Train Epoch: 3 [12800/108528 ( 12%)]  Loss: 0.276966\n",
      "Train Epoch: 3 [25600/108528 ( 24%)]  Loss: 0.268192\n",
      "Train Epoch: 3 [38400/108528 ( 35%)]  Loss: 0.226370\n",
      "Train Epoch: 3 [51200/108528 ( 47%)]  Loss: 0.500724\n",
      "Train Epoch: 3 [64000/108528 ( 59%)]  Loss: 0.419362\n",
      "Train Epoch: 3 [76800/108528 ( 71%)]  Loss: 0.292468\n",
      "Train Epoch: 3 [89600/108528 ( 83%)]  Loss: 0.384640\n",
      "Train Epoch: 3 [102400/108528 ( 94%)]  Loss: 0.199215\n",
      "Test set: Average loss: 0.0022, Accuracy: 98079/108528 (90.37%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 76071/88796 (85.67%)\n",
      "Train Epoch: 4 [    0/108528 (  0%)]  Loss: 0.404901\n",
      "Train Epoch: 4 [12800/108528 ( 12%)]  Loss: 0.375558\n",
      "Train Epoch: 4 [25600/108528 ( 24%)]  Loss: 0.414692\n",
      "Train Epoch: 4 [38400/108528 ( 35%)]  Loss: 0.363424\n",
      "Train Epoch: 4 [51200/108528 ( 47%)]  Loss: 0.268460\n",
      "Train Epoch: 4 [64000/108528 ( 59%)]  Loss: 0.363856\n",
      "Train Epoch: 4 [76800/108528 ( 71%)]  Loss: 0.232040\n",
      "Train Epoch: 4 [89600/108528 ( 83%)]  Loss: 0.398277\n",
      "Train Epoch: 4 [102400/108528 ( 94%)]  Loss: 0.243009\n",
      "Test set: Average loss: 0.0019, Accuracy: 99097/108528 (91.31%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 76519/88796 (86.17%)\n",
      "Train Epoch: 5 [    0/108528 (  0%)]  Loss: 0.234804\n",
      "Train Epoch: 5 [12800/108528 ( 12%)]  Loss: 0.174285\n",
      "Train Epoch: 5 [25600/108528 ( 24%)]  Loss: 0.239377\n",
      "Train Epoch: 5 [38400/108528 ( 35%)]  Loss: 0.260647\n",
      "Train Epoch: 5 [51200/108528 ( 47%)]  Loss: 0.202378\n",
      "Train Epoch: 5 [64000/108528 ( 59%)]  Loss: 0.270532\n",
      "Train Epoch: 5 [76800/108528 ( 71%)]  Loss: 0.234453\n",
      "Train Epoch: 5 [89600/108528 ( 83%)]  Loss: 0.120685\n",
      "Train Epoch: 5 [102400/108528 ( 94%)]  Loss: 0.188906\n",
      "Test set: Average loss: 0.0018, Accuracy: 99830/108528 (91.99%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 76728/88796 (86.41%)\n",
      "Train Epoch: 6 [    0/108528 (  0%)]  Loss: 0.210732\n",
      "Train Epoch: 6 [12800/108528 ( 12%)]  Loss: 0.114238\n",
      "Train Epoch: 6 [25600/108528 ( 24%)]  Loss: 0.150033\n",
      "Train Epoch: 6 [38400/108528 ( 35%)]  Loss: 0.183440\n",
      "Train Epoch: 6 [51200/108528 ( 47%)]  Loss: 0.275690\n",
      "Train Epoch: 6 [64000/108528 ( 59%)]  Loss: 0.291831\n",
      "Train Epoch: 6 [76800/108528 ( 71%)]  Loss: 0.349879\n",
      "Train Epoch: 6 [89600/108528 ( 83%)]  Loss: 0.251268\n",
      "Train Epoch: 6 [102400/108528 ( 94%)]  Loss: 0.308442\n",
      "Test set: Average loss: 0.0015, Accuracy: 100970/108528 (93.04%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 77146/88796 (86.88%)\n",
      "Train Epoch: 7 [    0/108528 (  0%)]  Loss: 0.160013\n",
      "Train Epoch: 7 [12800/108528 ( 12%)]  Loss: 0.126053\n",
      "Train Epoch: 7 [25600/108528 ( 24%)]  Loss: 0.149678\n",
      "Train Epoch: 7 [38400/108528 ( 35%)]  Loss: 0.265675\n",
      "Train Epoch: 7 [51200/108528 ( 47%)]  Loss: 0.308905\n",
      "Train Epoch: 7 [64000/108528 ( 59%)]  Loss: 0.170709\n",
      "Train Epoch: 7 [76800/108528 ( 71%)]  Loss: 0.205451\n",
      "Train Epoch: 7 [89600/108528 ( 83%)]  Loss: 0.332096\n",
      "Train Epoch: 7 [102400/108528 ( 94%)]  Loss: 0.244506\n",
      "Test set: Average loss: 0.0013, Accuracy: 102020/108528 (94.00%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 77683/88796 (87.48%)\n",
      "Train Epoch: 8 [    0/108528 (  0%)]  Loss: 0.179548\n",
      "Train Epoch: 8 [12800/108528 ( 12%)]  Loss: 0.148670\n",
      "Train Epoch: 8 [25600/108528 ( 24%)]  Loss: 0.122500\n",
      "Train Epoch: 8 [38400/108528 ( 35%)]  Loss: 0.175014\n",
      "Train Epoch: 8 [51200/108528 ( 47%)]  Loss: 0.182517\n",
      "Train Epoch: 8 [64000/108528 ( 59%)]  Loss: 0.295507\n",
      "Train Epoch: 8 [76800/108528 ( 71%)]  Loss: 0.144747\n",
      "Train Epoch: 8 [89600/108528 ( 83%)]  Loss: 0.223075\n",
      "Train Epoch: 8 [102400/108528 ( 94%)]  Loss: 0.230559\n",
      "Test set: Average loss: 0.0012, Accuracy: 102709/108528 (94.64%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 77761/88796 (87.57%)\n",
      "Train Epoch: 9 [    0/108528 (  0%)]  Loss: 0.128294\n",
      "Train Epoch: 9 [12800/108528 ( 12%)]  Loss: 0.272085\n",
      "Train Epoch: 9 [25600/108528 ( 24%)]  Loss: 0.154822\n",
      "Train Epoch: 9 [38400/108528 ( 35%)]  Loss: 0.169540\n",
      "Train Epoch: 9 [51200/108528 ( 47%)]  Loss: 0.113604\n",
      "Train Epoch: 9 [64000/108528 ( 59%)]  Loss: 0.119246\n",
      "Train Epoch: 9 [76800/108528 ( 71%)]  Loss: 0.207404\n",
      "Train Epoch: 9 [89600/108528 ( 83%)]  Loss: 0.105372\n",
      "Train Epoch: 9 [102400/108528 ( 94%)]  Loss: 0.202147\n",
      "Test set: Average loss: 0.0011, Accuracy: 103020/108528 (94.92%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 78039/88796 (87.89%)\n",
      "Test Accuracy: 2.647641785666021\n",
      "Train Accuracy: 2.638028895768834\n",
      "--- Before pruning ---\n",
      "fc1.weight           | nonzeros =  614400 /  614400 (100.00%) | total_pruned =       0 | shape = (1024, 600)\n",
      "fc1.bias             | nonzeros =    1024 /    1024 (100.00%) | total_pruned =       0 | shape = (1024,)\n",
      "fc2.weight           | nonzeros =  524288 /  524288 (100.00%) | total_pruned =       0 | shape = (512, 1024)\n",
      "fc2.bias             | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "fc3.weight           | nonzeros =  131072 /  131072 (100.00%) | total_pruned =       0 | shape = (256, 512)\n",
      "fc3.bias             | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "fc4.weight           | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (128, 256)\n",
      "fc4.bias             | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "fc5.weight           | nonzeros =   12800 /   12800 (100.00%) | total_pruned =       0 | shape = (100, 128)\n",
      "fc5.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "alive: 1317348, pruned : 0, total: 1317348, Compression rate :       1.00x  (  0.00% pruned)\n",
      "Accuracy:  (0.9492481203007519, 0.8788571557277355)\n",
      "model accuracy for training and test- (0.9492481203007519, 0.8788571557277355)\n",
      "membership inference accuracy is: 0.5450857023658056\n",
      "Maximum Accuracy: 0.5450857023658056\n"
     ]
    }
   ],
   "source": [
    "# Initial training\n",
    "print(\"--- Initial training ---\")\n",
    "train(epochs, model)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "print(\"--- Before pruning ---\")\n",
    "print_nonzeros(model)\n",
    "\n",
    "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
    "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
    "print(\"Maximum Accuracy:\",inference_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kkd72YSAmF_7"
   },
   "source": [
    "# **Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "3oZDgrn3mHva",
    "outputId": "834ac5b5-e556-4834-a5ba-7f705a303748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting pruning ---\n",
      "Pruning with threshold : 0.09469142090529203 for layer fc1\n",
      "Pruning with threshold : 0.10588790103793144 for layer fc2\n",
      "Pruning with threshold : 0.10614151321351528 for layer fc3\n",
      "Test set: Average loss: 0.0056, Accuracy: 22080/88796 (24.87%)\n",
      "Test set: Average loss: 0.0434, Accuracy: 27399/108528 (25.25%)\n",
      "Test Accuracy: 24.86598495427722\n",
      "Train Accuracy: 25.246019460415745\n",
      "--- After pruning ---\n",
      "fc1.weight           | nonzeros =   48389 /  614400 (  7.88%) | total_pruned =  566011 | shape = (1024, 600)\n",
      "fc1.bias             | nonzeros =    1024 /    1024 (100.00%) | total_pruned =       0 | shape = (1024,)\n",
      "fc2.weight           | nonzeros =   40535 /  524288 (  7.73%) | total_pruned =  483753 | shape = (512, 1024)\n",
      "fc2.bias             | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "fc3.weight           | nonzeros =   10016 /  131072 (  7.64%) | total_pruned =  121056 | shape = (256, 512)\n",
      "fc3.bias             | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "fc4.weight           | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (128, 256)\n",
      "fc4.bias             | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "fc5.weight           | nonzeros =   12800 /   12800 (100.00%) | total_pruned =       0 | shape = (100, 128)\n",
      "fc5.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "alive: 146528, pruned : 1170820, total: 1317348, Compression rate :       8.99x  ( 88.88% pruned)\n",
      "Accuracy:  (0.2524601946041575, 0.2486598495427722)\n",
      "model accuracy for training and test- (0.2524601946041575, 0.2486598495427722)\n",
      "membership inference accuracy is: 0.5028081007357244\n",
      "Maximum Accuracy: 0.5028081007357244\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "\n",
    "print(\"--- Starting pruning ---\")\n",
    "model.prune_by_std(0.25)\n",
    "test_accuracy = test(testloader,model)\n",
    "train_accuracy = test(trainloader, model)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"--- After pruning ---\")\n",
    "print_nonzeros(model)\n",
    "\n",
    "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
    "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
    "print(\"Maximum Accuracy:\",inference_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9orLFiaBmOBb"
   },
   "source": [
    "# **Retraining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mlczQKhymPy4",
    "outputId": "ea57bbe4-2c91-471a-b286-09617283ed0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retraining ---\n",
      "Train Epoch: 0 [    0/108528 (  0%)]  Loss: 6.268572\n",
      "Train Epoch: 0 [12800/108528 ( 12%)]  Loss: 0.754935\n",
      "Train Epoch: 0 [25600/108528 ( 24%)]  Loss: 0.572835\n",
      "Train Epoch: 0 [38400/108528 ( 35%)]  Loss: 0.360728\n",
      "Train Epoch: 0 [51200/108528 ( 47%)]  Loss: 0.494237\n",
      "Train Epoch: 0 [64000/108528 ( 59%)]  Loss: 0.443071\n",
      "Train Epoch: 0 [76800/108528 ( 71%)]  Loss: 0.337406\n",
      "Train Epoch: 0 [89600/108528 ( 83%)]  Loss: 0.465814\n",
      "Train Epoch: 0 [102400/108528 ( 94%)]  Loss: 0.304261\n",
      "Test set: Average loss: 0.0020, Accuracy: 98745/108528 (90.99%)\n",
      "Test set: Average loss: 0.0004, Accuracy: 75669/88796 (85.22%)\n",
      "Train Epoch: 1 [    0/108528 (  0%)]  Loss: 0.352067\n",
      "Train Epoch: 1 [12800/108528 ( 12%)]  Loss: 0.246084\n",
      "Train Epoch: 1 [25600/108528 ( 24%)]  Loss: 0.182292\n",
      "Train Epoch: 1 [38400/108528 ( 35%)]  Loss: 0.225765\n",
      "Train Epoch: 1 [51200/108528 ( 47%)]  Loss: 0.301822\n",
      "Train Epoch: 1 [64000/108528 ( 59%)]  Loss: 0.232427\n",
      "Train Epoch: 1 [76800/108528 ( 71%)]  Loss: 0.141276\n",
      "Train Epoch: 1 [89600/108528 ( 83%)]  Loss: 0.236888\n",
      "Train Epoch: 1 [102400/108528 ( 94%)]  Loss: 0.183973\n",
      "Test set: Average loss: 0.0013, Accuracy: 102409/108528 (94.36%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 77907/88796 (87.74%)\n",
      "Train Epoch: 2 [    0/108528 (  0%)]  Loss: 0.178193\n",
      "Train Epoch: 2 [12800/108528 ( 12%)]  Loss: 0.196707\n",
      "Train Epoch: 2 [25600/108528 ( 24%)]  Loss: 0.265160\n",
      "Train Epoch: 2 [38400/108528 ( 35%)]  Loss: 0.231421\n",
      "Train Epoch: 2 [51200/108528 ( 47%)]  Loss: 0.131995\n",
      "Train Epoch: 2 [64000/108528 ( 59%)]  Loss: 0.112638\n",
      "Train Epoch: 2 [76800/108528 ( 71%)]  Loss: 0.144933\n",
      "Train Epoch: 2 [89600/108528 ( 83%)]  Loss: 0.134496\n",
      "Train Epoch: 2 [102400/108528 ( 94%)]  Loss: 0.142241\n",
      "Test set: Average loss: 0.0010, Accuracy: 104267/108528 (96.07%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 78658/88796 (88.58%)\n",
      "Train Epoch: 3 [    0/108528 (  0%)]  Loss: 0.143417\n",
      "Train Epoch: 3 [12800/108528 ( 12%)]  Loss: 0.097680\n",
      "Train Epoch: 3 [25600/108528 ( 24%)]  Loss: 0.108610\n",
      "Train Epoch: 3 [38400/108528 ( 35%)]  Loss: 0.101109\n",
      "Train Epoch: 3 [51200/108528 ( 47%)]  Loss: 0.086732\n",
      "Train Epoch: 3 [64000/108528 ( 59%)]  Loss: 0.118800\n",
      "Train Epoch: 3 [76800/108528 ( 71%)]  Loss: 0.116283\n",
      "Train Epoch: 3 [89600/108528 ( 83%)]  Loss: 0.096329\n",
      "Train Epoch: 3 [102400/108528 ( 94%)]  Loss: 0.171814\n",
      "Test set: Average loss: 0.0007, Accuracy: 105635/108528 (97.33%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 79269/88796 (89.27%)\n",
      "Train Epoch: 4 [    0/108528 (  0%)]  Loss: 0.101743\n",
      "Train Epoch: 4 [12800/108528 ( 12%)]  Loss: 0.082089\n",
      "Train Epoch: 4 [25600/108528 ( 24%)]  Loss: 0.086616\n",
      "Train Epoch: 4 [38400/108528 ( 35%)]  Loss: 0.120164\n",
      "Train Epoch: 4 [51200/108528 ( 47%)]  Loss: 0.143148\n",
      "Train Epoch: 4 [64000/108528 ( 59%)]  Loss: 0.149369\n",
      "Train Epoch: 4 [76800/108528 ( 71%)]  Loss: 0.078953\n",
      "Train Epoch: 4 [89600/108528 ( 83%)]  Loss: 0.106629\n",
      "Train Epoch: 4 [102400/108528 ( 94%)]  Loss: 0.188627\n",
      "Test set: Average loss: 0.0005, Accuracy: 106666/108528 (98.28%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 79863/88796 (89.94%)\n",
      "Train Epoch: 5 [    0/108528 (  0%)]  Loss: 0.044846\n",
      "Train Epoch: 5 [12800/108528 ( 12%)]  Loss: 0.050608\n",
      "Train Epoch: 5 [25600/108528 ( 24%)]  Loss: 0.067372\n",
      "Train Epoch: 5 [38400/108528 ( 35%)]  Loss: 0.035440\n",
      "Train Epoch: 5 [51200/108528 ( 47%)]  Loss: 0.045671\n",
      "Train Epoch: 5 [64000/108528 ( 59%)]  Loss: 0.042978\n",
      "Train Epoch: 5 [76800/108528 ( 71%)]  Loss: 0.075164\n",
      "Train Epoch: 5 [89600/108528 ( 83%)]  Loss: 0.064030\n",
      "Train Epoch: 5 [102400/108528 ( 94%)]  Loss: 0.056706\n",
      "Test set: Average loss: 0.0005, Accuracy: 106743/108528 (98.36%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 79723/88796 (89.78%)\n",
      "Train Epoch: 6 [    0/108528 (  0%)]  Loss: 0.047246\n",
      "Train Epoch: 6 [12800/108528 ( 12%)]  Loss: 0.072012\n",
      "Train Epoch: 6 [25600/108528 ( 24%)]  Loss: 0.058913\n",
      "Train Epoch: 6 [38400/108528 ( 35%)]  Loss: 0.054317\n",
      "Train Epoch: 6 [51200/108528 ( 47%)]  Loss: 0.037036\n",
      "Train Epoch: 6 [64000/108528 ( 59%)]  Loss: 0.053178\n",
      "Train Epoch: 6 [76800/108528 ( 71%)]  Loss: 0.053904\n",
      "Train Epoch: 6 [89600/108528 ( 83%)]  Loss: 0.106018\n",
      "Train Epoch: 6 [102400/108528 ( 94%)]  Loss: 0.103604\n",
      "Test set: Average loss: 0.0004, Accuracy: 107155/108528 (98.73%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 79901/88796 (89.98%)\n",
      "Train Epoch: 7 [    0/108528 (  0%)]  Loss: 0.043018\n",
      "Train Epoch: 7 [12800/108528 ( 12%)]  Loss: 0.048073\n",
      "Train Epoch: 7 [25600/108528 ( 24%)]  Loss: 0.041568\n",
      "Train Epoch: 7 [38400/108528 ( 35%)]  Loss: 0.020502\n",
      "Train Epoch: 7 [51200/108528 ( 47%)]  Loss: 0.031459\n",
      "Train Epoch: 7 [64000/108528 ( 59%)]  Loss: 0.065179\n",
      "Train Epoch: 7 [76800/108528 ( 71%)]  Loss: 0.025963\n",
      "Train Epoch: 7 [89600/108528 ( 83%)]  Loss: 0.045690\n",
      "Train Epoch: 7 [102400/108528 ( 94%)]  Loss: 0.071929\n",
      "Test set: Average loss: 0.0003, Accuracy: 107603/108528 (99.15%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 80111/88796 (90.22%)\n",
      "Train Epoch: 8 [    0/108528 (  0%)]  Loss: 0.048238\n",
      "Train Epoch: 8 [12800/108528 ( 12%)]  Loss: 0.054175\n",
      "Train Epoch: 8 [25600/108528 ( 24%)]  Loss: 0.025495\n",
      "Train Epoch: 8 [38400/108528 ( 35%)]  Loss: 0.015335\n",
      "Train Epoch: 8 [51200/108528 ( 47%)]  Loss: 0.028937\n",
      "Train Epoch: 8 [64000/108528 ( 59%)]  Loss: 0.068485\n",
      "Train Epoch: 8 [76800/108528 ( 71%)]  Loss: 0.018021\n",
      "Train Epoch: 8 [89600/108528 ( 83%)]  Loss: 0.032889\n",
      "Train Epoch: 8 [102400/108528 ( 94%)]  Loss: 0.042325\n",
      "Test set: Average loss: 0.0003, Accuracy: 107739/108528 (99.27%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 80063/88796 (90.17%)\n",
      "Train Epoch: 9 [    0/108528 (  0%)]  Loss: 0.036620\n",
      "Train Epoch: 9 [12800/108528 ( 12%)]  Loss: 0.023799\n",
      "Train Epoch: 9 [25600/108528 ( 24%)]  Loss: 0.029587\n",
      "Train Epoch: 9 [38400/108528 ( 35%)]  Loss: 0.034909\n",
      "Train Epoch: 9 [51200/108528 ( 47%)]  Loss: 0.031580\n",
      "Train Epoch: 9 [64000/108528 ( 59%)]  Loss: 0.047825\n",
      "Train Epoch: 9 [76800/108528 ( 71%)]  Loss: 0.025624\n",
      "Train Epoch: 9 [89600/108528 ( 83%)]  Loss: 0.032088\n",
      "Train Epoch: 9 [102400/108528 ( 94%)]  Loss: 0.010608\n",
      "Test set: Average loss: 0.0002, Accuracy: 107873/108528 (99.40%)\n",
      "Test set: Average loss: 0.0003, Accuracy: 80201/88796 (90.32%)\n",
      "Test Accuracy: 24.86598495427722\n",
      "Train Accuracy: 25.246019460415745\n",
      "--- After Retraining ---\n",
      "fc1.weight           | nonzeros =   48389 /  614400 (  7.88%) | total_pruned =  566011 | shape = (1024, 600)\n",
      "fc1.bias             | nonzeros =    1024 /    1024 (100.00%) | total_pruned =       0 | shape = (1024,)\n",
      "fc2.weight           | nonzeros =   40535 /  524288 (  7.73%) | total_pruned =  483753 | shape = (512, 1024)\n",
      "fc2.bias             | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "fc3.weight           | nonzeros =   10016 /  131072 (  7.64%) | total_pruned =  121056 | shape = (256, 512)\n",
      "fc3.bias             | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "fc4.weight           | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (128, 256)\n",
      "fc4.bias             | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "fc5.weight           | nonzeros =   12800 /   12800 (100.00%) | total_pruned =       0 | shape = (100, 128)\n",
      "fc5.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "alive: 146528, pruned : 1170820, total: 1317348, Compression rate :       8.99x  ( 88.88% pruned)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-167ecd07e800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint_nonzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0minference_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_via_confidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maximum Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minference_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"--- Retraining ---\")\n",
    "optimizer.load_state_dict(initial_optimizer_state_dict) # Reset the optimizer\n",
    "train(epochs, model)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "print(\"--- After Retraining ---\")\n",
    "print_nonzeros(model)\n",
    "\n",
    "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
    "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
    "print(\"Maximum Accuracy:\",inference_accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Purchase_Pruning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
