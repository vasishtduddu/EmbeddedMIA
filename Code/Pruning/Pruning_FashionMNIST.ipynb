{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZrxLrhGfmGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-eO4RAbgabW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class PruningModule(Module):\n",
        "    def prune_by_percentile(self, q=5.0, **kwargs):\n",
        "        \"\"\"\n",
        "        Note:\n",
        "             The pruning percentile is based on all layer's parameters concatenated\n",
        "        Args:\n",
        "            q (float): percentile in float\n",
        "            **kwargs: may contain `cuda`\n",
        "        \"\"\"\n",
        "        # Calculate percentile value\n",
        "        alive_parameters = []\n",
        "        for name, p in self.named_parameters():\n",
        "            # We do not prune bias term\n",
        "            if 'bias' in name or 'mask' in name:\n",
        "                continue\n",
        "            tensor = p.data.cpu().numpy()\n",
        "            alive = tensor[np.nonzero(tensor)] # flattened array of nonzero values\n",
        "            alive_parameters.append(alive)\n",
        "\n",
        "        all_alives = np.concatenate(alive_parameters)\n",
        "        percentile_value = np.percentile(abs(all_alives), q)\n",
        "        print(f'Pruning with threshold : {percentile_value}')\n",
        "\n",
        "        # Prune the weights and mask\n",
        "        # Note that module here is the layer\n",
        "        # ex) fc1, fc2, fc3\n",
        "        for name, module in self.named_modules():\n",
        "            if name in ['fc1', 'fc2', 'fc3']:\n",
        "                module.prune(threshold=percentile_value)\n",
        "\n",
        "    def prune_by_std(self, s=0.25):\n",
        "        \"\"\"\n",
        "        Note that `s` is a quality parameter / sensitivity value according to the paper.\n",
        "        According to Song Han's previous paper (Learning both Weights and Connections for Efficient Neural Networks),\n",
        "        'The pruning threshold is chosen as a quality parameter multiplied by the standard deviation of a layerâ€™s weights'\n",
        "\n",
        "        I tried multiple values and empirically, 0.25 matches the paper's compression rate and number of parameters.\n",
        "        Note : In the paper, the authors used different sensitivity values for different layers.\n",
        "        \"\"\"\n",
        "        for name, module in self.named_modules():\n",
        "            if name in ['fc1', 'fc2', 'fc3']:\n",
        "                threshold = np.std(module.weight.data.cpu().numpy()) * s\n",
        "                print(f'Pruning with threshold : {threshold} for layer {name}')\n",
        "                module.prune(threshold)\n",
        "\n",
        "\n",
        "class MaskedLinear(Module):\n",
        "    r\"\"\"Applies a masked linear transformation to the incoming data: :math:`y = (A * M)x + b`\n",
        "\n",
        "    Args:\n",
        "        in_features: size of each input sample\n",
        "        out_features: size of each output sample\n",
        "        bias: If set to False, the layer will not learn an additive bias.\n",
        "            Default: ``True``\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
        "          additional dimensions\n",
        "        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
        "          are the same shape as the input.\n",
        "\n",
        "    Attributes:\n",
        "        weight: the learnable weights of the module of shape\n",
        "            (out_features x in_features)\n",
        "        bias:   the learnable bias of the module of shape (out_features)\n",
        "        mask: the unlearnable mask for the weight.\n",
        "            It has the same shape as weight (out_features x in_features)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(MaskedLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        # Initialize the mask with 1\n",
        "        self.mask = Parameter(torch.ones([out_features, in_features]), requires_grad=False)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.weight * self.mask, self.bias)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_features=' + str(self.in_features) \\\n",
        "            + ', out_features=' + str(self.out_features) \\\n",
        "            + ', bias=' + str(self.bias is not None) + ')'\n",
        "\n",
        "    def prune(self, threshold):\n",
        "        weight_dev = self.weight.device\n",
        "        mask_dev = self.mask.device\n",
        "        # Convert Tensors to numpy and calculate\n",
        "        tensor = self.weight.data.cpu().numpy()\n",
        "        mask = self.mask.data.cpu().numpy()\n",
        "        new_mask = np.where(abs(tensor) < threshold, 0, mask)\n",
        "        # Apply new weight and mask\n",
        "        self.weight.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
        "        self.mask.data = torch.from_numpy(new_mask).to(mask_dev)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LeNet(PruningModule):\n",
        "    def __init__(self, mask=False):\n",
        "        super(LeNet, self).__init__()\n",
        "        linear = MaskedLinear if mask else nn.Linear\n",
        "        self.fc1 = linear(784, 512)\n",
        "        self.fc2 = linear(512, 512)\n",
        "        self.fc3 = linear(512, 512)\n",
        "        self.fc4 = linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keqwbYkqgcfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import scipy\n",
        "import sys\n",
        "\n",
        "\n",
        "def softmax_by_row(logits, T = 1.0):\n",
        "    mx = np.max(logits, axis=-1, keepdims=True)\n",
        "    exp = np.exp((logits - mx)/T)\n",
        "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
        "    return exp/denominator\n",
        "\n",
        "def classifier_performance(model, train_loader, test_loader):\n",
        "\n",
        "    output_train_benign = []\n",
        "    train_label = []\n",
        "    for num, data in enumerate(train_loader):\n",
        "        images,labels = data\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        train_label.append(labels.numpy())\n",
        "        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    train_label = np.concatenate(train_label)\n",
        "    output_train_benign=np.concatenate(output_train_benign)\n",
        "\n",
        "    test_label = []\n",
        "    output_test_benign = []\n",
        "\n",
        "    for num, data in enumerate(test_loader):\n",
        "        images,labels = data\n",
        "\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        test_label.append(labels.numpy())\n",
        "        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    test_label = np.concatenate(test_label)\n",
        "    output_test_benign=np.concatenate(output_test_benign)\n",
        "\n",
        "\n",
        "    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n",
        "    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n",
        "\n",
        "    print('Accuracy: ', (train_acc1, test_acc1))\n",
        "\n",
        "    return output_train_benign, output_test_benign, train_label, test_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
        "\n",
        "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
        "    confidence1 = []\n",
        "    confidence2 = []\n",
        "    acc1 = 0\n",
        "    acc2 = 0\n",
        "    for num in range(confidence_mtx1.shape[0]):\n",
        "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
        "        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n",
        "            acc1 += 1\n",
        "\n",
        "    for num in range(confidence_mtx2.shape[0]):\n",
        "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
        "        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n",
        "            acc2 += 1\n",
        "    confidence1 = np.array(confidence1)\n",
        "    confidence2 = np.array(confidence2)\n",
        "\n",
        "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
        "\n",
        "\n",
        "    #sort_confidence = np.sort(confidence1)\n",
        "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
        "    max_accuracy = 0.5\n",
        "    best_precision = 0.5\n",
        "    best_recall = 0.5\n",
        "    for num in range(len(sort_confidence)):\n",
        "        delta = sort_confidence[num]\n",
        "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
        "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
        "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
        "        if accuracy_now > max_accuracy:\n",
        "            max_accuracy = accuracy_now\n",
        "            best_precision = ratio1/(ratio1+ratio2)\n",
        "            best_recall = ratio1\n",
        "    print('membership inference accuracy is:', max_accuracy)\n",
        "    return max_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjchxHoJgBPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_model_parameters(model, with_values=False):\n",
        "    print(f\"{'Param name':20} {'Shape':30} {'Type':15}\")\n",
        "    print('-'*70)\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f'{name:20} {str(param.shape):30} {str(param.dtype):15}')\n",
        "        if with_values:\n",
        "            print(param)\n",
        "\n",
        "\n",
        "def print_nonzeros(model):\n",
        "    nonzero = total = 0\n",
        "    for name, p in model.named_parameters():\n",
        "        if 'mask' in name:\n",
        "            continue\n",
        "        tensor = p.data.cpu().numpy()\n",
        "        nz_count = np.count_nonzero(tensor)\n",
        "        total_params = np.prod(tensor.shape)\n",
        "        nonzero += nz_count\n",
        "        total += total_params\n",
        "        print(f'{name:20} | nonzeros = {nz_count:7} / {total_params:7} ({100 * nz_count / total_params:6.2f}%) | total_pruned = {total_params - nz_count :7} | shape = {tensor.shape}')\n",
        "    print(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17v-jrNfgExG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98f7adca-7ffc-4565-93ff-88d4e3a45c80"
      },
      "source": [
        "batch_size=64\n",
        "test_batch_size=1000\n",
        "epochs=75\n",
        "lr=1e-3\n",
        "no_cuda=False\n",
        "seed=1337\n",
        "log_interval=100\n",
        "logfile='log.txt'\n",
        "sensitivity=2  #sensitivity value that is multiplied to layer's std in order to get threshold value\n",
        "\n",
        "# Control Seed\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Select Device\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else 'cpu')\n",
        "if use_cuda:\n",
        "    print(\"Using CUDA!\")\n",
        "    torch.cuda.manual_seed(seed)\n",
        "else:\n",
        "    print('Not using CUDA!!!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not using CUDA!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9RN9unMgJYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "728cac74-c763-4dff-f948-ef7c73d63f5b"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(datasets.FashionMNIST('data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(datasets.FashionMNIST('data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),batch_size=test_batch_size, shuffle=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:02, 12603690.35it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 98538.72it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 4164872.86it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 28303.47it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUM5-1CbgM1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "c94e8db6-3c06-4a2a-bc90-3da912be7ca7"
      },
      "source": [
        "model = LeNet(mask=True).to(device)\n",
        "\n",
        "print(model)\n",
        "print_model_parameters(model)\n",
        "\n",
        "# NOTE : `weight_decay` term denotes L2 regularization loss term\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.000)\n",
        "initial_optimizer_state_dict = optimizer.state_dict()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (fc1): MaskedLinear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): MaskedLinear(in_features=512, out_features=512, bias=True)\n",
            "  (fc3): MaskedLinear(in_features=512, out_features=512, bias=True)\n",
            "  (fc4): MaskedLinear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Param name           Shape                          Type           \n",
            "----------------------------------------------------------------------\n",
            "fc1.weight           torch.Size([512, 784])         torch.float32  \n",
            "fc1.mask             torch.Size([512, 784])         torch.float32  \n",
            "fc1.bias             torch.Size([512])              torch.float32  \n",
            "fc2.weight           torch.Size([512, 512])         torch.float32  \n",
            "fc2.mask             torch.Size([512, 512])         torch.float32  \n",
            "fc2.bias             torch.Size([512])              torch.float32  \n",
            "fc3.weight           torch.Size([512, 512])         torch.float32  \n",
            "fc3.mask             torch.Size([512, 512])         torch.float32  \n",
            "fc3.bias             torch.Size([512])              torch.float32  \n",
            "fc4.weight           torch.Size([10, 512])          torch.float32  \n",
            "fc4.mask             torch.Size([10, 512])          torch.float32  \n",
            "fc4.bias             torch.Size([10])               torch.float32  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GkopEAMgNVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # zero-out all the gradients corresponding to the pruned connections\n",
        "            for name, p in model.named_parameters():\n",
        "                if 'mask' in name:\n",
        "                    continue\n",
        "                tensor = p.data.cpu().numpy()\n",
        "                grad_tensor = p.grad.data.cpu().numpy()\n",
        "                grad_tensor = np.where(tensor==0, 0, grad_tensor)\n",
        "                p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
        "\n",
        "            optimizer.step()\n",
        "            if batch_idx % log_interval == 0:\n",
        "                done = batch_idx * len(data)\n",
        "                percentage = 100. * batch_idx / len(train_loader)\n",
        "                print(f'Train Epoch: {epoch} [{done:5}/{len(train_loader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(loader.dataset)\n",
        "        accuracy = 100. * correct / len(loader.dataset)\n",
        "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
        "    return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNR04oT4gTDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "e73c20fe-4d62-4cc3-d60a-9c0991a92cdf"
      },
      "source": [
        "# Initial training\n",
        "print(\"--- Initial training ---\")\n",
        "train(epochs)\n",
        "test_accuracy = test(test_loader)\n",
        "train_accuracy = test(train_loader)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "\n",
        "\n",
        "print(\"--- Before pruning ---\")\n",
        "print_nonzeros(model)\n",
        "\n",
        "output_train, output_test, train_label, test_label = classifier_performance(model, train_loader, test_loader)\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Initial training ---\n",
            "Train Epoch: 0 [    0/60000 (  0%)]  Loss: 2.299644\n",
            "Train Epoch: 0 [ 6400/60000 ( 11%)]  Loss: 0.549537\n",
            "Train Epoch: 0 [12800/60000 ( 21%)]  Loss: 0.775609\n",
            "Train Epoch: 0 [19200/60000 ( 32%)]  Loss: 0.375063\n",
            "Train Epoch: 0 [25600/60000 ( 43%)]  Loss: 0.321147\n",
            "Train Epoch: 0 [32000/60000 ( 53%)]  Loss: 0.519415\n",
            "Train Epoch: 0 [38400/60000 ( 64%)]  Loss: 0.451665\n",
            "Train Epoch: 0 [44800/60000 ( 75%)]  Loss: 0.403279\n",
            "Train Epoch: 0 [51200/60000 ( 85%)]  Loss: 0.328973\n",
            "Train Epoch: 0 [57600/60000 ( 96%)]  Loss: 0.233651\n",
            "Train Epoch: 1 [    0/60000 (  0%)]  Loss: 0.431317\n",
            "Train Epoch: 1 [ 6400/60000 ( 11%)]  Loss: 0.413782\n",
            "Train Epoch: 1 [12800/60000 ( 21%)]  Loss: 0.381628\n",
            "Train Epoch: 1 [19200/60000 ( 32%)]  Loss: 0.468640\n",
            "Train Epoch: 1 [25600/60000 ( 43%)]  Loss: 0.304571\n",
            "Train Epoch: 1 [32000/60000 ( 53%)]  Loss: 0.341546\n",
            "Train Epoch: 1 [38400/60000 ( 64%)]  Loss: 0.554334\n",
            "Train Epoch: 1 [44800/60000 ( 75%)]  Loss: 0.308051\n",
            "Train Epoch: 1 [51200/60000 ( 85%)]  Loss: 0.279284\n",
            "Train Epoch: 1 [57600/60000 ( 96%)]  Loss: 0.259942\n",
            "Train Epoch: 2 [    0/60000 (  0%)]  Loss: 0.608840\n",
            "Train Epoch: 2 [ 6400/60000 ( 11%)]  Loss: 0.246144\n",
            "Train Epoch: 2 [12800/60000 ( 21%)]  Loss: 0.261957\n",
            "Train Epoch: 2 [19200/60000 ( 32%)]  Loss: 0.503306\n",
            "Train Epoch: 2 [25600/60000 ( 43%)]  Loss: 0.425997\n",
            "Train Epoch: 2 [32000/60000 ( 53%)]  Loss: 0.330238\n",
            "Train Epoch: 2 [38400/60000 ( 64%)]  Loss: 0.219128\n",
            "Train Epoch: 2 [44800/60000 ( 75%)]  Loss: 0.319961\n",
            "Train Epoch: 2 [51200/60000 ( 85%)]  Loss: 0.342230\n",
            "Train Epoch: 2 [57600/60000 ( 96%)]  Loss: 0.229080\n",
            "Train Epoch: 3 [    0/60000 (  0%)]  Loss: 0.291876\n",
            "Train Epoch: 3 [ 6400/60000 ( 11%)]  Loss: 0.271047\n",
            "Train Epoch: 3 [12800/60000 ( 21%)]  Loss: 0.244115\n",
            "Train Epoch: 3 [19200/60000 ( 32%)]  Loss: 0.260612\n",
            "Train Epoch: 3 [25600/60000 ( 43%)]  Loss: 0.391750\n",
            "Train Epoch: 3 [32000/60000 ( 53%)]  Loss: 0.248282\n",
            "Train Epoch: 3 [38400/60000 ( 64%)]  Loss: 0.252278\n",
            "Train Epoch: 3 [44800/60000 ( 75%)]  Loss: 0.333534\n",
            "Train Epoch: 3 [51200/60000 ( 85%)]  Loss: 0.223330\n",
            "Train Epoch: 3 [57600/60000 ( 96%)]  Loss: 0.224750\n",
            "Train Epoch: 4 [    0/60000 (  0%)]  Loss: 0.197688\n",
            "Train Epoch: 4 [ 6400/60000 ( 11%)]  Loss: 0.494634\n",
            "Train Epoch: 4 [12800/60000 ( 21%)]  Loss: 0.304956\n",
            "Train Epoch: 4 [19200/60000 ( 32%)]  Loss: 0.390577\n",
            "Train Epoch: 4 [25600/60000 ( 43%)]  Loss: 0.236503\n",
            "Train Epoch: 4 [32000/60000 ( 53%)]  Loss: 0.361424\n",
            "Train Epoch: 4 [38400/60000 ( 64%)]  Loss: 0.192116\n",
            "Train Epoch: 4 [44800/60000 ( 75%)]  Loss: 0.270878\n",
            "Train Epoch: 4 [51200/60000 ( 85%)]  Loss: 0.333460\n",
            "Train Epoch: 4 [57600/60000 ( 96%)]  Loss: 0.218527\n",
            "Train Epoch: 5 [    0/60000 (  0%)]  Loss: 0.305515\n",
            "Train Epoch: 5 [ 6400/60000 ( 11%)]  Loss: 0.176708\n",
            "Train Epoch: 5 [12800/60000 ( 21%)]  Loss: 0.323892\n",
            "Train Epoch: 5 [19200/60000 ( 32%)]  Loss: 0.153560\n",
            "Train Epoch: 5 [25600/60000 ( 43%)]  Loss: 0.301840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tolnT9SahND3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pruning\n",
        "\n",
        "print(\"--- Starting pruning ---\")\n",
        "model.prune_by_std(1)\n",
        "test_accuracy = test(test_loader)\n",
        "train_accuracy = test(train_loader)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"--- After pruning ---\")\n",
        "print_nonzeros(model)\n",
        "\n",
        "output_train, output_test, train_label, test_label = classifier_performance(model, train_loader, test_loader)\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCmxX2LSoeXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"--- Retraining ---\")\n",
        "optimizer.load_state_dict(initial_optimizer_state_dict) # Reset the optimizer\n",
        "train(epochs)\n",
        "\n",
        "test_accuracy = test(test_loader)\n",
        "train_accuracy = test(train_loader)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "\n",
        "print(\"--- After Retraining ---\")\n",
        "print_nonzeros(model)\n",
        "\n",
        "output_train, output_test, train_label, test_label = classifier_performance(model, train_loader, test_loader)\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}