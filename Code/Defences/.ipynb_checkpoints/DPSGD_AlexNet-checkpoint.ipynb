{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7alZV0YJfa9h"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import torch\n",
    "\n",
    "import uuid\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_4Tu-ZIfr-c"
   },
   "outputs": [],
   "source": [
    "PI= torch.cuda.FloatTensor([math.pi])\n",
    "NORM = (torch.sqrt(2.0*PI))\n",
    "def norm_pdf(x,mean,std):\n",
    "    y= (x-mean)/std\n",
    "    return (torch.exp( -(y).pow(2)/2.0)/NORM)/std\n",
    "\n",
    "\n",
    "class DPSGD(Optimizer):\n",
    "    \n",
    "\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False,C=1,noise_multiplier= 1.0 , batch_size=256):\n",
    "        \n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(DPSGD, self).__init__(params, defaults)\n",
    "\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.C = C\n",
    "        self.bigger_batch = {}\n",
    "\n",
    "\n",
    "        self.bigger_batch_count = {}\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        \n",
    "    def __setstate__(self, state):\n",
    "        super(SGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        noise_std =self.noise_multiplier *self.C\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        norm  = 0\n",
    "            \n",
    "        for group in self.param_groups:\n",
    "        \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                \n",
    "                if not hasattr(p,'myid'):\n",
    "                    p.myid = uuid.uuid4()\n",
    "                    self.bigger_batch[p.myid] = torch.zeros_like(grad)\n",
    "                    \n",
    "                    if self.bigger_batch[p.myid].is_cuda:\n",
    "                        self.bigger_batch_count[p.myid] = torch.cuda.LongTensor(size=[1]).zero_()\n",
    "                    else:\n",
    "                        self.bigger_batch_count[p.myid] = torch.LongTensor(size=[1]).zero_()\n",
    "\n",
    "                norm+=grad.norm()**2.0\n",
    "        \n",
    "        norm=norm**(0.5)\n",
    "                \n",
    "        \n",
    "        for group in self.param_groups:\n",
    "        \n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                \n",
    "                \n",
    "                cliped = (grad*self.C) /( torch.max(norm,torch.ones_like(norm)*self.C))\n",
    "                self.bigger_batch [p.myid].add_(cliped)\n",
    "                self.bigger_batch_count[p.myid]+=1\n",
    "                \n",
    "\n",
    "        if   self.bigger_batch_count[p.myid] == self.batch_size:\n",
    "\n",
    "\n",
    "            for group in self.param_groups:\n",
    "            \n",
    "                for p in group['params']:\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "                    base =  self.bigger_batch[p.myid]\n",
    "                    my_rand = torch.zeros_like(base) \n",
    "                    my_rand.normal_(mean=0, std =noise_std  )\n",
    "                    \n",
    "                    base.add_(my_rand)\n",
    "                \n",
    "                    base = base/float(self.batch_size) \n",
    "\n",
    "                    self.bigger_batch[p.myid] =base\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                weight_decay = group['weight_decay']\n",
    "                momentum = group['momentum']\n",
    "                dampening = group['dampening']\n",
    "                nesterov = group['nesterov']\n",
    "\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    d_p = self.bigger_batch[p.myid]\n",
    "                    if weight_decay != 0:\n",
    "                        d_p.add_(weight_decay, p.data)\n",
    "                    if momentum != 0:\n",
    "                        param_state = self.state[p]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(1 - dampening, d_p)\n",
    "                        if nesterov:\n",
    "                            d_p = d_p.add(momentum, buf)\n",
    "                        else:\n",
    "                            d_p = buf\n",
    "\n",
    "                    p.data.add_(-group['lr'], d_p)\n",
    "                    self.bigger_batch[p.myid].zero_()\n",
    "                    self.bigger_batch_count[p.myid].zero_()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kixto0Jz_f3i"
   },
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(trainloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            #loss = nn.NLLLoss(output,target)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                done = batch_idx * len(data)\n",
    "                percentage = 100. * batch_idx / len(trainloader)\n",
    "                print(f'Train Epoch: {epoch} [{done:5}/{len(trainloader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
    "\n",
    "        test(trainloader)\n",
    "        test(testloader)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            #test_loss += nn.NLLLoss(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(loader.dataset)\n",
    "        accuracy = 100. * correct / len(loader.dataset)\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "cbed03b023df42bfb594e7927a5c00bf",
      "52b6e59d68364bebbc075529a4d9ed09",
      "b296b65e0c0c40bd97c3c48fe1af5a96",
      "835c9c95bf90419fa253cee723176c83",
      "eee2e1c14fd9452f87ea07bd307b03ff",
      "2ca7abc1118f4cee8daf833d3eb35ad7",
      "d3fed02459f740538f0b79d1b062e420",
      "31f859a8befd42b899ebb1d59da433fe"
     ]
    },
    "colab_type": "code",
    "id": "nzLjJS_mgGnp",
    "outputId": "3221bf78-36c4-4dbb-c852-07b18d3e7ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbed03b023df42bfb594e7927a5c00bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 2 * 2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o0lS0N15H9e"
   },
   "outputs": [],
   "source": [
    "batchsize=64\n",
    "test_batch_size=1000\n",
    "epochs=70\n",
    "lr=0.5\n",
    "momentum=0.5\n",
    "norm_clip=1.0\n",
    "noise_multiplier=1e-6\n",
    "weight_decay=5e-4\n",
    "no_cuda=False\n",
    "microbatches=32\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "train_batch = microbatches\n",
    "test_batch = test_batch_size\n",
    "\n",
    "\n",
    "\n",
    "model = AlexNet()\n",
    "model = model.to(device)\n",
    "\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = DPSGD(model.parameters(),lr=lr,batch_size=batchsize//microbatches,C=norm_clip,noise_multiplier=noise_multiplier)#,momentum=momentum,weight_decay=weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "colab_type": "code",
    "id": "5Q2bOjc5_rTK",
    "outputId": "879118eb-662a-4a77-fa55-2d78db25fc05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [    0/50000 (  0%)]  Loss: 2.304005\n",
      "Train Epoch: 0 [12800/50000 ( 26%)]  Loss: 2.299865\n",
      "Train Epoch: 0 [25600/50000 ( 51%)]  Loss: 2.257190\n",
      "Train Epoch: 0 [38400/50000 ( 77%)]  Loss: 1.921347\n",
      "Test set: Average loss: 0.0157, Accuracy: 11200/50000 (22.40%)\n",
      "Test set: Average loss: 0.0193, Accuracy: 2569/10000 (25.69%)\n",
      "Train Epoch: 1 [    0/50000 (  0%)]  Loss: 2.009126\n",
      "Train Epoch: 1 [12800/50000 ( 26%)]  Loss: 1.899980\n",
      "Train Epoch: 1 [25600/50000 ( 51%)]  Loss: 1.877723\n",
      "Train Epoch: 1 [38400/50000 ( 77%)]  Loss: 1.786758\n",
      "Test set: Average loss: 0.0131, Accuracy: 18127/50000 (36.25%)\n",
      "Test set: Average loss: 0.0169, Accuracy: 3692/10000 (36.92%)\n",
      "Train Epoch: 2 [    0/50000 (  0%)]  Loss: 1.745809\n",
      "Train Epoch: 2 [12800/50000 ( 26%)]  Loss: 1.508871\n",
      "Train Epoch: 2 [25600/50000 ( 51%)]  Loss: 1.684474\n",
      "Train Epoch: 2 [38400/50000 ( 77%)]  Loss: 1.576610\n",
      "Test set: Average loss: 0.0120, Accuracy: 21836/50000 (43.67%)\n",
      "Test set: Average loss: 0.0146, Accuracy: 4632/10000 (46.32%)\n",
      "Train Epoch: 3 [    0/50000 (  0%)]  Loss: 1.609548\n",
      "Train Epoch: 3 [12800/50000 ( 26%)]  Loss: 1.392856\n",
      "Train Epoch: 3 [25600/50000 ( 51%)]  Loss: 1.250067\n",
      "Train Epoch: 3 [38400/50000 ( 77%)]  Loss: 1.338837\n",
      "Test set: Average loss: 0.0105, Accuracy: 25488/50000 (50.98%)\n",
      "Test set: Average loss: 0.0134, Accuracy: 5116/10000 (51.16%)\n",
      "Train Epoch: 4 [    0/50000 (  0%)]  Loss: 1.224391\n",
      "Train Epoch: 4 [12800/50000 ( 26%)]  Loss: 1.377203\n",
      "Train Epoch: 4 [25600/50000 ( 51%)]  Loss: 1.389484\n",
      "Train Epoch: 4 [38400/50000 ( 77%)]  Loss: 1.479874\n",
      "Test set: Average loss: 0.0097, Accuracy: 27431/50000 (54.86%)\n",
      "Test set: Average loss: 0.0118, Accuracy: 5726/10000 (57.26%)\n",
      "Train Epoch: 5 [    0/50000 (  0%)]  Loss: 1.198298\n",
      "Train Epoch: 5 [12800/50000 ( 26%)]  Loss: 1.130271\n",
      "Train Epoch: 5 [25600/50000 ( 51%)]  Loss: 1.094470\n",
      "Train Epoch: 5 [38400/50000 ( 77%)]  Loss: 1.185060\n",
      "Test set: Average loss: 0.0089, Accuracy: 29643/50000 (59.29%)\n",
      "Test set: Average loss: 0.0111, Accuracy: 6053/10000 (60.53%)\n",
      "Train Epoch: 6 [    0/50000 (  0%)]  Loss: 1.001720\n",
      "Train Epoch: 6 [12800/50000 ( 26%)]  Loss: 1.108263\n",
      "Train Epoch: 6 [25600/50000 ( 51%)]  Loss: 1.205593\n",
      "Train Epoch: 6 [38400/50000 ( 77%)]  Loss: 1.203436\n",
      "Test set: Average loss: 0.0086, Accuracy: 30353/50000 (60.71%)\n",
      "Test set: Average loss: 0.0111, Accuracy: 6009/10000 (60.09%)\n",
      "Train Epoch: 7 [    0/50000 (  0%)]  Loss: 1.152079\n",
      "Train Epoch: 7 [12800/50000 ( 26%)]  Loss: 1.195203\n",
      "Train Epoch: 7 [25600/50000 ( 51%)]  Loss: 1.136407\n",
      "Train Epoch: 7 [38400/50000 ( 77%)]  Loss: 1.129763\n",
      "Test set: Average loss: 0.0078, Accuracy: 32211/50000 (64.42%)\n",
      "Test set: Average loss: 0.0101, Accuracy: 6401/10000 (64.01%)\n",
      "Train Epoch: 8 [    0/50000 (  0%)]  Loss: 1.031558\n",
      "Train Epoch: 8 [12800/50000 ( 26%)]  Loss: 1.057897\n",
      "Train Epoch: 8 [25600/50000 ( 51%)]  Loss: 0.950842\n",
      "Train Epoch: 8 [38400/50000 ( 77%)]  Loss: 1.099492\n"
     ]
    }
   ],
   "source": [
    "train(125)\n",
    "test(trainloader)\n",
    "test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6bMGiWkQK88P"
   },
   "outputs": [],
   "source": [
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "def classifier_performance(model, train_loader, test_loader):\n",
    "\n",
    "    output_train_benign = []\n",
    "    train_label = []\n",
    "    for num, data in enumerate(train_loader):\n",
    "        images,labels = data\n",
    "        image_tensor= images.to(device)\n",
    "        img_variable = Variable(image_tensor, requires_grad=True)\n",
    "        output = model.forward(img_variable)\n",
    "\n",
    "        train_label.append(labels.numpy())\n",
    "        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
    "\n",
    "\n",
    "    train_label = np.concatenate(train_label)\n",
    "    output_train_benign=np.concatenate(output_train_benign)\n",
    "\n",
    "    test_label = []\n",
    "    output_test_benign = []\n",
    "\n",
    "    for num, data in enumerate(test_loader):\n",
    "        images,labels = data\n",
    "\n",
    "        image_tensor= images.to(device)\n",
    "        img_variable = Variable(image_tensor, requires_grad=True)\n",
    "\n",
    "        output = model.forward(img_variable)\n",
    "\n",
    "        test_label.append(labels.numpy())\n",
    "        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
    "\n",
    "\n",
    "    test_label = np.concatenate(test_label)\n",
    "    output_test_benign=np.concatenate(output_test_benign)\n",
    "\n",
    "\n",
    "    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n",
    "    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n",
    "\n",
    "    print('Accuracy: ', (train_acc1, test_acc1))\n",
    "\n",
    "    return output_train_benign, output_test_benign, train_label, test_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
    "    \n",
    "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
    "    confidence1 = []\n",
    "    confidence2 = []\n",
    "    acc1 = 0\n",
    "    acc2 = 0\n",
    "    for num in range(confidence_mtx1.shape[0]):\n",
    "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
    "        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n",
    "            acc1 += 1\n",
    "            \n",
    "    for num in range(confidence_mtx2.shape[0]):\n",
    "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
    "        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n",
    "            acc2 += 1\n",
    "    confidence1 = np.array(confidence1)\n",
    "    confidence2 = np.array(confidence2)\n",
    "    \n",
    "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
    "    \n",
    "    \n",
    "    #sort_confidence = np.sort(confidence1)\n",
    "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
    "    max_accuracy = 0.5\n",
    "    best_precision = 0.5\n",
    "    best_recall = 0.5\n",
    "    for num in range(len(sort_confidence)):\n",
    "        delta = sort_confidence[num]\n",
    "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
    "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
    "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
    "        if accuracy_now > max_accuracy:\n",
    "            max_accuracy = accuracy_now\n",
    "            best_precision = ratio1/(ratio1+ratio2)\n",
    "            best_recall = ratio1\n",
    "    print('membership inference accuracy is:', max_accuracy)\n",
    "    return max_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "AAhlpP4uK_Bb",
    "outputId": "a0ce4d3e-51e9-4df0-f326-e8abaa448f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  (0.87906, 0.7989)\n",
      "model accuracy for training and test- (0.87906, 0.7989)\n",
      "membership inference accuracy is: 0.5399\n",
      "Maximum Accuracy: 0.5399\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import math \n",
    "import scipy\n",
    "import sys  \n",
    "\n",
    "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
    "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
    "print(\"Maximum Accuracy:\",inference_accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DPSGD_AlexNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2ca7abc1118f4cee8daf833d3eb35ad7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31f859a8befd42b899ebb1d59da433fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52b6e59d68364bebbc075529a4d9ed09": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "835c9c95bf90419fa253cee723176c83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31f859a8befd42b899ebb1d59da433fe",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d3fed02459f740538f0b79d1b062e420",
      "value": "170500096it [00:06, 27618972.12it/s]"
     }
    },
    "b296b65e0c0c40bd97c3c48fe1af5a96": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ca7abc1118f4cee8daf833d3eb35ad7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eee2e1c14fd9452f87ea07bd307b03ff",
      "value": 1
     }
    },
    "cbed03b023df42bfb594e7927a5c00bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b296b65e0c0c40bd97c3c48fe1af5a96",
       "IPY_MODEL_835c9c95bf90419fa253cee723176c83"
      ],
      "layout": "IPY_MODEL_52b6e59d68364bebbc075529a4d9ed09"
     }
    },
    "d3fed02459f740538f0b79d1b062e420": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eee2e1c14fd9452f87ea07bd307b03ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
