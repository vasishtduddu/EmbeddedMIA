{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPSGD_AlexNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cbed03b023df42bfb594e7927a5c00bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_52b6e59d68364bebbc075529a4d9ed09",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b296b65e0c0c40bd97c3c48fe1af5a96",
              "IPY_MODEL_835c9c95bf90419fa253cee723176c83"
            ]
          }
        },
        "52b6e59d68364bebbc075529a4d9ed09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b296b65e0c0c40bd97c3c48fe1af5a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eee2e1c14fd9452f87ea07bd307b03ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ca7abc1118f4cee8daf833d3eb35ad7"
          }
        },
        "835c9c95bf90419fa253cee723176c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d3fed02459f740538f0b79d1b062e420",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:06, 27618972.12it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31f859a8befd42b899ebb1d59da433fe"
          }
        },
        "eee2e1c14fd9452f87ea07bd307b03ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ca7abc1118f4cee8daf833d3eb35ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3fed02459f740538f0b79d1b062e420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31f859a8befd42b899ebb1d59da433fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7alZV0YJfa9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import math\n",
        "import torch\n",
        "\n",
        "import uuid\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_4Tu-ZIfr-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PI= torch.cuda.FloatTensor([math.pi])\n",
        "NORM = (torch.sqrt(2.0*PI))\n",
        "def norm_pdf(x,mean,std):\n",
        "    y= (x-mean)/std\n",
        "    return (torch.exp( -(y).pow(2)/2.0)/NORM)/std\n",
        "\n",
        "\n",
        "class DPSGD(Optimizer):\n",
        "    \n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False,C=1,noise_multiplier= 1.0 , batch_size=256):\n",
        "        \n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(DPSGD, self).__init__(params, defaults)\n",
        "\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.C = C\n",
        "        self.bigger_batch = {}\n",
        "\n",
        "\n",
        "        self.bigger_batch_count = {}\n",
        "        self.noise_multiplier = noise_multiplier\n",
        "        \n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        noise_std =self.noise_multiplier *self.C\n",
        "        \n",
        "        loss = None\n",
        "        \n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        norm  = 0\n",
        "            \n",
        "        for group in self.param_groups:\n",
        "        \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                \n",
        "                if not hasattr(p,'myid'):\n",
        "                    p.myid = uuid.uuid4()\n",
        "                    self.bigger_batch[p.myid] = torch.zeros_like(grad)\n",
        "                    \n",
        "                    if self.bigger_batch[p.myid].is_cuda:\n",
        "                        self.bigger_batch_count[p.myid] = torch.cuda.LongTensor(size=[1]).zero_()\n",
        "                    else:\n",
        "                        self.bigger_batch_count[p.myid] = torch.LongTensor(size=[1]).zero_()\n",
        "\n",
        "                norm+=grad.norm()**2.0\n",
        "        \n",
        "        norm=norm**(0.5)\n",
        "                \n",
        "        \n",
        "        for group in self.param_groups:\n",
        "        \n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "                \n",
        "                \n",
        "                cliped = (grad*self.C) /( torch.max(norm,torch.ones_like(norm)*self.C))\n",
        "                self.bigger_batch [p.myid].add_(cliped)\n",
        "                self.bigger_batch_count[p.myid]+=1\n",
        "                \n",
        "\n",
        "        if   self.bigger_batch_count[p.myid] == self.batch_size:\n",
        "\n",
        "\n",
        "            for group in self.param_groups:\n",
        "            \n",
        "                for p in group['params']:\n",
        "                    \n",
        "                    \n",
        "\n",
        "                    \n",
        "                    base =  self.bigger_batch[p.myid]\n",
        "                    my_rand = torch.zeros_like(base) \n",
        "                    my_rand.normal_(mean=0, std =noise_std  )\n",
        "                    \n",
        "                    base.add_(my_rand)\n",
        "                \n",
        "                    base = base/float(self.batch_size) \n",
        "\n",
        "                    self.bigger_batch[p.myid] =base\n",
        "\n",
        "            for group in self.param_groups:\n",
        "                weight_decay = group['weight_decay']\n",
        "                momentum = group['momentum']\n",
        "                dampening = group['dampening']\n",
        "                nesterov = group['nesterov']\n",
        "\n",
        "                for p in group['params']:\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "                    d_p = self.bigger_batch[p.myid]\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "                    if momentum != 0:\n",
        "                        param_state = self.state[p]\n",
        "                        if 'momentum_buffer' not in param_state:\n",
        "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                        else:\n",
        "                            buf = param_state['momentum_buffer']\n",
        "                            buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                        if nesterov:\n",
        "                            d_p = d_p.add(momentum, buf)\n",
        "                        else:\n",
        "                            d_p = buf\n",
        "\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    self.bigger_batch[p.myid].zero_()\n",
        "                    self.bigger_batch_count[p.myid].zero_()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kixto0Jz_f3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(trainloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            #loss = nn.NLLLoss(output,target)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if batch_idx % 100 == 0:\n",
        "                done = batch_idx * len(data)\n",
        "                percentage = 100. * batch_idx / len(trainloader)\n",
        "                print(f'Train Epoch: {epoch} [{done:5}/{len(trainloader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
        "\n",
        "        test(trainloader)\n",
        "        test(testloader)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            #test_loss += nn.NLLLoss(output, target).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(loader.dataset)\n",
        "        accuracy = 100. * correct / len(loader.dataset)\n",
        "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
        "    return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzLjJS_mgGnp",
        "colab_type": "code",
        "outputId": "3221bf78-36c4-4dbb-c852-07b18d3e7ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "cbed03b023df42bfb594e7927a5c00bf",
            "52b6e59d68364bebbc075529a4d9ed09",
            "b296b65e0c0c40bd97c3c48fe1af5a96",
            "835c9c95bf90419fa253cee723176c83",
            "eee2e1c14fd9452f87ea07bd307b03ff",
            "2ca7abc1118f4cee8daf833d3eb35ad7",
            "d3fed02459f740538f0b79d1b062e420",
            "31f859a8befd42b899ebb1d59da433fe"
          ]
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=1)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbed03b023df42bfb594e7927a5c00bf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o0lS0N15H9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsize=64\n",
        "test_batch_size=1000\n",
        "epochs=70\n",
        "lr=0.5\n",
        "momentum=0.5\n",
        "norm_clip=1.0\n",
        "noise_multiplier=1e-6\n",
        "weight_decay=5e-4\n",
        "no_cuda=False\n",
        "microbatches=32\n",
        "\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "\n",
        "train_batch = microbatches\n",
        "test_batch = test_batch_size\n",
        "\n",
        "\n",
        "\n",
        "model = AlexNet()\n",
        "model = model.to(device)\n",
        "\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = DPSGD(model.parameters(),lr=lr,batch_size=batchsize//microbatches,C=norm_clip,noise_multiplier=noise_multiplier)#,momentum=momentum,weight_decay=weight_decay)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2bOjc5_rTK",
        "colab_type": "code",
        "outputId": "879118eb-662a-4a77-fa55-2d78db25fc05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "source": [
        "train(125)\n",
        "test(trainloader)\n",
        "test(testloader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [    0/50000 (  0%)]  Loss: 2.304005\n",
            "Train Epoch: 0 [12800/50000 ( 26%)]  Loss: 2.299865\n",
            "Train Epoch: 0 [25600/50000 ( 51%)]  Loss: 2.257190\n",
            "Train Epoch: 0 [38400/50000 ( 77%)]  Loss: 1.921347\n",
            "Test set: Average loss: 0.0157, Accuracy: 11200/50000 (22.40%)\n",
            "Test set: Average loss: 0.0193, Accuracy: 2569/10000 (25.69%)\n",
            "Train Epoch: 1 [    0/50000 (  0%)]  Loss: 2.009126\n",
            "Train Epoch: 1 [12800/50000 ( 26%)]  Loss: 1.899980\n",
            "Train Epoch: 1 [25600/50000 ( 51%)]  Loss: 1.877723\n",
            "Train Epoch: 1 [38400/50000 ( 77%)]  Loss: 1.786758\n",
            "Test set: Average loss: 0.0131, Accuracy: 18127/50000 (36.25%)\n",
            "Test set: Average loss: 0.0169, Accuracy: 3692/10000 (36.92%)\n",
            "Train Epoch: 2 [    0/50000 (  0%)]  Loss: 1.745809\n",
            "Train Epoch: 2 [12800/50000 ( 26%)]  Loss: 1.508871\n",
            "Train Epoch: 2 [25600/50000 ( 51%)]  Loss: 1.684474\n",
            "Train Epoch: 2 [38400/50000 ( 77%)]  Loss: 1.576610\n",
            "Test set: Average loss: 0.0120, Accuracy: 21836/50000 (43.67%)\n",
            "Test set: Average loss: 0.0146, Accuracy: 4632/10000 (46.32%)\n",
            "Train Epoch: 3 [    0/50000 (  0%)]  Loss: 1.609548\n",
            "Train Epoch: 3 [12800/50000 ( 26%)]  Loss: 1.392856\n",
            "Train Epoch: 3 [25600/50000 ( 51%)]  Loss: 1.250067\n",
            "Train Epoch: 3 [38400/50000 ( 77%)]  Loss: 1.338837\n",
            "Test set: Average loss: 0.0105, Accuracy: 25488/50000 (50.98%)\n",
            "Test set: Average loss: 0.0134, Accuracy: 5116/10000 (51.16%)\n",
            "Train Epoch: 4 [    0/50000 (  0%)]  Loss: 1.224391\n",
            "Train Epoch: 4 [12800/50000 ( 26%)]  Loss: 1.377203\n",
            "Train Epoch: 4 [25600/50000 ( 51%)]  Loss: 1.389484\n",
            "Train Epoch: 4 [38400/50000 ( 77%)]  Loss: 1.479874\n",
            "Test set: Average loss: 0.0097, Accuracy: 27431/50000 (54.86%)\n",
            "Test set: Average loss: 0.0118, Accuracy: 5726/10000 (57.26%)\n",
            "Train Epoch: 5 [    0/50000 (  0%)]  Loss: 1.198298\n",
            "Train Epoch: 5 [12800/50000 ( 26%)]  Loss: 1.130271\n",
            "Train Epoch: 5 [25600/50000 ( 51%)]  Loss: 1.094470\n",
            "Train Epoch: 5 [38400/50000 ( 77%)]  Loss: 1.185060\n",
            "Test set: Average loss: 0.0089, Accuracy: 29643/50000 (59.29%)\n",
            "Test set: Average loss: 0.0111, Accuracy: 6053/10000 (60.53%)\n",
            "Train Epoch: 6 [    0/50000 (  0%)]  Loss: 1.001720\n",
            "Train Epoch: 6 [12800/50000 ( 26%)]  Loss: 1.108263\n",
            "Train Epoch: 6 [25600/50000 ( 51%)]  Loss: 1.205593\n",
            "Train Epoch: 6 [38400/50000 ( 77%)]  Loss: 1.203436\n",
            "Test set: Average loss: 0.0086, Accuracy: 30353/50000 (60.71%)\n",
            "Test set: Average loss: 0.0111, Accuracy: 6009/10000 (60.09%)\n",
            "Train Epoch: 7 [    0/50000 (  0%)]  Loss: 1.152079\n",
            "Train Epoch: 7 [12800/50000 ( 26%)]  Loss: 1.195203\n",
            "Train Epoch: 7 [25600/50000 ( 51%)]  Loss: 1.136407\n",
            "Train Epoch: 7 [38400/50000 ( 77%)]  Loss: 1.129763\n",
            "Test set: Average loss: 0.0078, Accuracy: 32211/50000 (64.42%)\n",
            "Test set: Average loss: 0.0101, Accuracy: 6401/10000 (64.01%)\n",
            "Train Epoch: 8 [    0/50000 (  0%)]  Loss: 1.031558\n",
            "Train Epoch: 8 [12800/50000 ( 26%)]  Loss: 1.057897\n",
            "Train Epoch: 8 [25600/50000 ( 51%)]  Loss: 0.950842\n",
            "Train Epoch: 8 [38400/50000 ( 77%)]  Loss: 1.099492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bMGiWkQK88P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_by_row(logits, T = 1.0):\n",
        "    mx = np.max(logits, axis=-1, keepdims=True)\n",
        "    exp = np.exp((logits - mx)/T)\n",
        "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
        "    return exp/denominator\n",
        "\n",
        "def classifier_performance(model, train_loader, test_loader):\n",
        "\n",
        "    output_train_benign = []\n",
        "    train_label = []\n",
        "    for num, data in enumerate(train_loader):\n",
        "        images,labels = data\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        train_label.append(labels.numpy())\n",
        "        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    train_label = np.concatenate(train_label)\n",
        "    output_train_benign=np.concatenate(output_train_benign)\n",
        "\n",
        "    test_label = []\n",
        "    output_test_benign = []\n",
        "\n",
        "    for num, data in enumerate(test_loader):\n",
        "        images,labels = data\n",
        "\n",
        "        image_tensor= images.to(device)\n",
        "        img_variable = Variable(image_tensor, requires_grad=True)\n",
        "\n",
        "        output = model.forward(img_variable)\n",
        "\n",
        "        test_label.append(labels.numpy())\n",
        "        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
        "\n",
        "\n",
        "    test_label = np.concatenate(test_label)\n",
        "    output_test_benign=np.concatenate(output_test_benign)\n",
        "\n",
        "\n",
        "    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n",
        "    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n",
        "\n",
        "    print('Accuracy: ', (train_acc1, test_acc1))\n",
        "\n",
        "    return output_train_benign, output_test_benign, train_label, test_label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
        "    \n",
        "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
        "    confidence1 = []\n",
        "    confidence2 = []\n",
        "    acc1 = 0\n",
        "    acc2 = 0\n",
        "    for num in range(confidence_mtx1.shape[0]):\n",
        "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
        "        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n",
        "            acc1 += 1\n",
        "            \n",
        "    for num in range(confidence_mtx2.shape[0]):\n",
        "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
        "        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n",
        "            acc2 += 1\n",
        "    confidence1 = np.array(confidence1)\n",
        "    confidence2 = np.array(confidence2)\n",
        "    \n",
        "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
        "    \n",
        "    \n",
        "    #sort_confidence = np.sort(confidence1)\n",
        "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
        "    max_accuracy = 0.5\n",
        "    best_precision = 0.5\n",
        "    best_recall = 0.5\n",
        "    for num in range(len(sort_confidence)):\n",
        "        delta = sort_confidence[num]\n",
        "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
        "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
        "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
        "        if accuracy_now > max_accuracy:\n",
        "            max_accuracy = accuracy_now\n",
        "            best_precision = ratio1/(ratio1+ratio2)\n",
        "            best_recall = ratio1\n",
        "    print('membership inference accuracy is:', max_accuracy)\n",
        "    return max_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAhlpP4uK_Bb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a0ce4d3e-51e9-4df0-f326-e8abaa448f43"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import os\n",
        "import numpy as np\n",
        "import math \n",
        "import scipy\n",
        "import sys  \n",
        "\n",
        "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
        "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
        "print(\"Maximum Accuracy:\",inference_accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  (0.87906, 0.7989)\n",
            "model accuracy for training and test- (0.87906, 0.7989)\n",
            "membership inference accuracy is: 0.5399\n",
            "Maximum Accuracy: 0.5399\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}