{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of KD_AlexNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO91qCCqdqfk6V2Biv7bKVS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0t-hPybLM4vN","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as fcnal\n","from sklearn.pipeline import Pipeline\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","from collections import OrderedDict\n","import math\n","\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlJUHUVrhCil","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","\n","\n","class Net(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(Net, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 2 * 2, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), 256 * 2 * 2)\n","        x = self.classifier(x)\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZGLo1B4Nsgs","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","class BinActive(torch.autograd.Function):\n","    '''\n","    Binarize the input activations and calculate the mean across channel dimension.\n","    '''\n","    def forward(self, input):\n","        self.save_for_backward(input)\n","        size = input.size()\n","        mean = torch.mean(input.abs(), 1, keepdim=True)\n","        input = input.sign()\n","        return input, mean\n","\n","    def backward(self, grad_output, grad_output_mean):\n","        input, = self.saved_tensors\n","        grad_input = grad_output.clone()\n","        grad_input[input.ge(1)] = 0\n","        grad_input[input.le(-1)] = 0\n","        return grad_input\n","\n","class BinConv2d(nn.Module):\n","    def __init__(self, input_channels, output_channels,\n","            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n","        super(BinConv2d, self).__init__()\n","        self.layer_type = 'BinConv2d'\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.dropout_ratio = 0\n","        dropout=0\n","        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n","        self.bn.weight.data = self.bn.weight.data.zero_().add(1.0)\n","        if dropout!=0:\n","            self.dropout = nn.Dropout(dropout)\n","        self.conv = nn.Conv2d(input_channels, output_channels,\n","                kernel_size=kernel_size, stride=stride, padding=padding)\n","        self.relu = nn.ReLU(inplace=True)\n","    \n","    def forward(self, x):\n","        x = self.bn(x)\n","        x, mean = BinActive()(x)\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        return x\n","\n","class BinNet(nn.Module):\n","\n","    def __init__(self, num_classes=10):\n","        super(BinNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=2, padding=2),\n","            nn.BatchNorm2d(64, eps=1e-4, momentum=0.1, affine=False),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            BinConv2d(64, 192, stride=1, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            BinConv2d(192, 384, stride=1, kernel_size=3, padding=2),\n","            nn.BatchNorm2d(384, eps=1e-4, momentum=0.1, affine=False),\n","            nn.ReLU(inplace=True),\n","            BinConv2d(384, 256, stride=1, kernel_size=3, padding=2),\n","            nn.BatchNorm2d(256, eps=1e-4, momentum=0.1, affine=False),\n","            nn.ReLU(inplace=True),\n","            BinConv2d(256, 256, stride=1, kernel_size=3, padding=2),\n","            nn.BatchNorm2d(256, eps=1e-4, momentum=0.1, affine=False),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXiyRQx2jmW-","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import numpy\n","\n","class BinOp():\n","    def __init__(self, model):\n","        # count the number of Conv2d\n","        count_Conv2d = 0\n","        for m in model.modules():\n","            if isinstance(m, nn.Conv2d):\n","                count_Conv2d = count_Conv2d + 1\n","\n","        start_range = 1\n","        end_range = count_Conv2d-2\n","        self.bin_range = numpy.linspace(start_range,\n","                end_range, end_range-start_range+1)\\\n","                        .astype('int').tolist()\n","        self.num_of_params = len(self.bin_range)\n","        self.saved_params = []\n","        self.target_params = []\n","        self.target_modules = []\n","        index = -1\n","        for m in model.modules():\n","            if isinstance(m, nn.Conv2d):\n","                index = index + 1\n","                if index in self.bin_range:\n","                    tmp = m.weight.data.clone()\n","                    self.saved_params.append(tmp)\n","                    self.target_modules.append(m.weight)\n","\n","    def binarization(self):\n","        self.meancenterConvParams()\n","        self.clampConvParams()\n","        self.save_params()\n","        self.binarizeConvParams()\n","\n","    def meancenterConvParams(self):\n","        for index in range(self.num_of_params):\n","            s = self.target_modules[index].data.size()\n","            negMean = self.target_modules[index].data.mean(1, keepdim=True).\\\n","                    mul(-1).expand_as(self.target_modules[index].data)\n","            self.target_modules[index].data = self.target_modules[index].data.add(negMean)\n","\n","    def clampConvParams(self):\n","        for index in range(self.num_of_params):\n","            self.target_modules[index].data = \\\n","                    self.target_modules[index].data.clamp(-1.0, 1.0)\n","\n","    def save_params(self):\n","        for index in range(self.num_of_params):\n","            self.saved_params[index].copy_(self.target_modules[index].data)\n","\n","    def binarizeConvParams(self):\n","        for index in range(self.num_of_params):\n","            n = self.target_modules[index].data[0].nelement()\n","            s = self.target_modules[index].data.size()\n","            m = self.target_modules[index].data.norm(1, 3, keepdim=True)\\\n","                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n)\n","            self.target_modules[index].data = \\\n","                    self.target_modules[index].data.sign().mul(m.expand(s))\n","\n","    def restore(self):\n","        for index in range(self.num_of_params):\n","            self.target_modules[index].data.copy_(self.saved_params[index])\n","\n","    def updateBinaryGradWeight(self):\n","        for index in range(self.num_of_params):\n","            weight = self.target_modules[index].data\n","            n = weight[0].nelement()\n","            s = weight.size()\n","            m = weight.norm(1, 3, keepdim=True)\\\n","                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n","            m[weight.lt(-1.0)] = 0 \n","            m[weight.gt(1.0)] = 0\n","            # m = m.add(1.0/n).mul(1.0-1.0/s[1]).mul(n)\n","            # self.target_modules[index].grad.data = \\\n","            #         self.target_modules[index].grad.data.mul(m)\n","            m = m.mul(self.target_modules[index].grad.data)\n","            m_add = weight.sign().mul(self.target_modules[index].grad.data)\n","            m_add = m_add.sum(3, keepdim=True)\\\n","                    .sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n","            m_add = m_add.mul(weight.sign())\n","            self.target_modules[index].grad.data = m.add(m_add).mul(1.0-1.0/s[1]).mul(n)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UM9Y1oghNyeU","colab_type":"code","outputId":"b7cad07c-8615-4ebd-90a7-4e988e3b7e05","executionInfo":{"status":"ok","timestamp":1581156810053,"user_tz":-330,"elapsed":2919,"user":{"displayName":"Vasisht Vasisht","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAVwnMADhdoq0kiAbKetXytg58LjCY-cepd5I4m=s64","userId":"18321686459009329525"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["batch_size=128\n","lr=1e-3\n","log_interval=100\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XnXlrLa9NwbA","colab_type":"code","colab":{}},"source":["def train(model, epochs):\n","    model.train()\n","    for epoch in range(epochs):\n","\n","        for batch_idx, (data, target) in enumerate(trainloader):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            #loss = nn.NLLLoss(output,target)\n","            loss.backward()\n","\n","            optimizer.step()\n","            if batch_idx % 100 == 0:\n","                done = batch_idx * len(data)\n","                percentage = 100. * batch_idx / len(trainloader)\n","                print(f'Train Epoch: {epoch} [{done:5}/{len(trainloader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n","\n","        test(model, trainloader)\n","        test(model, testloader)\n","\n","\n","def test(model, loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += criterion(output, target).item() # sum up batch loss\n","            #test_loss += nn.NLLLoss(output, target).item()\n","            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n","            correct += pred.eq(target.data.view_as(pred)).sum().item()\n","\n","        test_loss /= len(loader.dataset)\n","        accuracy = 100. * correct / len(loader.dataset)\n","        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n","    return accuracy\n","\n","\n","\n","\n","\n","\n","def distill_training(teacher=None, learner=None, data_loader=None,\n","                     test_loader=None, optimizer=None,\n","                     criterion=None, n_epochs=0):\n","    \"\"\"\n","    :param teacher: network to provide soft labels in training\n","    :param learner: network to distill knowledge into\n","    :param data_loader: data loader for training data set\n","    :param test_loaderL data loader for validation data\n","    :param optimizer: optimizer for training\n","    :param criterion: objective function, should allow for soft labels.\n","                      We suggest softCrossEntropy\n","    :param n_epochs: epochs for training\n","    :param verbose: verbose == True will print loss at each batch\n","    :return: None, teacher model is trained in place\n","    \"\"\"\n","    losses = []\n","    for epoch in range(n_epochs):\n","        teacher.eval()\n","        learner.train()\n","        print(\"[{}/{}] \".format(epoch, n_epochs))\n","        for i, batch in enumerate(data_loader):\n","            with torch.set_grad_enabled(False):\n","                data, labels = batch\n","                data, labels = data.to(device), labels.to(device)\n","                soft_lables = teacher(data)\n","\n","            with torch.set_grad_enabled(True):\n","                optimizer.zero_grad()\n","                outputs = learner(data)\n","                loss = criterion(outputs, torch.max(soft_lables.type(torch.cuda.LongTensor), 1)[1])#, labels)\n","                loss.backward()\n","                optimizer.step()\n","                losses.append(loss.item())\n","\n","            if i%100==0:\n","                done= i * len(data)\n","                percentage = 100. * i / len(data_loader)\n","                print(f'Train Epoch: {epoch} [{done:5}/{len(data_loader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n","\n","        # evaluate performance on testset at the end of each epoch\n","       \n","\n","        train_acc = test(learner, data_loader)\n","        test_acc = test(learner, test_loader)\n","    return train_acc, test_acc\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tAWwRzdKOB2a","colab_type":"code","outputId":"58755a9f-7f02-40ff-905f-fc7f40d0994d","executionInfo":{"status":"error","timestamp":1581159557548,"user_tz":-330,"elapsed":2740684,"user":{"displayName":"Vasisht Vasisht","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAVwnMADhdoq0kiAbKetXytg58LjCY-cepd5I4m=s64","userId":"18321686459009329525"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Model\n","print('==> Building model..')\n","teacher = Net()\n","teacher = teacher.to(device)\n","teacher = torch.nn.DataParallel(teacher)\n","\n","criterion = nn.CrossEntropyLoss()\n","#optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=0)\n","optimizer = optim.Adam(teacher.parameters(), lr=1e-3, weight_decay=0.0000)\n","\n","train(teacher,  150)\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["==> Building model..\n","Train Epoch: 0 [    0/50000 (  0%)]  Loss: 2.303069\n","Train Epoch: 0 [12800/50000 ( 26%)]  Loss: 1.680549\n","Train Epoch: 0 [25600/50000 ( 51%)]  Loss: 1.618903\n","Train Epoch: 0 [38400/50000 ( 77%)]  Loss: 1.736200\n","Test set: Average loss: 0.0123, Accuracy: 20597/50000 (41.19%)\n","Test set: Average loss: 0.0151, Accuracy: 4328/10000 (43.28%)\n","Train Epoch: 1 [    0/50000 (  0%)]  Loss: 1.531754\n","Train Epoch: 1 [12800/50000 ( 26%)]  Loss: 1.524465\n","Train Epoch: 1 [25600/50000 ( 51%)]  Loss: 1.288069\n","Train Epoch: 1 [38400/50000 ( 77%)]  Loss: 1.166009\n","Test set: Average loss: 0.0105, Accuracy: 25661/50000 (51.32%)\n","Test set: Average loss: 0.0128, Accuracy: 5425/10000 (54.25%)\n","Train Epoch: 2 [    0/50000 (  0%)]  Loss: 1.365436\n","Train Epoch: 2 [12800/50000 ( 26%)]  Loss: 1.369176\n","Train Epoch: 2 [25600/50000 ( 51%)]  Loss: 1.345633\n","Train Epoch: 2 [38400/50000 ( 77%)]  Loss: 1.252254\n","Test set: Average loss: 0.0090, Accuracy: 29241/50000 (58.48%)\n","Test set: Average loss: 0.0112, Accuracy: 5971/10000 (59.71%)\n","Train Epoch: 3 [    0/50000 (  0%)]  Loss: 1.138909\n","Train Epoch: 3 [12800/50000 ( 26%)]  Loss: 1.016517\n","Train Epoch: 3 [25600/50000 ( 51%)]  Loss: 1.238734\n","Train Epoch: 3 [38400/50000 ( 77%)]  Loss: 1.056119\n","Test set: Average loss: 0.0082, Accuracy: 31200/50000 (62.40%)\n","Test set: Average loss: 0.0104, Accuracy: 6268/10000 (62.68%)\n","Train Epoch: 4 [    0/50000 (  0%)]  Loss: 1.056316\n","Train Epoch: 4 [12800/50000 ( 26%)]  Loss: 1.254107\n","Train Epoch: 4 [25600/50000 ( 51%)]  Loss: 0.947455\n","Train Epoch: 4 [38400/50000 ( 77%)]  Loss: 0.935642\n","Test set: Average loss: 0.0081, Accuracy: 31473/50000 (62.95%)\n","Test set: Average loss: 0.0102, Accuracy: 6352/10000 (63.52%)\n","Train Epoch: 5 [    0/50000 (  0%)]  Loss: 0.954431\n","Train Epoch: 5 [12800/50000 ( 26%)]  Loss: 0.875210\n","Train Epoch: 5 [25600/50000 ( 51%)]  Loss: 1.141746\n","Train Epoch: 5 [38400/50000 ( 77%)]  Loss: 0.970874\n","Test set: Average loss: 0.0077, Accuracy: 32762/50000 (65.52%)\n","Test set: Average loss: 0.0098, Accuracy: 6606/10000 (66.06%)\n","Train Epoch: 6 [    0/50000 (  0%)]  Loss: 0.893845\n","Train Epoch: 6 [12800/50000 ( 26%)]  Loss: 0.908093\n","Train Epoch: 6 [25600/50000 ( 51%)]  Loss: 0.965353\n","Train Epoch: 6 [38400/50000 ( 77%)]  Loss: 0.930915\n","Test set: Average loss: 0.0069, Accuracy: 34444/50000 (68.89%)\n","Test set: Average loss: 0.0090, Accuracy: 6894/10000 (68.94%)\n","Train Epoch: 7 [    0/50000 (  0%)]  Loss: 0.952558\n","Train Epoch: 7 [12800/50000 ( 26%)]  Loss: 0.885823\n","Train Epoch: 7 [25600/50000 ( 51%)]  Loss: 0.822218\n","Train Epoch: 7 [38400/50000 ( 77%)]  Loss: 0.775811\n","Test set: Average loss: 0.0066, Accuracy: 35179/50000 (70.36%)\n","Test set: Average loss: 0.0087, Accuracy: 7001/10000 (70.01%)\n","Train Epoch: 8 [    0/50000 (  0%)]  Loss: 0.865510\n","Train Epoch: 8 [12800/50000 ( 26%)]  Loss: 0.826779\n","Train Epoch: 8 [25600/50000 ( 51%)]  Loss: 0.965720\n","Train Epoch: 8 [38400/50000 ( 77%)]  Loss: 0.743604\n","Test set: Average loss: 0.0063, Accuracy: 35985/50000 (71.97%)\n","Test set: Average loss: 0.0083, Accuracy: 7148/10000 (71.48%)\n","Train Epoch: 9 [    0/50000 (  0%)]  Loss: 0.691870\n","Train Epoch: 9 [12800/50000 ( 26%)]  Loss: 0.717227\n","Train Epoch: 9 [25600/50000 ( 51%)]  Loss: 0.660446\n","Train Epoch: 9 [38400/50000 ( 77%)]  Loss: 0.694125\n","Test set: Average loss: 0.0062, Accuracy: 36023/50000 (72.05%)\n","Test set: Average loss: 0.0084, Accuracy: 7074/10000 (70.74%)\n","Train Epoch: 10 [    0/50000 (  0%)]  Loss: 0.716850\n","Train Epoch: 10 [12800/50000 ( 26%)]  Loss: 0.536325\n","Train Epoch: 10 [25600/50000 ( 51%)]  Loss: 0.892396\n","Train Epoch: 10 [38400/50000 ( 77%)]  Loss: 0.642219\n","Test set: Average loss: 0.0059, Accuracy: 36681/50000 (73.36%)\n","Test set: Average loss: 0.0080, Accuracy: 7213/10000 (72.13%)\n","Train Epoch: 11 [    0/50000 (  0%)]  Loss: 0.829197\n","Train Epoch: 11 [12800/50000 ( 26%)]  Loss: 0.871228\n","Train Epoch: 11 [25600/50000 ( 51%)]  Loss: 0.896841\n","Train Epoch: 11 [38400/50000 ( 77%)]  Loss: 0.743785\n","Test set: Average loss: 0.0058, Accuracy: 36934/50000 (73.87%)\n","Test set: Average loss: 0.0080, Accuracy: 7183/10000 (71.83%)\n","Train Epoch: 12 [    0/50000 (  0%)]  Loss: 0.774064\n","Train Epoch: 12 [12800/50000 ( 26%)]  Loss: 0.815290\n","Train Epoch: 12 [25600/50000 ( 51%)]  Loss: 0.802836\n","Train Epoch: 12 [38400/50000 ( 77%)]  Loss: 0.640972\n","Test set: Average loss: 0.0057, Accuracy: 37254/50000 (74.51%)\n","Test set: Average loss: 0.0079, Accuracy: 7287/10000 (72.87%)\n","Train Epoch: 13 [    0/50000 (  0%)]  Loss: 0.814070\n","Train Epoch: 13 [12800/50000 ( 26%)]  Loss: 0.745626\n","Train Epoch: 13 [25600/50000 ( 51%)]  Loss: 0.671468\n","Train Epoch: 13 [38400/50000 ( 77%)]  Loss: 0.676194\n","Test set: Average loss: 0.0057, Accuracy: 37309/50000 (74.62%)\n","Test set: Average loss: 0.0080, Accuracy: 7337/10000 (73.37%)\n","Train Epoch: 14 [    0/50000 (  0%)]  Loss: 0.776492\n","Train Epoch: 14 [12800/50000 ( 26%)]  Loss: 0.656584\n","Train Epoch: 14 [25600/50000 ( 51%)]  Loss: 0.636221\n","Train Epoch: 14 [38400/50000 ( 77%)]  Loss: 0.758741\n","Test set: Average loss: 0.0055, Accuracy: 37771/50000 (75.54%)\n","Test set: Average loss: 0.0077, Accuracy: 7380/10000 (73.80%)\n","Train Epoch: 15 [    0/50000 (  0%)]  Loss: 0.893265\n","Train Epoch: 15 [12800/50000 ( 26%)]  Loss: 0.639673\n","Train Epoch: 15 [25600/50000 ( 51%)]  Loss: 0.855528\n","Train Epoch: 15 [38400/50000 ( 77%)]  Loss: 0.554998\n","Test set: Average loss: 0.0052, Accuracy: 38339/50000 (76.68%)\n","Test set: Average loss: 0.0073, Accuracy: 7484/10000 (74.84%)\n","Train Epoch: 16 [    0/50000 (  0%)]  Loss: 0.482290\n","Train Epoch: 16 [12800/50000 ( 26%)]  Loss: 0.677146\n","Train Epoch: 16 [25600/50000 ( 51%)]  Loss: 0.659009\n","Train Epoch: 16 [38400/50000 ( 77%)]  Loss: 0.615890\n","Test set: Average loss: 0.0054, Accuracy: 37965/50000 (75.93%)\n","Test set: Average loss: 0.0077, Accuracy: 7338/10000 (73.38%)\n","Train Epoch: 17 [    0/50000 (  0%)]  Loss: 0.671567\n","Train Epoch: 17 [12800/50000 ( 26%)]  Loss: 0.809138\n","Train Epoch: 17 [25600/50000 ( 51%)]  Loss: 0.630573\n","Train Epoch: 17 [38400/50000 ( 77%)]  Loss: 0.618576\n","Test set: Average loss: 0.0052, Accuracy: 38474/50000 (76.95%)\n","Test set: Average loss: 0.0074, Accuracy: 7521/10000 (75.21%)\n","Train Epoch: 18 [    0/50000 (  0%)]  Loss: 0.639731\n","Train Epoch: 18 [12800/50000 ( 26%)]  Loss: 0.783179\n","Train Epoch: 18 [25600/50000 ( 51%)]  Loss: 0.459093\n","Train Epoch: 18 [38400/50000 ( 77%)]  Loss: 0.624917\n","Test set: Average loss: 0.0049, Accuracy: 39079/50000 (78.16%)\n","Test set: Average loss: 0.0069, Accuracy: 7620/10000 (76.20%)\n","Train Epoch: 19 [    0/50000 (  0%)]  Loss: 0.735939\n","Train Epoch: 19 [12800/50000 ( 26%)]  Loss: 0.526556\n","Train Epoch: 19 [25600/50000 ( 51%)]  Loss: 0.716577\n","Train Epoch: 19 [38400/50000 ( 77%)]  Loss: 0.573254\n","Test set: Average loss: 0.0047, Accuracy: 39434/50000 (78.87%)\n","Test set: Average loss: 0.0070, Accuracy: 7608/10000 (76.08%)\n","Train Epoch: 20 [    0/50000 (  0%)]  Loss: 0.568297\n","Train Epoch: 20 [12800/50000 ( 26%)]  Loss: 0.604267\n","Train Epoch: 20 [25600/50000 ( 51%)]  Loss: 0.617552\n","Train Epoch: 20 [38400/50000 ( 77%)]  Loss: 0.598423\n","Test set: Average loss: 0.0047, Accuracy: 39410/50000 (78.82%)\n","Test set: Average loss: 0.0069, Accuracy: 7661/10000 (76.61%)\n","Train Epoch: 21 [    0/50000 (  0%)]  Loss: 0.553503\n","Train Epoch: 21 [12800/50000 ( 26%)]  Loss: 0.595149\n","Train Epoch: 21 [25600/50000 ( 51%)]  Loss: 0.693668\n","Train Epoch: 21 [38400/50000 ( 77%)]  Loss: 0.579539\n","Test set: Average loss: 0.0048, Accuracy: 39218/50000 (78.44%)\n","Test set: Average loss: 0.0072, Accuracy: 7618/10000 (76.18%)\n","Train Epoch: 22 [    0/50000 (  0%)]  Loss: 0.758167\n","Train Epoch: 22 [12800/50000 ( 26%)]  Loss: 0.548933\n","Train Epoch: 22 [25600/50000 ( 51%)]  Loss: 0.451065\n","Train Epoch: 22 [38400/50000 ( 77%)]  Loss: 0.647753\n","Test set: Average loss: 0.0048, Accuracy: 39447/50000 (78.89%)\n","Test set: Average loss: 0.0073, Accuracy: 7644/10000 (76.44%)\n","Train Epoch: 23 [    0/50000 (  0%)]  Loss: 0.674574\n","Train Epoch: 23 [12800/50000 ( 26%)]  Loss: 0.599492\n","Train Epoch: 23 [25600/50000 ( 51%)]  Loss: 0.786864\n","Train Epoch: 23 [38400/50000 ( 77%)]  Loss: 0.547804\n","Test set: Average loss: 0.0047, Accuracy: 39412/50000 (78.82%)\n","Test set: Average loss: 0.0074, Accuracy: 7587/10000 (75.87%)\n","Train Epoch: 24 [    0/50000 (  0%)]  Loss: 0.524834\n","Train Epoch: 24 [12800/50000 ( 26%)]  Loss: 0.626629\n","Train Epoch: 24 [25600/50000 ( 51%)]  Loss: 0.539267\n","Train Epoch: 24 [38400/50000 ( 77%)]  Loss: 0.725875\n","Test set: Average loss: 0.0045, Accuracy: 39990/50000 (79.98%)\n","Test set: Average loss: 0.0070, Accuracy: 7644/10000 (76.44%)\n","Train Epoch: 25 [    0/50000 (  0%)]  Loss: 0.624238\n","Train Epoch: 25 [12800/50000 ( 26%)]  Loss: 0.412643\n","Train Epoch: 25 [25600/50000 ( 51%)]  Loss: 0.572579\n","Train Epoch: 25 [38400/50000 ( 77%)]  Loss: 0.534637\n","Test set: Average loss: 0.0044, Accuracy: 40169/50000 (80.34%)\n","Test set: Average loss: 0.0069, Accuracy: 7671/10000 (76.71%)\n","Train Epoch: 26 [    0/50000 (  0%)]  Loss: 0.531537\n","Train Epoch: 26 [12800/50000 ( 26%)]  Loss: 0.599410\n","Train Epoch: 26 [25600/50000 ( 51%)]  Loss: 0.639362\n","Train Epoch: 26 [38400/50000 ( 77%)]  Loss: 0.544705\n","Test set: Average loss: 0.0043, Accuracy: 40436/50000 (80.87%)\n","Test set: Average loss: 0.0068, Accuracy: 7727/10000 (77.27%)\n","Train Epoch: 27 [    0/50000 (  0%)]  Loss: 0.573497\n","Train Epoch: 27 [12800/50000 ( 26%)]  Loss: 0.533416\n","Train Epoch: 27 [25600/50000 ( 51%)]  Loss: 0.648592\n","Train Epoch: 27 [38400/50000 ( 77%)]  Loss: 0.551040\n","Test set: Average loss: 0.0043, Accuracy: 40249/50000 (80.50%)\n","Test set: Average loss: 0.0068, Accuracy: 7742/10000 (77.42%)\n","Train Epoch: 28 [    0/50000 (  0%)]  Loss: 0.524637\n","Train Epoch: 28 [12800/50000 ( 26%)]  Loss: 0.842074\n","Train Epoch: 28 [25600/50000 ( 51%)]  Loss: 0.614243\n","Train Epoch: 28 [38400/50000 ( 77%)]  Loss: 0.432775\n","Test set: Average loss: 0.0041, Accuracy: 40697/50000 (81.39%)\n","Test set: Average loss: 0.0067, Accuracy: 7808/10000 (78.08%)\n","Train Epoch: 29 [    0/50000 (  0%)]  Loss: 0.502686\n","Train Epoch: 29 [12800/50000 ( 26%)]  Loss: 0.442160\n","Train Epoch: 29 [25600/50000 ( 51%)]  Loss: 0.773439\n","Train Epoch: 29 [38400/50000 ( 77%)]  Loss: 0.552965\n","Test set: Average loss: 0.0042, Accuracy: 40537/50000 (81.07%)\n","Test set: Average loss: 0.0066, Accuracy: 7786/10000 (77.86%)\n","Train Epoch: 30 [    0/50000 (  0%)]  Loss: 0.464186\n","Train Epoch: 30 [12800/50000 ( 26%)]  Loss: 0.614079\n","Train Epoch: 30 [25600/50000 ( 51%)]  Loss: 0.477529\n","Train Epoch: 30 [38400/50000 ( 77%)]  Loss: 0.571589\n","Test set: Average loss: 0.0042, Accuracy: 40642/50000 (81.28%)\n","Test set: Average loss: 0.0067, Accuracy: 7799/10000 (77.99%)\n","Train Epoch: 31 [    0/50000 (  0%)]  Loss: 0.511376\n","Train Epoch: 31 [12800/50000 ( 26%)]  Loss: 0.405669\n","Train Epoch: 31 [25600/50000 ( 51%)]  Loss: 0.499024\n","Train Epoch: 31 [38400/50000 ( 77%)]  Loss: 0.416622\n","Test set: Average loss: 0.0041, Accuracy: 40848/50000 (81.70%)\n","Test set: Average loss: 0.0068, Accuracy: 7751/10000 (77.51%)\n","Train Epoch: 32 [    0/50000 (  0%)]  Loss: 0.601847\n","Train Epoch: 32 [12800/50000 ( 26%)]  Loss: 0.536396\n","Train Epoch: 32 [25600/50000 ( 51%)]  Loss: 0.302206\n","Train Epoch: 32 [38400/50000 ( 77%)]  Loss: 0.513676\n","Test set: Average loss: 0.0043, Accuracy: 40324/50000 (80.65%)\n","Test set: Average loss: 0.0070, Accuracy: 7721/10000 (77.21%)\n","Train Epoch: 33 [    0/50000 (  0%)]  Loss: 0.498546\n","Train Epoch: 33 [12800/50000 ( 26%)]  Loss: 0.401799\n","Train Epoch: 33 [25600/50000 ( 51%)]  Loss: 0.436769\n","Train Epoch: 33 [38400/50000 ( 77%)]  Loss: 0.344778\n","Test set: Average loss: 0.0039, Accuracy: 41266/50000 (82.53%)\n","Test set: Average loss: 0.0066, Accuracy: 7841/10000 (78.41%)\n","Train Epoch: 34 [    0/50000 (  0%)]  Loss: 0.585910\n","Train Epoch: 34 [12800/50000 ( 26%)]  Loss: 0.493408\n","Train Epoch: 34 [25600/50000 ( 51%)]  Loss: 0.475363\n","Train Epoch: 34 [38400/50000 ( 77%)]  Loss: 0.579146\n","Test set: Average loss: 0.0040, Accuracy: 41013/50000 (82.03%)\n","Test set: Average loss: 0.0068, Accuracy: 7729/10000 (77.29%)\n","Train Epoch: 35 [    0/50000 (  0%)]  Loss: 0.561028\n","Train Epoch: 35 [12800/50000 ( 26%)]  Loss: 0.477417\n","Train Epoch: 35 [25600/50000 ( 51%)]  Loss: 0.482583\n","Train Epoch: 35 [38400/50000 ( 77%)]  Loss: 0.667636\n","Test set: Average loss: 0.0039, Accuracy: 41170/50000 (82.34%)\n","Test set: Average loss: 0.0065, Accuracy: 7853/10000 (78.53%)\n","Train Epoch: 36 [    0/50000 (  0%)]  Loss: 0.523688\n","Train Epoch: 36 [12800/50000 ( 26%)]  Loss: 0.470737\n","Train Epoch: 36 [25600/50000 ( 51%)]  Loss: 0.537799\n","Train Epoch: 36 [38400/50000 ( 77%)]  Loss: 0.476682\n","Test set: Average loss: 0.0040, Accuracy: 40914/50000 (81.83%)\n","Test set: Average loss: 0.0068, Accuracy: 7750/10000 (77.50%)\n","Train Epoch: 37 [    0/50000 (  0%)]  Loss: 0.457557\n","Train Epoch: 37 [12800/50000 ( 26%)]  Loss: 0.485943\n","Train Epoch: 37 [25600/50000 ( 51%)]  Loss: 0.555156\n","Train Epoch: 37 [38400/50000 ( 77%)]  Loss: 0.597293\n","Test set: Average loss: 0.0038, Accuracy: 41310/50000 (82.62%)\n","Test set: Average loss: 0.0066, Accuracy: 7811/10000 (78.11%)\n","Train Epoch: 38 [    0/50000 (  0%)]  Loss: 0.419260\n","Train Epoch: 38 [12800/50000 ( 26%)]  Loss: 0.593215\n","Train Epoch: 38 [25600/50000 ( 51%)]  Loss: 0.533435\n","Train Epoch: 38 [38400/50000 ( 77%)]  Loss: 0.608923\n","Test set: Average loss: 0.0038, Accuracy: 41462/50000 (82.92%)\n","Test set: Average loss: 0.0068, Accuracy: 7801/10000 (78.01%)\n","Train Epoch: 39 [    0/50000 (  0%)]  Loss: 0.447297\n","Train Epoch: 39 [12800/50000 ( 26%)]  Loss: 0.371348\n","Train Epoch: 39 [25600/50000 ( 51%)]  Loss: 0.406342\n","Train Epoch: 39 [38400/50000 ( 77%)]  Loss: 0.506915\n","Test set: Average loss: 0.0036, Accuracy: 41873/50000 (83.75%)\n","Test set: Average loss: 0.0063, Accuracy: 7908/10000 (79.08%)\n","Train Epoch: 40 [    0/50000 (  0%)]  Loss: 0.549580\n","Train Epoch: 40 [12800/50000 ( 26%)]  Loss: 0.515886\n","Train Epoch: 40 [25600/50000 ( 51%)]  Loss: 0.468079\n","Train Epoch: 40 [38400/50000 ( 77%)]  Loss: 0.463748\n","Test set: Average loss: 0.0037, Accuracy: 41496/50000 (82.99%)\n","Test set: Average loss: 0.0067, Accuracy: 7841/10000 (78.41%)\n","Train Epoch: 41 [    0/50000 (  0%)]  Loss: 0.390452\n","Train Epoch: 41 [12800/50000 ( 26%)]  Loss: 0.379728\n","Train Epoch: 41 [25600/50000 ( 51%)]  Loss: 0.507120\n","Train Epoch: 41 [38400/50000 ( 77%)]  Loss: 0.571385\n","Test set: Average loss: 0.0037, Accuracy: 41736/50000 (83.47%)\n","Test set: Average loss: 0.0066, Accuracy: 7839/10000 (78.39%)\n","Train Epoch: 42 [    0/50000 (  0%)]  Loss: 0.735560\n","Train Epoch: 42 [12800/50000 ( 26%)]  Loss: 0.579206\n","Train Epoch: 42 [25600/50000 ( 51%)]  Loss: 0.491587\n","Train Epoch: 42 [38400/50000 ( 77%)]  Loss: 0.646607\n","Test set: Average loss: 0.0036, Accuracy: 41924/50000 (83.85%)\n","Test set: Average loss: 0.0064, Accuracy: 7896/10000 (78.96%)\n","Train Epoch: 43 [    0/50000 (  0%)]  Loss: 0.419851\n","Train Epoch: 43 [12800/50000 ( 26%)]  Loss: 0.533081\n","Train Epoch: 43 [25600/50000 ( 51%)]  Loss: 0.348682\n","Train Epoch: 43 [38400/50000 ( 77%)]  Loss: 0.488635\n","Test set: Average loss: 0.0036, Accuracy: 41936/50000 (83.87%)\n","Test set: Average loss: 0.0066, Accuracy: 7856/10000 (78.56%)\n","Train Epoch: 44 [    0/50000 (  0%)]  Loss: 0.351996\n","Train Epoch: 44 [12800/50000 ( 26%)]  Loss: 0.604251\n","Train Epoch: 44 [25600/50000 ( 51%)]  Loss: 0.434963\n","Train Epoch: 44 [38400/50000 ( 77%)]  Loss: 0.553403\n","Test set: Average loss: 0.0035, Accuracy: 42158/50000 (84.32%)\n","Test set: Average loss: 0.0066, Accuracy: 7869/10000 (78.69%)\n","Train Epoch: 45 [    0/50000 (  0%)]  Loss: 0.299482\n","Train Epoch: 45 [12800/50000 ( 26%)]  Loss: 0.523033\n","Train Epoch: 45 [25600/50000 ( 51%)]  Loss: 0.518188\n","Train Epoch: 45 [38400/50000 ( 77%)]  Loss: 0.574771\n","Test set: Average loss: 0.0035, Accuracy: 42156/50000 (84.31%)\n","Test set: Average loss: 0.0068, Accuracy: 7861/10000 (78.61%)\n","Train Epoch: 46 [    0/50000 (  0%)]  Loss: 0.426310\n","Train Epoch: 46 [12800/50000 ( 26%)]  Loss: 0.518818\n","Train Epoch: 46 [25600/50000 ( 51%)]  Loss: 0.389571\n","Train Epoch: 46 [38400/50000 ( 77%)]  Loss: 0.492078\n","Test set: Average loss: 0.0034, Accuracy: 42227/50000 (84.45%)\n","Test set: Average loss: 0.0066, Accuracy: 7898/10000 (78.98%)\n","Train Epoch: 47 [    0/50000 (  0%)]  Loss: 0.472604\n","Train Epoch: 47 [12800/50000 ( 26%)]  Loss: 0.513411\n","Train Epoch: 47 [25600/50000 ( 51%)]  Loss: 0.613584\n","Train Epoch: 47 [38400/50000 ( 77%)]  Loss: 0.305820\n","Test set: Average loss: 0.0035, Accuracy: 42017/50000 (84.03%)\n","Test set: Average loss: 0.0067, Accuracy: 7839/10000 (78.39%)\n","Train Epoch: 48 [    0/50000 (  0%)]  Loss: 0.411536\n","Train Epoch: 48 [12800/50000 ( 26%)]  Loss: 0.503888\n","Train Epoch: 48 [25600/50000 ( 51%)]  Loss: 0.404488\n","Train Epoch: 48 [38400/50000 ( 77%)]  Loss: 0.523538\n","Test set: Average loss: 0.0034, Accuracy: 42328/50000 (84.66%)\n","Test set: Average loss: 0.0069, Accuracy: 7903/10000 (79.03%)\n","Train Epoch: 49 [    0/50000 (  0%)]  Loss: 0.583454\n","Train Epoch: 49 [12800/50000 ( 26%)]  Loss: 0.323761\n","Train Epoch: 49 [25600/50000 ( 51%)]  Loss: 0.490579\n","Train Epoch: 49 [38400/50000 ( 77%)]  Loss: 0.457898\n","Test set: Average loss: 0.0034, Accuracy: 42268/50000 (84.54%)\n","Test set: Average loss: 0.0065, Accuracy: 7893/10000 (78.93%)\n","Train Epoch: 50 [    0/50000 (  0%)]  Loss: 0.365911\n","Train Epoch: 50 [12800/50000 ( 26%)]  Loss: 0.472159\n","Train Epoch: 50 [25600/50000 ( 51%)]  Loss: 0.431500\n","Train Epoch: 50 [38400/50000 ( 77%)]  Loss: 0.406341\n","Test set: Average loss: 0.0032, Accuracy: 42642/50000 (85.28%)\n","Test set: Average loss: 0.0065, Accuracy: 7957/10000 (79.57%)\n","Train Epoch: 51 [    0/50000 (  0%)]  Loss: 0.559608\n","Train Epoch: 51 [12800/50000 ( 26%)]  Loss: 0.475973\n","Train Epoch: 51 [25600/50000 ( 51%)]  Loss: 0.411133\n","Train Epoch: 51 [38400/50000 ( 77%)]  Loss: 0.362175\n","Test set: Average loss: 0.0036, Accuracy: 42040/50000 (84.08%)\n","Test set: Average loss: 0.0065, Accuracy: 7912/10000 (79.12%)\n","Train Epoch: 52 [    0/50000 (  0%)]  Loss: 0.291836\n","Train Epoch: 52 [12800/50000 ( 26%)]  Loss: 0.347226\n","Train Epoch: 52 [25600/50000 ( 51%)]  Loss: 0.654307\n","Train Epoch: 52 [38400/50000 ( 77%)]  Loss: 0.319585\n","Test set: Average loss: 0.0036, Accuracy: 41901/50000 (83.80%)\n","Test set: Average loss: 0.0066, Accuracy: 7890/10000 (78.90%)\n","Train Epoch: 53 [    0/50000 (  0%)]  Loss: 0.536332\n","Train Epoch: 53 [12800/50000 ( 26%)]  Loss: 0.446338\n","Train Epoch: 53 [25600/50000 ( 51%)]  Loss: 0.422214\n","Train Epoch: 53 [38400/50000 ( 77%)]  Loss: 0.495615\n","Test set: Average loss: 0.0033, Accuracy: 42412/50000 (84.82%)\n","Test set: Average loss: 0.0066, Accuracy: 7911/10000 (79.11%)\n","Train Epoch: 54 [    0/50000 (  0%)]  Loss: 0.425920\n","Train Epoch: 54 [12800/50000 ( 26%)]  Loss: 0.457095\n","Train Epoch: 54 [25600/50000 ( 51%)]  Loss: 0.490502\n","Train Epoch: 54 [38400/50000 ( 77%)]  Loss: 0.471108\n","Test set: Average loss: 0.0032, Accuracy: 42779/50000 (85.56%)\n","Test set: Average loss: 0.0065, Accuracy: 7919/10000 (79.19%)\n","Train Epoch: 55 [    0/50000 (  0%)]  Loss: 0.470699\n","Train Epoch: 55 [12800/50000 ( 26%)]  Loss: 0.541151\n","Train Epoch: 55 [25600/50000 ( 51%)]  Loss: 0.339350\n","Train Epoch: 55 [38400/50000 ( 77%)]  Loss: 0.465302\n","Test set: Average loss: 0.0031, Accuracy: 42859/50000 (85.72%)\n","Test set: Average loss: 0.0066, Accuracy: 7946/10000 (79.46%)\n","Train Epoch: 56 [    0/50000 (  0%)]  Loss: 0.443014\n","Train Epoch: 56 [12800/50000 ( 26%)]  Loss: 0.447437\n","Train Epoch: 56 [25600/50000 ( 51%)]  Loss: 0.563254\n","Train Epoch: 56 [38400/50000 ( 77%)]  Loss: 0.498815\n","Test set: Average loss: 0.0031, Accuracy: 42899/50000 (85.80%)\n","Test set: Average loss: 0.0065, Accuracy: 7977/10000 (79.77%)\n","Train Epoch: 57 [    0/50000 (  0%)]  Loss: 0.356985\n","Train Epoch: 57 [12800/50000 ( 26%)]  Loss: 0.385051\n","Train Epoch: 57 [25600/50000 ( 51%)]  Loss: 0.449751\n","Train Epoch: 57 [38400/50000 ( 77%)]  Loss: 0.478699\n","Test set: Average loss: 0.0031, Accuracy: 42877/50000 (85.75%)\n","Test set: Average loss: 0.0066, Accuracy: 7946/10000 (79.46%)\n","Train Epoch: 58 [    0/50000 (  0%)]  Loss: 0.403114\n","Train Epoch: 58 [12800/50000 ( 26%)]  Loss: 0.426748\n","Train Epoch: 58 [25600/50000 ( 51%)]  Loss: 0.483434\n","Train Epoch: 58 [38400/50000 ( 77%)]  Loss: 0.485687\n","Test set: Average loss: 0.0031, Accuracy: 43080/50000 (86.16%)\n","Test set: Average loss: 0.0063, Accuracy: 7997/10000 (79.97%)\n","Train Epoch: 59 [    0/50000 (  0%)]  Loss: 0.415011\n","Train Epoch: 59 [12800/50000 ( 26%)]  Loss: 0.409900\n","Train Epoch: 59 [25600/50000 ( 51%)]  Loss: 0.312922\n","Train Epoch: 59 [38400/50000 ( 77%)]  Loss: 0.433167\n","Test set: Average loss: 0.0031, Accuracy: 43042/50000 (86.08%)\n","Test set: Average loss: 0.0065, Accuracy: 7961/10000 (79.61%)\n","Train Epoch: 60 [    0/50000 (  0%)]  Loss: 0.367292\n","Train Epoch: 60 [12800/50000 ( 26%)]  Loss: 0.362203\n","Train Epoch: 60 [25600/50000 ( 51%)]  Loss: 0.483487\n","Train Epoch: 60 [38400/50000 ( 77%)]  Loss: 0.356560\n","Test set: Average loss: 0.0030, Accuracy: 43291/50000 (86.58%)\n","Test set: Average loss: 0.0064, Accuracy: 7972/10000 (79.72%)\n","Train Epoch: 61 [    0/50000 (  0%)]  Loss: 0.297754\n","Train Epoch: 61 [12800/50000 ( 26%)]  Loss: 0.375371\n","Train Epoch: 61 [25600/50000 ( 51%)]  Loss: 0.373122\n","Train Epoch: 61 [38400/50000 ( 77%)]  Loss: 0.481220\n","Test set: Average loss: 0.0029, Accuracy: 43310/50000 (86.62%)\n","Test set: Average loss: 0.0065, Accuracy: 7994/10000 (79.94%)\n","Train Epoch: 62 [    0/50000 (  0%)]  Loss: 0.372619\n","Train Epoch: 62 [12800/50000 ( 26%)]  Loss: 0.363122\n","Train Epoch: 62 [25600/50000 ( 51%)]  Loss: 0.492026\n","Train Epoch: 62 [38400/50000 ( 77%)]  Loss: 0.417721\n","Test set: Average loss: 0.0030, Accuracy: 43236/50000 (86.47%)\n","Test set: Average loss: 0.0066, Accuracy: 7884/10000 (78.84%)\n","Train Epoch: 63 [    0/50000 (  0%)]  Loss: 0.372750\n","Train Epoch: 63 [12800/50000 ( 26%)]  Loss: 0.331222\n","Train Epoch: 63 [25600/50000 ( 51%)]  Loss: 0.390112\n","Train Epoch: 63 [38400/50000 ( 77%)]  Loss: 0.462760\n","Test set: Average loss: 0.0031, Accuracy: 43209/50000 (86.42%)\n","Test set: Average loss: 0.0069, Accuracy: 7940/10000 (79.40%)\n","Train Epoch: 64 [    0/50000 (  0%)]  Loss: 0.441991\n","Train Epoch: 64 [12800/50000 ( 26%)]  Loss: 0.452172\n","Train Epoch: 64 [25600/50000 ( 51%)]  Loss: 0.300205\n","Train Epoch: 64 [38400/50000 ( 77%)]  Loss: 0.261492\n","Test set: Average loss: 0.0031, Accuracy: 43011/50000 (86.02%)\n","Test set: Average loss: 0.0068, Accuracy: 7909/10000 (79.09%)\n","Train Epoch: 65 [    0/50000 (  0%)]  Loss: 0.392844\n","Train Epoch: 65 [12800/50000 ( 26%)]  Loss: 0.398096\n","Train Epoch: 65 [25600/50000 ( 51%)]  Loss: 0.341574\n","Train Epoch: 65 [38400/50000 ( 77%)]  Loss: 0.340540\n","Test set: Average loss: 0.0030, Accuracy: 43231/50000 (86.46%)\n","Test set: Average loss: 0.0065, Accuracy: 7934/10000 (79.34%)\n","Train Epoch: 66 [    0/50000 (  0%)]  Loss: 0.344639\n","Train Epoch: 66 [12800/50000 ( 26%)]  Loss: 0.510947\n","Train Epoch: 66 [25600/50000 ( 51%)]  Loss: 0.342239\n","Train Epoch: 66 [38400/50000 ( 77%)]  Loss: 0.504003\n","Test set: Average loss: 0.0030, Accuracy: 43265/50000 (86.53%)\n","Test set: Average loss: 0.0066, Accuracy: 7924/10000 (79.24%)\n","Train Epoch: 67 [    0/50000 (  0%)]  Loss: 0.477204\n","Train Epoch: 67 [12800/50000 ( 26%)]  Loss: 0.447234\n","Train Epoch: 67 [25600/50000 ( 51%)]  Loss: 0.350376\n","Train Epoch: 67 [38400/50000 ( 77%)]  Loss: 0.279171\n","Test set: Average loss: 0.0029, Accuracy: 43418/50000 (86.84%)\n","Test set: Average loss: 0.0067, Accuracy: 7956/10000 (79.56%)\n","Train Epoch: 68 [    0/50000 (  0%)]  Loss: 0.319135\n","Train Epoch: 68 [12800/50000 ( 26%)]  Loss: 0.446919\n","Train Epoch: 68 [25600/50000 ( 51%)]  Loss: 0.383484\n","Train Epoch: 68 [38400/50000 ( 77%)]  Loss: 0.303003\n","Test set: Average loss: 0.0029, Accuracy: 43525/50000 (87.05%)\n","Test set: Average loss: 0.0066, Accuracy: 7979/10000 (79.79%)\n","Train Epoch: 69 [    0/50000 (  0%)]  Loss: 0.438795\n","Train Epoch: 69 [12800/50000 ( 26%)]  Loss: 0.239979\n","Train Epoch: 69 [25600/50000 ( 51%)]  Loss: 0.304067\n","Train Epoch: 69 [38400/50000 ( 77%)]  Loss: 0.428954\n","Test set: Average loss: 0.0029, Accuracy: 43439/50000 (86.88%)\n","Test set: Average loss: 0.0068, Accuracy: 7952/10000 (79.52%)\n","Train Epoch: 70 [    0/50000 (  0%)]  Loss: 0.294832\n","Train Epoch: 70 [12800/50000 ( 26%)]  Loss: 0.378330\n","Train Epoch: 70 [25600/50000 ( 51%)]  Loss: 0.270285\n","Train Epoch: 70 [38400/50000 ( 77%)]  Loss: 0.335285\n","Test set: Average loss: 0.0028, Accuracy: 43671/50000 (87.34%)\n","Test set: Average loss: 0.0066, Accuracy: 8023/10000 (80.23%)\n","Train Epoch: 71 [    0/50000 (  0%)]  Loss: 0.422104\n","Train Epoch: 71 [12800/50000 ( 26%)]  Loss: 0.388827\n","Train Epoch: 71 [25600/50000 ( 51%)]  Loss: 0.526768\n","Train Epoch: 71 [38400/50000 ( 77%)]  Loss: 0.333903\n","Test set: Average loss: 0.0028, Accuracy: 43858/50000 (87.72%)\n","Test set: Average loss: 0.0067, Accuracy: 7944/10000 (79.44%)\n","Train Epoch: 72 [    0/50000 (  0%)]  Loss: 0.289629\n","Train Epoch: 72 [12800/50000 ( 26%)]  Loss: 0.315195\n","Train Epoch: 72 [25600/50000 ( 51%)]  Loss: 0.431273\n","Train Epoch: 72 [38400/50000 ( 77%)]  Loss: 0.482461\n","Test set: Average loss: 0.0029, Accuracy: 43462/50000 (86.92%)\n","Test set: Average loss: 0.0068, Accuracy: 7955/10000 (79.55%)\n","Train Epoch: 73 [    0/50000 (  0%)]  Loss: 0.359748\n","Train Epoch: 73 [12800/50000 ( 26%)]  Loss: 0.339096\n","Train Epoch: 73 [25600/50000 ( 51%)]  Loss: 0.227614\n","Train Epoch: 73 [38400/50000 ( 77%)]  Loss: 0.274443\n","Test set: Average loss: 0.0027, Accuracy: 43838/50000 (87.68%)\n","Test set: Average loss: 0.0068, Accuracy: 7960/10000 (79.60%)\n","Train Epoch: 74 [    0/50000 (  0%)]  Loss: 0.346908\n","Train Epoch: 74 [12800/50000 ( 26%)]  Loss: 0.424875\n","Train Epoch: 74 [25600/50000 ( 51%)]  Loss: 0.466468\n","Train Epoch: 74 [38400/50000 ( 77%)]  Loss: 0.371693\n","Test set: Average loss: 0.0028, Accuracy: 43613/50000 (87.23%)\n","Test set: Average loss: 0.0068, Accuracy: 7975/10000 (79.75%)\n","Train Epoch: 75 [    0/50000 (  0%)]  Loss: 0.326893\n","Train Epoch: 75 [12800/50000 ( 26%)]  Loss: 0.285271\n","Train Epoch: 75 [25600/50000 ( 51%)]  Loss: 0.325406\n","Train Epoch: 75 [38400/50000 ( 77%)]  Loss: 0.301899\n","Test set: Average loss: 0.0027, Accuracy: 43777/50000 (87.55%)\n","Test set: Average loss: 0.0067, Accuracy: 7973/10000 (79.73%)\n","Train Epoch: 76 [    0/50000 (  0%)]  Loss: 0.314297\n","Train Epoch: 76 [12800/50000 ( 26%)]  Loss: 0.492571\n","Train Epoch: 76 [25600/50000 ( 51%)]  Loss: 0.319230\n","Train Epoch: 76 [38400/50000 ( 77%)]  Loss: 0.344437\n","Test set: Average loss: 0.0027, Accuracy: 43915/50000 (87.83%)\n","Test set: Average loss: 0.0071, Accuracy: 8012/10000 (80.12%)\n","Train Epoch: 77 [    0/50000 (  0%)]  Loss: 0.387432\n","Train Epoch: 77 [12800/50000 ( 26%)]  Loss: 0.269646\n","Train Epoch: 77 [25600/50000 ( 51%)]  Loss: 0.267128\n","Train Epoch: 77 [38400/50000 ( 77%)]  Loss: 0.292760\n","Test set: Average loss: 0.0027, Accuracy: 43858/50000 (87.72%)\n","Test set: Average loss: 0.0066, Accuracy: 7971/10000 (79.71%)\n","Train Epoch: 78 [    0/50000 (  0%)]  Loss: 0.278880\n","Train Epoch: 78 [12800/50000 ( 26%)]  Loss: 0.343575\n","Train Epoch: 78 [25600/50000 ( 51%)]  Loss: 0.349766\n","Train Epoch: 78 [38400/50000 ( 77%)]  Loss: 0.347091\n","Test set: Average loss: 0.0027, Accuracy: 43889/50000 (87.78%)\n","Test set: Average loss: 0.0070, Accuracy: 7894/10000 (78.94%)\n","Train Epoch: 79 [    0/50000 (  0%)]  Loss: 0.333359\n","Train Epoch: 79 [12800/50000 ( 26%)]  Loss: 0.366364\n","Train Epoch: 79 [25600/50000 ( 51%)]  Loss: 0.341169\n","Train Epoch: 79 [38400/50000 ( 77%)]  Loss: 0.338387\n","Test set: Average loss: 0.0027, Accuracy: 43836/50000 (87.67%)\n","Test set: Average loss: 0.0069, Accuracy: 7937/10000 (79.37%)\n","Train Epoch: 80 [    0/50000 (  0%)]  Loss: 0.298221\n","Train Epoch: 80 [12800/50000 ( 26%)]  Loss: 0.329767\n","Train Epoch: 80 [25600/50000 ( 51%)]  Loss: 0.301118\n","Train Epoch: 80 [38400/50000 ( 77%)]  Loss: 0.221471\n","Test set: Average loss: 0.0028, Accuracy: 43824/50000 (87.65%)\n","Test set: Average loss: 0.0073, Accuracy: 7929/10000 (79.29%)\n","Train Epoch: 81 [    0/50000 (  0%)]  Loss: 0.268688\n","Train Epoch: 81 [12800/50000 ( 26%)]  Loss: 0.347302\n","Train Epoch: 81 [25600/50000 ( 51%)]  Loss: 0.347931\n","Train Epoch: 81 [38400/50000 ( 77%)]  Loss: 0.559559\n","Test set: Average loss: 0.0027, Accuracy: 44022/50000 (88.04%)\n","Test set: Average loss: 0.0066, Accuracy: 7995/10000 (79.95%)\n","Train Epoch: 82 [    0/50000 (  0%)]  Loss: 0.224953\n","Train Epoch: 82 [12800/50000 ( 26%)]  Loss: 0.391091\n","Train Epoch: 82 [25600/50000 ( 51%)]  Loss: 0.419649\n","Train Epoch: 82 [38400/50000 ( 77%)]  Loss: 0.369905\n","Test set: Average loss: 0.0026, Accuracy: 44279/50000 (88.56%)\n","Test set: Average loss: 0.0068, Accuracy: 8021/10000 (80.21%)\n","Train Epoch: 83 [    0/50000 (  0%)]  Loss: 0.273445\n","Train Epoch: 83 [12800/50000 ( 26%)]  Loss: 0.393519\n","Train Epoch: 83 [25600/50000 ( 51%)]  Loss: 0.248958\n","Train Epoch: 83 [38400/50000 ( 77%)]  Loss: 0.383903\n","Test set: Average loss: 0.0026, Accuracy: 44250/50000 (88.50%)\n","Test set: Average loss: 0.0069, Accuracy: 7998/10000 (79.98%)\n","Train Epoch: 84 [    0/50000 (  0%)]  Loss: 0.244580\n","Train Epoch: 84 [12800/50000 ( 26%)]  Loss: 0.315951\n","Train Epoch: 84 [25600/50000 ( 51%)]  Loss: 0.329333\n","Train Epoch: 84 [38400/50000 ( 77%)]  Loss: 0.320658\n","Test set: Average loss: 0.0024, Accuracy: 44641/50000 (89.28%)\n","Test set: Average loss: 0.0070, Accuracy: 8040/10000 (80.40%)\n","Train Epoch: 85 [    0/50000 (  0%)]  Loss: 0.320366\n","Train Epoch: 85 [12800/50000 ( 26%)]  Loss: 0.185542\n","Train Epoch: 85 [25600/50000 ( 51%)]  Loss: 0.368403\n","Train Epoch: 85 [38400/50000 ( 77%)]  Loss: 0.305599\n","Test set: Average loss: 0.0026, Accuracy: 44187/50000 (88.37%)\n","Test set: Average loss: 0.0067, Accuracy: 7971/10000 (79.71%)\n","Train Epoch: 86 [    0/50000 (  0%)]  Loss: 0.356761\n","Train Epoch: 86 [12800/50000 ( 26%)]  Loss: 0.465018\n","Train Epoch: 86 [25600/50000 ( 51%)]  Loss: 0.379440\n","Train Epoch: 86 [38400/50000 ( 77%)]  Loss: 0.322430\n","Test set: Average loss: 0.0025, Accuracy: 44466/50000 (88.93%)\n","Test set: Average loss: 0.0071, Accuracy: 7949/10000 (79.49%)\n","Train Epoch: 87 [    0/50000 (  0%)]  Loss: 0.303305\n","Train Epoch: 87 [12800/50000 ( 26%)]  Loss: 0.408357\n","Train Epoch: 87 [25600/50000 ( 51%)]  Loss: 0.422187\n","Train Epoch: 87 [38400/50000 ( 77%)]  Loss: 0.464625\n","Test set: Average loss: 0.0024, Accuracy: 44541/50000 (89.08%)\n","Test set: Average loss: 0.0071, Accuracy: 7988/10000 (79.88%)\n","Train Epoch: 88 [    0/50000 (  0%)]  Loss: 0.261490\n","Train Epoch: 88 [12800/50000 ( 26%)]  Loss: 0.275068\n","Train Epoch: 88 [25600/50000 ( 51%)]  Loss: 0.214227\n","Train Epoch: 88 [38400/50000 ( 77%)]  Loss: 0.522719\n","Test set: Average loss: 0.0025, Accuracy: 44456/50000 (88.91%)\n","Test set: Average loss: 0.0072, Accuracy: 7980/10000 (79.80%)\n","Train Epoch: 89 [    0/50000 (  0%)]  Loss: 0.253049\n","Train Epoch: 89 [12800/50000 ( 26%)]  Loss: 0.283175\n","Train Epoch: 89 [25600/50000 ( 51%)]  Loss: 0.283063\n","Train Epoch: 89 [38400/50000 ( 77%)]  Loss: 0.408783\n","Test set: Average loss: 0.0025, Accuracy: 44469/50000 (88.94%)\n","Test set: Average loss: 0.0066, Accuracy: 8025/10000 (80.25%)\n","Train Epoch: 90 [    0/50000 (  0%)]  Loss: 0.306520\n","Train Epoch: 90 [12800/50000 ( 26%)]  Loss: 0.186461\n","Train Epoch: 90 [25600/50000 ( 51%)]  Loss: 0.441948\n","Train Epoch: 90 [38400/50000 ( 77%)]  Loss: 0.398455\n","Test set: Average loss: 0.0026, Accuracy: 44343/50000 (88.69%)\n","Test set: Average loss: 0.0073, Accuracy: 7954/10000 (79.54%)\n","Train Epoch: 91 [    0/50000 (  0%)]  Loss: 0.256082\n","Train Epoch: 91 [12800/50000 ( 26%)]  Loss: 0.346634\n","Train Epoch: 91 [25600/50000 ( 51%)]  Loss: 0.233029\n","Train Epoch: 91 [38400/50000 ( 77%)]  Loss: 0.225814\n","Test set: Average loss: 0.0025, Accuracy: 44282/50000 (88.56%)\n","Test set: Average loss: 0.0067, Accuracy: 7976/10000 (79.76%)\n","Train Epoch: 92 [    0/50000 (  0%)]  Loss: 0.348397\n","Train Epoch: 92 [12800/50000 ( 26%)]  Loss: 0.302500\n","Train Epoch: 92 [25600/50000 ( 51%)]  Loss: 0.210086\n","Train Epoch: 92 [38400/50000 ( 77%)]  Loss: 0.297123\n","Test set: Average loss: 0.0025, Accuracy: 44460/50000 (88.92%)\n","Test set: Average loss: 0.0069, Accuracy: 7986/10000 (79.86%)\n","Train Epoch: 93 [    0/50000 (  0%)]  Loss: 0.350843\n","Train Epoch: 93 [12800/50000 ( 26%)]  Loss: 0.434326\n","Train Epoch: 93 [25600/50000 ( 51%)]  Loss: 0.366179\n","Train Epoch: 93 [38400/50000 ( 77%)]  Loss: 0.242650\n","Test set: Average loss: 0.0023, Accuracy: 44759/50000 (89.52%)\n","Test set: Average loss: 0.0070, Accuracy: 8039/10000 (80.39%)\n","Train Epoch: 94 [    0/50000 (  0%)]  Loss: 0.338982\n","Train Epoch: 94 [12800/50000 ( 26%)]  Loss: 0.276753\n","Train Epoch: 94 [25600/50000 ( 51%)]  Loss: 0.272159\n","Train Epoch: 94 [38400/50000 ( 77%)]  Loss: 0.407113\n","Test set: Average loss: 0.0025, Accuracy: 44458/50000 (88.92%)\n","Test set: Average loss: 0.0070, Accuracy: 8002/10000 (80.02%)\n","Train Epoch: 95 [    0/50000 (  0%)]  Loss: 0.260941\n","Train Epoch: 95 [12800/50000 ( 26%)]  Loss: 0.500537\n","Train Epoch: 95 [25600/50000 ( 51%)]  Loss: 0.221586\n","Train Epoch: 95 [38400/50000 ( 77%)]  Loss: 0.307066\n","Test set: Average loss: 0.0025, Accuracy: 44500/50000 (89.00%)\n","Test set: Average loss: 0.0073, Accuracy: 7951/10000 (79.51%)\n","Train Epoch: 96 [    0/50000 (  0%)]  Loss: 0.474043\n","Train Epoch: 96 [12800/50000 ( 26%)]  Loss: 0.359182\n","Train Epoch: 96 [25600/50000 ( 51%)]  Loss: 0.404730\n","Train Epoch: 96 [38400/50000 ( 77%)]  Loss: 0.277077\n","Test set: Average loss: 0.0024, Accuracy: 44714/50000 (89.43%)\n","Test set: Average loss: 0.0068, Accuracy: 7997/10000 (79.97%)\n","Train Epoch: 97 [    0/50000 (  0%)]  Loss: 0.256394\n","Train Epoch: 97 [12800/50000 ( 26%)]  Loss: 0.458031\n","Train Epoch: 97 [25600/50000 ( 51%)]  Loss: 0.262027\n","Train Epoch: 97 [38400/50000 ( 77%)]  Loss: 0.284373\n","Test set: Average loss: 0.0025, Accuracy: 44614/50000 (89.23%)\n","Test set: Average loss: 0.0069, Accuracy: 7965/10000 (79.65%)\n","Train Epoch: 98 [    0/50000 (  0%)]  Loss: 0.277180\n","Train Epoch: 98 [12800/50000 ( 26%)]  Loss: 0.292350\n","Train Epoch: 98 [25600/50000 ( 51%)]  Loss: 0.257286\n","Train Epoch: 98 [38400/50000 ( 77%)]  Loss: 0.416160\n","Test set: Average loss: 0.0025, Accuracy: 44531/50000 (89.06%)\n","Test set: Average loss: 0.0070, Accuracy: 7991/10000 (79.91%)\n","Train Epoch: 99 [    0/50000 (  0%)]  Loss: 0.259715\n","Train Epoch: 99 [12800/50000 ( 26%)]  Loss: 0.310458\n","Train Epoch: 99 [25600/50000 ( 51%)]  Loss: 0.249715\n","Train Epoch: 99 [38400/50000 ( 77%)]  Loss: 0.246909\n","Test set: Average loss: 0.0024, Accuracy: 44709/50000 (89.42%)\n","Test set: Average loss: 0.0074, Accuracy: 7963/10000 (79.63%)\n","Train Epoch: 100 [    0/50000 (  0%)]  Loss: 0.315915\n","Train Epoch: 100 [12800/50000 ( 26%)]  Loss: 0.304769\n","Train Epoch: 100 [25600/50000 ( 51%)]  Loss: 0.228839\n","Train Epoch: 100 [38400/50000 ( 77%)]  Loss: 0.380646\n","Test set: Average loss: 0.0023, Accuracy: 44887/50000 (89.77%)\n","Test set: Average loss: 0.0071, Accuracy: 7995/10000 (79.95%)\n","Train Epoch: 101 [    0/50000 (  0%)]  Loss: 0.273251\n","Train Epoch: 101 [12800/50000 ( 26%)]  Loss: 0.507850\n","Train Epoch: 101 [25600/50000 ( 51%)]  Loss: 0.409316\n","Train Epoch: 101 [38400/50000 ( 77%)]  Loss: 0.249113\n","Test set: Average loss: 0.0025, Accuracy: 44556/50000 (89.11%)\n","Test set: Average loss: 0.0073, Accuracy: 7988/10000 (79.88%)\n","Train Epoch: 102 [    0/50000 (  0%)]  Loss: 0.214958\n","Train Epoch: 102 [12800/50000 ( 26%)]  Loss: 0.304839\n","Train Epoch: 102 [25600/50000 ( 51%)]  Loss: 0.346529\n","Train Epoch: 102 [38400/50000 ( 77%)]  Loss: 0.306033\n","Test set: Average loss: 0.0024, Accuracy: 44819/50000 (89.64%)\n","Test set: Average loss: 0.0073, Accuracy: 7967/10000 (79.67%)\n","Train Epoch: 103 [    0/50000 (  0%)]  Loss: 0.181855\n","Train Epoch: 103 [12800/50000 ( 26%)]  Loss: 0.357384\n","Train Epoch: 103 [25600/50000 ( 51%)]  Loss: 0.219212\n","Train Epoch: 103 [38400/50000 ( 77%)]  Loss: 0.444819\n","Test set: Average loss: 0.0021, Accuracy: 45233/50000 (90.47%)\n","Test set: Average loss: 0.0070, Accuracy: 8042/10000 (80.42%)\n","Train Epoch: 104 [    0/50000 (  0%)]  Loss: 0.278936\n","Train Epoch: 104 [12800/50000 ( 26%)]  Loss: 0.304022\n","Train Epoch: 104 [25600/50000 ( 51%)]  Loss: 0.296398\n","Train Epoch: 104 [38400/50000 ( 77%)]  Loss: 0.332664\n","Test set: Average loss: 0.0023, Accuracy: 44921/50000 (89.84%)\n","Test set: Average loss: 0.0070, Accuracy: 8018/10000 (80.18%)\n","Train Epoch: 105 [    0/50000 (  0%)]  Loss: 0.199522\n","Train Epoch: 105 [12800/50000 ( 26%)]  Loss: 0.343591\n","Train Epoch: 105 [25600/50000 ( 51%)]  Loss: 0.306586\n","Train Epoch: 105 [38400/50000 ( 77%)]  Loss: 0.252187\n","Test set: Average loss: 0.0021, Accuracy: 45150/50000 (90.30%)\n","Test set: Average loss: 0.0069, Accuracy: 8000/10000 (80.00%)\n","Train Epoch: 106 [    0/50000 (  0%)]  Loss: 0.274267\n","Train Epoch: 106 [12800/50000 ( 26%)]  Loss: 0.230860\n","Train Epoch: 106 [25600/50000 ( 51%)]  Loss: 0.251735\n","Train Epoch: 106 [38400/50000 ( 77%)]  Loss: 0.292571\n","Test set: Average loss: 0.0022, Accuracy: 44966/50000 (89.93%)\n","Test set: Average loss: 0.0075, Accuracy: 7998/10000 (79.98%)\n","Train Epoch: 107 [    0/50000 (  0%)]  Loss: 0.305719\n","Train Epoch: 107 [12800/50000 ( 26%)]  Loss: 0.396837\n","Train Epoch: 107 [25600/50000 ( 51%)]  Loss: 0.296225\n","Train Epoch: 107 [38400/50000 ( 77%)]  Loss: 0.252502\n","Test set: Average loss: 0.0022, Accuracy: 44964/50000 (89.93%)\n","Test set: Average loss: 0.0070, Accuracy: 8029/10000 (80.29%)\n","Train Epoch: 108 [    0/50000 (  0%)]  Loss: 0.305896\n","Train Epoch: 108 [12800/50000 ( 26%)]  Loss: 0.328978\n","Train Epoch: 108 [25600/50000 ( 51%)]  Loss: 0.223423\n","Train Epoch: 108 [38400/50000 ( 77%)]  Loss: 0.433027\n","Test set: Average loss: 0.0023, Accuracy: 45092/50000 (90.18%)\n","Test set: Average loss: 0.0069, Accuracy: 8008/10000 (80.08%)\n","Train Epoch: 109 [    0/50000 (  0%)]  Loss: 0.218897\n","Train Epoch: 109 [12800/50000 ( 26%)]  Loss: 0.311498\n","Train Epoch: 109 [25600/50000 ( 51%)]  Loss: 0.372722\n","Train Epoch: 109 [38400/50000 ( 77%)]  Loss: 0.282790\n","Test set: Average loss: 0.0021, Accuracy: 45275/50000 (90.55%)\n","Test set: Average loss: 0.0072, Accuracy: 8029/10000 (80.29%)\n","Train Epoch: 110 [    0/50000 (  0%)]  Loss: 0.342022\n","Train Epoch: 110 [12800/50000 ( 26%)]  Loss: 0.243259\n","Train Epoch: 110 [25600/50000 ( 51%)]  Loss: 0.374258\n","Train Epoch: 110 [38400/50000 ( 77%)]  Loss: 0.307464\n","Test set: Average loss: 0.0022, Accuracy: 45171/50000 (90.34%)\n","Test set: Average loss: 0.0071, Accuracy: 8089/10000 (80.89%)\n","Train Epoch: 111 [    0/50000 (  0%)]  Loss: 0.241854\n","Train Epoch: 111 [12800/50000 ( 26%)]  Loss: 0.242288\n","Train Epoch: 111 [25600/50000 ( 51%)]  Loss: 0.240397\n","Train Epoch: 111 [38400/50000 ( 77%)]  Loss: 0.169779\n","Test set: Average loss: 0.0023, Accuracy: 44927/50000 (89.85%)\n","Test set: Average loss: 0.0075, Accuracy: 8005/10000 (80.05%)\n","Train Epoch: 112 [    0/50000 (  0%)]  Loss: 0.439165\n","Train Epoch: 112 [12800/50000 ( 26%)]  Loss: 0.310823\n","Train Epoch: 112 [25600/50000 ( 51%)]  Loss: 0.257768\n","Train Epoch: 112 [38400/50000 ( 77%)]  Loss: 0.393469\n","Test set: Average loss: 0.0023, Accuracy: 44800/50000 (89.60%)\n","Test set: Average loss: 0.0070, Accuracy: 8064/10000 (80.64%)\n","Train Epoch: 113 [    0/50000 (  0%)]  Loss: 0.257590\n","Train Epoch: 113 [12800/50000 ( 26%)]  Loss: 0.294935\n","Train Epoch: 113 [25600/50000 ( 51%)]  Loss: 0.278129\n","Train Epoch: 113 [38400/50000 ( 77%)]  Loss: 0.308632\n","Test set: Average loss: 0.0022, Accuracy: 45040/50000 (90.08%)\n","Test set: Average loss: 0.0073, Accuracy: 7975/10000 (79.75%)\n","Train Epoch: 114 [    0/50000 (  0%)]  Loss: 0.334230\n","Train Epoch: 114 [12800/50000 ( 26%)]  Loss: 0.161484\n","Train Epoch: 114 [25600/50000 ( 51%)]  Loss: 0.259457\n","Train Epoch: 114 [38400/50000 ( 77%)]  Loss: 0.242973\n","Test set: Average loss: 0.0021, Accuracy: 45377/50000 (90.75%)\n","Test set: Average loss: 0.0071, Accuracy: 8047/10000 (80.47%)\n","Train Epoch: 115 [    0/50000 (  0%)]  Loss: 0.291742\n","Train Epoch: 115 [12800/50000 ( 26%)]  Loss: 0.201751\n","Train Epoch: 115 [25600/50000 ( 51%)]  Loss: 0.299181\n","Train Epoch: 115 [38400/50000 ( 77%)]  Loss: 0.182070\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f680c6330ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-b50169840dd7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ownOtIBWQO0J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":372},"outputId":"c404c8ab-6893-40c7-cb1d-9fa3d7eeeebb","executionInfo":{"status":"ok","timestamp":1581159755048,"user_tz":-330,"elapsed":1068,"user":{"displayName":"Vasisht Vasisht","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAVwnMADhdoq0kiAbKetXytg58LjCY-cepd5I4m=s64","userId":"18321686459009329525"}}},"source":["temptrainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=1, shuffle=False)\n","temptestloader = torch.utils.data.DataLoader(dataset=testset, batch_size=1, shuffle=False)\n","\n","train=[]\n","for data, target in temptrainloader:\n","  data, target = data.to(device), target.to(device)\n","  if target==3:\n","    out=teacher(data)\n","    out=F.softmax(out)\n","    \n","    for i in out:\n","      for j in i:\n","        train.append(j.item())\n","    #print(out)\n","    pred = out.data.max(1, keepdim=True)[1]\n","    print(pred)\n","    break\n","\n","test=[]\n","for data, target in temptestloader:\n","  data, target = data.to(device), target.to(device)\n","  if target==3:\n","    out=teacher(data)\n","    out=F.softmax(out)\n","    \n","    for i in out:\n","      for j in i:\n","        test.append(j.item())\n","    #print(out)\n","    pred = out.data.max(1, keepdim=True)[1]\n","    print(pred)\n","    break\n","\n","#print(train)\n","#print(test)\n","\n","\n","\n","\n","print(train)\n","print(test)\n","\n","import matplotlib.pyplot as plt\n","\n","ind = np.arange(len(train))\n","width = 1     \n","plt.xlim(right=10) \n","plt.bar(ind, train, width, label='train')\n","plt.bar(ind + width, test, width,label='test')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["tensor([[3]], device='cuda:0')\n","tensor([[3]], device='cuda:0')\n","[0.06327114254236221, 0.025417674332857132, 0.011714407242834568, 0.5831579566001892, 0.04046289622783661, 0.02152826450765133, 0.00799587182700634, 0.1768314242362976, 0.021124042570590973, 0.04849636182188988]\n","[0.0010145490523427725, 0.00030558116850443184, 0.0008154626120813191, 0.9787409901618958, 0.00029880856163799763, 0.010064681060612202, 0.00022722511494066566, 0.007986118085682392, 0.00020130971097387373, 0.00034515938023105264]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<BarContainer object of 10 artists>"]},"metadata":{"tags":[]},"execution_count":21},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTsgAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"WhgYCdffOD7L","colab_type":"code","outputId":"5693d936-10f5-4a59-f77e-26c855a25c9c","executionInfo":{"status":"error","timestamp":1581163045591,"user_tz":-330,"elapsed":2549017,"user":{"displayName":"Vasisht Vasisht","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAVwnMADhdoq0kiAbKetXytg58LjCY-cepd5I4m=s64","userId":"18321686459009329525"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["learner = BinNet()\n","\n","learner.cuda()\n","#learner = torch.nn.DataParallel(learner, device_ids=range(torch.cuda.device_count()))\n","\n","\n","optimizer = optim.Adam(learner.parameters(), lr=1e-3,weight_decay=0.0000)\n","criterion = nn.CrossEntropyLoss()\n","\n","# define the binarization operator\n","bin_op = BinOp(learner)\n","\n","distill_training(teacher, learner, trainloader, testloader, optimizer, criterion, 100)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[0/100] \n","Train Epoch: 0 [    0/50000 (  0%)]  Loss: 2.370922\n","Train Epoch: 0 [12800/50000 ( 26%)]  Loss: 2.000820\n","Train Epoch: 0 [25600/50000 ( 51%)]  Loss: 1.717410\n","Train Epoch: 0 [38400/50000 ( 77%)]  Loss: 1.755207\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0051e3b780>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0051e3b780>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Test set: Average loss: 0.0132, Accuracy: 17517/50000 (35.03%)\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0051e3b780>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0051e3b780>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n","    self._shutdown_workers()\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n","    w.join()\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Test set: Average loss: 0.0166, Accuracy: 3542/10000 (35.42%)\n","[1/100] \n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0051e3b780>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0051e3b780>>\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [    0/50000 (  0%)]  Loss: 1.847076\n"],"name":"stdout"},{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [12800/50000 ( 26%)]  Loss: 1.706652\n","Train Epoch: 1 [25600/50000 ( 51%)]  Loss: 1.549164\n","Train Epoch: 1 [38400/50000 ( 77%)]  Loss: 1.529888\n","Test set: Average loss: 0.0115, Accuracy: 22881/50000 (45.76%)\n","Test set: Average loss: 0.0144, Accuracy: 4680/10000 (46.80%)\n","[2/100] \n","Train Epoch: 2 [    0/50000 (  0%)]  Loss: 1.246835\n","Train Epoch: 2 [12800/50000 ( 26%)]  Loss: 1.351807\n","Train Epoch: 2 [25600/50000 ( 51%)]  Loss: 1.254956\n","Train Epoch: 2 [38400/50000 ( 77%)]  Loss: 1.407531\n","Test set: Average loss: 0.0111, Accuracy: 25098/50000 (50.20%)\n","Test set: Average loss: 0.0144, Accuracy: 4972/10000 (49.72%)\n","[3/100] \n","Train Epoch: 3 [    0/50000 (  0%)]  Loss: 1.475382\n","Train Epoch: 3 [12800/50000 ( 26%)]  Loss: 1.559021\n","Train Epoch: 3 [25600/50000 ( 51%)]  Loss: 1.318977\n","Train Epoch: 3 [38400/50000 ( 77%)]  Loss: 1.114582\n","Test set: Average loss: 0.0099, Accuracy: 27340/50000 (54.68%)\n","Test set: Average loss: 0.0128, Accuracy: 5424/10000 (54.24%)\n","[4/100] \n","Train Epoch: 4 [    0/50000 (  0%)]  Loss: 1.264966\n","Train Epoch: 4 [12800/50000 ( 26%)]  Loss: 1.340809\n","Train Epoch: 4 [25600/50000 ( 51%)]  Loss: 1.160772\n","Train Epoch: 4 [38400/50000 ( 77%)]  Loss: 1.158780\n","Test set: Average loss: 0.0094, Accuracy: 28470/50000 (56.94%)\n","Test set: Average loss: 0.0120, Accuracy: 5790/10000 (57.90%)\n","[5/100] \n","Train Epoch: 5 [    0/50000 (  0%)]  Loss: 1.287357\n","Train Epoch: 5 [12800/50000 ( 26%)]  Loss: 1.153960\n","Train Epoch: 5 [25600/50000 ( 51%)]  Loss: 1.098666\n","Train Epoch: 5 [38400/50000 ( 77%)]  Loss: 1.244242\n","Test set: Average loss: 0.0088, Accuracy: 30065/50000 (60.13%)\n","Test set: Average loss: 0.0117, Accuracy: 5955/10000 (59.55%)\n","[6/100] \n","Train Epoch: 6 [    0/50000 (  0%)]  Loss: 1.191926\n","Train Epoch: 6 [12800/50000 ( 26%)]  Loss: 1.077370\n","Train Epoch: 6 [25600/50000 ( 51%)]  Loss: 1.188929\n","Train Epoch: 6 [38400/50000 ( 77%)]  Loss: 1.226047\n","Test set: Average loss: 0.0088, Accuracy: 30217/50000 (60.43%)\n","Test set: Average loss: 0.0114, Accuracy: 6049/10000 (60.49%)\n","[7/100] \n","Train Epoch: 7 [    0/50000 (  0%)]  Loss: 0.987667\n","Train Epoch: 7 [12800/50000 ( 26%)]  Loss: 1.011495\n","Train Epoch: 7 [25600/50000 ( 51%)]  Loss: 1.306228\n","Train Epoch: 7 [38400/50000 ( 77%)]  Loss: 1.111506\n","Test set: Average loss: 0.0083, Accuracy: 31223/50000 (62.45%)\n","Test set: Average loss: 0.0109, Accuracy: 6158/10000 (61.58%)\n","[8/100] \n","Train Epoch: 8 [    0/50000 (  0%)]  Loss: 1.095079\n","Train Epoch: 8 [12800/50000 ( 26%)]  Loss: 1.327767\n","Train Epoch: 8 [25600/50000 ( 51%)]  Loss: 1.089564\n","Train Epoch: 8 [38400/50000 ( 77%)]  Loss: 0.862405\n","Test set: Average loss: 0.0080, Accuracy: 32216/50000 (64.43%)\n","Test set: Average loss: 0.0105, Accuracy: 6354/10000 (63.54%)\n","[9/100] \n","Train Epoch: 9 [    0/50000 (  0%)]  Loss: 1.046313\n","Train Epoch: 9 [12800/50000 ( 26%)]  Loss: 0.772349\n","Train Epoch: 9 [25600/50000 ( 51%)]  Loss: 1.039224\n","Train Epoch: 9 [38400/50000 ( 77%)]  Loss: 1.164751\n","Test set: Average loss: 0.0081, Accuracy: 32006/50000 (64.01%)\n","Test set: Average loss: 0.0106, Accuracy: 6289/10000 (62.89%)\n","[10/100] \n","Train Epoch: 10 [    0/50000 (  0%)]  Loss: 1.112576\n","Train Epoch: 10 [12800/50000 ( 26%)]  Loss: 1.053179\n","Train Epoch: 10 [25600/50000 ( 51%)]  Loss: 0.844833\n","Train Epoch: 10 [38400/50000 ( 77%)]  Loss: 0.902010\n","Test set: Average loss: 0.0077, Accuracy: 32816/50000 (65.63%)\n","Test set: Average loss: 0.0100, Accuracy: 6551/10000 (65.51%)\n","[11/100] \n","Train Epoch: 11 [    0/50000 (  0%)]  Loss: 0.814622\n","Train Epoch: 11 [12800/50000 ( 26%)]  Loss: 1.013628\n","Train Epoch: 11 [25600/50000 ( 51%)]  Loss: 1.035770\n","Train Epoch: 11 [38400/50000 ( 77%)]  Loss: 1.066217\n","Test set: Average loss: 0.0078, Accuracy: 32536/50000 (65.07%)\n","Test set: Average loss: 0.0104, Accuracy: 6400/10000 (64.00%)\n","[12/100] \n","Train Epoch: 12 [    0/50000 (  0%)]  Loss: 1.140087\n","Train Epoch: 12 [12800/50000 ( 26%)]  Loss: 0.973360\n","Train Epoch: 12 [25600/50000 ( 51%)]  Loss: 1.024606\n","Train Epoch: 12 [38400/50000 ( 77%)]  Loss: 1.057712\n","Test set: Average loss: 0.0074, Accuracy: 33355/50000 (66.71%)\n","Test set: Average loss: 0.0098, Accuracy: 6586/10000 (65.86%)\n","[13/100] \n","Train Epoch: 13 [    0/50000 (  0%)]  Loss: 0.837151\n","Train Epoch: 13 [12800/50000 ( 26%)]  Loss: 1.197467\n","Train Epoch: 13 [25600/50000 ( 51%)]  Loss: 0.930496\n","Train Epoch: 13 [38400/50000 ( 77%)]  Loss: 0.943911\n","Test set: Average loss: 0.0074, Accuracy: 33488/50000 (66.98%)\n","Test set: Average loss: 0.0098, Accuracy: 6561/10000 (65.61%)\n","[14/100] \n","Train Epoch: 14 [    0/50000 (  0%)]  Loss: 0.972682\n","Train Epoch: 14 [12800/50000 ( 26%)]  Loss: 0.884850\n","Train Epoch: 14 [25600/50000 ( 51%)]  Loss: 1.111441\n","Train Epoch: 14 [38400/50000 ( 77%)]  Loss: 0.956514\n","Test set: Average loss: 0.0071, Accuracy: 34175/50000 (68.35%)\n","Test set: Average loss: 0.0096, Accuracy: 6692/10000 (66.92%)\n","[15/100] \n","Train Epoch: 15 [    0/50000 (  0%)]  Loss: 0.795409\n","Train Epoch: 15 [12800/50000 ( 26%)]  Loss: 0.868738\n","Train Epoch: 15 [25600/50000 ( 51%)]  Loss: 1.055553\n","Train Epoch: 15 [38400/50000 ( 77%)]  Loss: 0.814921\n","Test set: Average loss: 0.0073, Accuracy: 33825/50000 (67.65%)\n","Test set: Average loss: 0.0097, Accuracy: 6729/10000 (67.29%)\n","[16/100] \n","Train Epoch: 16 [    0/50000 (  0%)]  Loss: 0.961469\n","Train Epoch: 16 [12800/50000 ( 26%)]  Loss: 0.986092\n","Train Epoch: 16 [25600/50000 ( 51%)]  Loss: 1.016285\n","Train Epoch: 16 [38400/50000 ( 77%)]  Loss: 0.984769\n","Test set: Average loss: 0.0069, Accuracy: 34505/50000 (69.01%)\n","Test set: Average loss: 0.0094, Accuracy: 6749/10000 (67.49%)\n","[17/100] \n","Train Epoch: 17 [    0/50000 (  0%)]  Loss: 0.971417\n","Train Epoch: 17 [12800/50000 ( 26%)]  Loss: 0.753801\n","Train Epoch: 17 [25600/50000 ( 51%)]  Loss: 0.883863\n","Train Epoch: 17 [38400/50000 ( 77%)]  Loss: 0.935240\n","Test set: Average loss: 0.0069, Accuracy: 34820/50000 (69.64%)\n","Test set: Average loss: 0.0095, Accuracy: 6778/10000 (67.78%)\n","[18/100] \n","Train Epoch: 18 [    0/50000 (  0%)]  Loss: 0.853582\n","Train Epoch: 18 [12800/50000 ( 26%)]  Loss: 0.731689\n","Train Epoch: 18 [25600/50000 ( 51%)]  Loss: 0.956347\n","Train Epoch: 18 [38400/50000 ( 77%)]  Loss: 0.783207\n","Test set: Average loss: 0.0071, Accuracy: 34217/50000 (68.43%)\n","Test set: Average loss: 0.0098, Accuracy: 6635/10000 (66.35%)\n","[19/100] \n","Train Epoch: 19 [    0/50000 (  0%)]  Loss: 0.847882\n","Train Epoch: 19 [12800/50000 ( 26%)]  Loss: 1.004500\n","Train Epoch: 19 [25600/50000 ( 51%)]  Loss: 0.885006\n","Train Epoch: 19 [38400/50000 ( 77%)]  Loss: 0.946064\n","Test set: Average loss: 0.0066, Accuracy: 35279/50000 (70.56%)\n","Test set: Average loss: 0.0091, Accuracy: 6896/10000 (68.96%)\n","[20/100] \n","Train Epoch: 20 [    0/50000 (  0%)]  Loss: 0.720469\n","Train Epoch: 20 [12800/50000 ( 26%)]  Loss: 1.087267\n","Train Epoch: 20 [25600/50000 ( 51%)]  Loss: 0.746532\n","Train Epoch: 20 [38400/50000 ( 77%)]  Loss: 0.708105\n","Test set: Average loss: 0.0066, Accuracy: 35403/50000 (70.81%)\n","Test set: Average loss: 0.0089, Accuracy: 6966/10000 (69.66%)\n","[21/100] \n","Train Epoch: 21 [    0/50000 (  0%)]  Loss: 0.816279\n","Train Epoch: 21 [12800/50000 ( 26%)]  Loss: 0.719657\n","Train Epoch: 21 [25600/50000 ( 51%)]  Loss: 0.883080\n","Train Epoch: 21 [38400/50000 ( 77%)]  Loss: 0.943264\n","Test set: Average loss: 0.0066, Accuracy: 35431/50000 (70.86%)\n","Test set: Average loss: 0.0089, Accuracy: 6993/10000 (69.93%)\n","[22/100] \n","Train Epoch: 22 [    0/50000 (  0%)]  Loss: 1.009034\n","Train Epoch: 22 [12800/50000 ( 26%)]  Loss: 0.917406\n","Train Epoch: 22 [25600/50000 ( 51%)]  Loss: 0.772798\n","Train Epoch: 22 [38400/50000 ( 77%)]  Loss: 0.959329\n","Test set: Average loss: 0.0064, Accuracy: 35827/50000 (71.65%)\n","Test set: Average loss: 0.0086, Accuracy: 7031/10000 (70.31%)\n","[23/100] \n","Train Epoch: 23 [    0/50000 (  0%)]  Loss: 0.743948\n","Train Epoch: 23 [12800/50000 ( 26%)]  Loss: 0.845782\n","Train Epoch: 23 [25600/50000 ( 51%)]  Loss: 0.721196\n","Train Epoch: 23 [38400/50000 ( 77%)]  Loss: 0.740840\n","Test set: Average loss: 0.0066, Accuracy: 35822/50000 (71.64%)\n","Test set: Average loss: 0.0093, Accuracy: 6849/10000 (68.49%)\n","[24/100] \n","Train Epoch: 24 [    0/50000 (  0%)]  Loss: 0.792355\n","Train Epoch: 24 [12800/50000 ( 26%)]  Loss: 0.684186\n","Train Epoch: 24 [25600/50000 ( 51%)]  Loss: 1.010811\n","Train Epoch: 24 [38400/50000 ( 77%)]  Loss: 0.948676\n","Test set: Average loss: 0.0062, Accuracy: 36334/50000 (72.67%)\n","Test set: Average loss: 0.0086, Accuracy: 7065/10000 (70.65%)\n","[25/100] \n","Train Epoch: 25 [    0/50000 (  0%)]  Loss: 0.778137\n","Train Epoch: 25 [12800/50000 ( 26%)]  Loss: 0.887661\n","Train Epoch: 25 [25600/50000 ( 51%)]  Loss: 0.779933\n","Train Epoch: 25 [38400/50000 ( 77%)]  Loss: 0.798439\n","Test set: Average loss: 0.0062, Accuracy: 36402/50000 (72.80%)\n","Test set: Average loss: 0.0087, Accuracy: 7046/10000 (70.46%)\n","[26/100] \n","Train Epoch: 26 [    0/50000 (  0%)]  Loss: 0.592882\n","Train Epoch: 26 [12800/50000 ( 26%)]  Loss: 0.805199\n","Train Epoch: 26 [25600/50000 ( 51%)]  Loss: 0.783602\n","Train Epoch: 26 [38400/50000 ( 77%)]  Loss: 0.699774\n","Test set: Average loss: 0.0064, Accuracy: 36031/50000 (72.06%)\n","Test set: Average loss: 0.0088, Accuracy: 7001/10000 (70.01%)\n","[27/100] \n","Train Epoch: 27 [    0/50000 (  0%)]  Loss: 0.826363\n","Train Epoch: 27 [12800/50000 ( 26%)]  Loss: 0.685261\n","Train Epoch: 27 [25600/50000 ( 51%)]  Loss: 0.969571\n","Train Epoch: 27 [38400/50000 ( 77%)]  Loss: 0.719844\n","Test set: Average loss: 0.0063, Accuracy: 35914/50000 (71.83%)\n","Test set: Average loss: 0.0087, Accuracy: 7056/10000 (70.56%)\n","[28/100] \n","Train Epoch: 28 [    0/50000 (  0%)]  Loss: 0.714983\n","Train Epoch: 28 [12800/50000 ( 26%)]  Loss: 0.708261\n","Train Epoch: 28 [25600/50000 ( 51%)]  Loss: 0.762892\n","Train Epoch: 28 [38400/50000 ( 77%)]  Loss: 0.719446\n","Test set: Average loss: 0.0063, Accuracy: 36313/50000 (72.63%)\n","Test set: Average loss: 0.0090, Accuracy: 6992/10000 (69.92%)\n","[29/100] \n","Train Epoch: 29 [    0/50000 (  0%)]  Loss: 0.929602\n","Train Epoch: 29 [12800/50000 ( 26%)]  Loss: 0.762544\n","Train Epoch: 29 [25600/50000 ( 51%)]  Loss: 0.958053\n","Train Epoch: 29 [38400/50000 ( 77%)]  Loss: 0.656779\n","Test set: Average loss: 0.0062, Accuracy: 36465/50000 (72.93%)\n","Test set: Average loss: 0.0087, Accuracy: 7096/10000 (70.96%)\n","[30/100] \n","Train Epoch: 30 [    0/50000 (  0%)]  Loss: 0.795403\n","Train Epoch: 30 [12800/50000 ( 26%)]  Loss: 0.797233\n","Train Epoch: 30 [25600/50000 ( 51%)]  Loss: 0.852160\n","Train Epoch: 30 [38400/50000 ( 77%)]  Loss: 0.745277\n","Test set: Average loss: 0.0061, Accuracy: 36535/50000 (73.07%)\n","Test set: Average loss: 0.0085, Accuracy: 7071/10000 (70.71%)\n","[31/100] \n","Train Epoch: 31 [    0/50000 (  0%)]  Loss: 0.774996\n","Train Epoch: 31 [12800/50000 ( 26%)]  Loss: 0.921379\n","Train Epoch: 31 [25600/50000 ( 51%)]  Loss: 0.745374\n","Train Epoch: 31 [38400/50000 ( 77%)]  Loss: 0.857982\n","Test set: Average loss: 0.0061, Accuracy: 36674/50000 (73.35%)\n","Test set: Average loss: 0.0086, Accuracy: 7159/10000 (71.59%)\n","[32/100] \n","Train Epoch: 32 [    0/50000 (  0%)]  Loss: 0.641346\n","Train Epoch: 32 [12800/50000 ( 26%)]  Loss: 0.660423\n","Train Epoch: 32 [25600/50000 ( 51%)]  Loss: 0.989083\n","Train Epoch: 32 [38400/50000 ( 77%)]  Loss: 0.746471\n","Test set: Average loss: 0.0061, Accuracy: 36650/50000 (73.30%)\n","Test set: Average loss: 0.0084, Accuracy: 7122/10000 (71.22%)\n","[33/100] \n","Train Epoch: 33 [    0/50000 (  0%)]  Loss: 0.900626\n","Train Epoch: 33 [12800/50000 ( 26%)]  Loss: 0.820013\n","Train Epoch: 33 [25600/50000 ( 51%)]  Loss: 0.712023\n","Train Epoch: 33 [38400/50000 ( 77%)]  Loss: 0.679332\n","Test set: Average loss: 0.0060, Accuracy: 36672/50000 (73.34%)\n","Test set: Average loss: 0.0084, Accuracy: 7139/10000 (71.39%)\n","[34/100] \n","Train Epoch: 34 [    0/50000 (  0%)]  Loss: 0.577030\n","Train Epoch: 34 [12800/50000 ( 26%)]  Loss: 1.018222\n","Train Epoch: 34 [25600/50000 ( 51%)]  Loss: 0.818496\n","Train Epoch: 34 [38400/50000 ( 77%)]  Loss: 0.803954\n","Test set: Average loss: 0.0058, Accuracy: 37297/50000 (74.59%)\n","Test set: Average loss: 0.0082, Accuracy: 7224/10000 (72.24%)\n","[35/100] \n","Train Epoch: 35 [    0/50000 (  0%)]  Loss: 0.770667\n","Train Epoch: 35 [12800/50000 ( 26%)]  Loss: 0.816777\n","Train Epoch: 35 [25600/50000 ( 51%)]  Loss: 0.841382\n","Train Epoch: 35 [38400/50000 ( 77%)]  Loss: 0.790307\n","Test set: Average loss: 0.0059, Accuracy: 37028/50000 (74.06%)\n","Test set: Average loss: 0.0081, Accuracy: 7243/10000 (72.43%)\n","[36/100] \n","Train Epoch: 36 [    0/50000 (  0%)]  Loss: 0.692466\n","Train Epoch: 36 [12800/50000 ( 26%)]  Loss: 0.735296\n","Train Epoch: 36 [25600/50000 ( 51%)]  Loss: 0.964445\n","Train Epoch: 36 [38400/50000 ( 77%)]  Loss: 0.666567\n","Test set: Average loss: 0.0060, Accuracy: 36859/50000 (73.72%)\n","Test set: Average loss: 0.0084, Accuracy: 7158/10000 (71.58%)\n","[37/100] \n","Train Epoch: 37 [    0/50000 (  0%)]  Loss: 0.856166\n","Train Epoch: 37 [12800/50000 ( 26%)]  Loss: 0.969795\n","Train Epoch: 37 [25600/50000 ( 51%)]  Loss: 0.826428\n","Train Epoch: 37 [38400/50000 ( 77%)]  Loss: 0.756878\n","Test set: Average loss: 0.0056, Accuracy: 37593/50000 (75.19%)\n","Test set: Average loss: 0.0079, Accuracy: 7318/10000 (73.18%)\n","[38/100] \n","Train Epoch: 38 [    0/50000 (  0%)]  Loss: 0.641116\n","Train Epoch: 38 [12800/50000 ( 26%)]  Loss: 0.720537\n","Train Epoch: 38 [25600/50000 ( 51%)]  Loss: 0.721140\n","Train Epoch: 38 [38400/50000 ( 77%)]  Loss: 0.741157\n","Test set: Average loss: 0.0057, Accuracy: 37654/50000 (75.31%)\n","Test set: Average loss: 0.0080, Accuracy: 7346/10000 (73.46%)\n","[39/100] \n","Train Epoch: 39 [    0/50000 (  0%)]  Loss: 0.788310\n","Train Epoch: 39 [12800/50000 ( 26%)]  Loss: 0.789920\n","Train Epoch: 39 [25600/50000 ( 51%)]  Loss: 0.637746\n","Train Epoch: 39 [38400/50000 ( 77%)]  Loss: 0.636052\n","Test set: Average loss: 0.0057, Accuracy: 37379/50000 (74.76%)\n","Test set: Average loss: 0.0080, Accuracy: 7303/10000 (73.03%)\n","[40/100] \n","Train Epoch: 40 [    0/50000 (  0%)]  Loss: 0.754460\n","Train Epoch: 40 [12800/50000 ( 26%)]  Loss: 0.769786\n","Train Epoch: 40 [25600/50000 ( 51%)]  Loss: 0.727335\n","Train Epoch: 40 [38400/50000 ( 77%)]  Loss: 0.744841\n","Test set: Average loss: 0.0058, Accuracy: 37367/50000 (74.73%)\n","Test set: Average loss: 0.0082, Accuracy: 7283/10000 (72.83%)\n","[41/100] \n","Train Epoch: 41 [    0/50000 (  0%)]  Loss: 0.748080\n","Train Epoch: 41 [12800/50000 ( 26%)]  Loss: 0.661251\n","Train Epoch: 41 [25600/50000 ( 51%)]  Loss: 0.774814\n","Train Epoch: 41 [38400/50000 ( 77%)]  Loss: 0.702957\n","Test set: Average loss: 0.0058, Accuracy: 37378/50000 (74.76%)\n","Test set: Average loss: 0.0081, Accuracy: 7276/10000 (72.76%)\n","[42/100] \n","Train Epoch: 42 [    0/50000 (  0%)]  Loss: 0.643846\n","Train Epoch: 42 [12800/50000 ( 26%)]  Loss: 0.763156\n","Train Epoch: 42 [25600/50000 ( 51%)]  Loss: 0.708573\n","Train Epoch: 42 [38400/50000 ( 77%)]  Loss: 0.783702\n","Test set: Average loss: 0.0056, Accuracy: 37736/50000 (75.47%)\n","Test set: Average loss: 0.0081, Accuracy: 7309/10000 (73.09%)\n","[43/100] \n","Train Epoch: 43 [    0/50000 (  0%)]  Loss: 0.805304\n","Train Epoch: 43 [12800/50000 ( 26%)]  Loss: 0.801512\n","Train Epoch: 43 [25600/50000 ( 51%)]  Loss: 0.772138\n","Train Epoch: 43 [38400/50000 ( 77%)]  Loss: 0.778490\n","Test set: Average loss: 0.0060, Accuracy: 36953/50000 (73.91%)\n","Test set: Average loss: 0.0082, Accuracy: 7160/10000 (71.60%)\n","[44/100] \n","Train Epoch: 44 [    0/50000 (  0%)]  Loss: 0.803564\n","Train Epoch: 44 [12800/50000 ( 26%)]  Loss: 0.702850\n","Train Epoch: 44 [25600/50000 ( 51%)]  Loss: 0.898520\n","Train Epoch: 44 [38400/50000 ( 77%)]  Loss: 0.672665\n","Test set: Average loss: 0.0056, Accuracy: 37778/50000 (75.56%)\n","Test set: Average loss: 0.0081, Accuracy: 7285/10000 (72.85%)\n","[45/100] \n","Train Epoch: 45 [    0/50000 (  0%)]  Loss: 0.583351\n","Train Epoch: 45 [12800/50000 ( 26%)]  Loss: 0.605905\n","Train Epoch: 45 [25600/50000 ( 51%)]  Loss: 0.819466\n","Train Epoch: 45 [38400/50000 ( 77%)]  Loss: 0.632485\n","Test set: Average loss: 0.0056, Accuracy: 37841/50000 (75.68%)\n","Test set: Average loss: 0.0080, Accuracy: 7367/10000 (73.67%)\n","[46/100] \n","Train Epoch: 46 [    0/50000 (  0%)]  Loss: 0.998927\n","Train Epoch: 46 [12800/50000 ( 26%)]  Loss: 0.716486\n","Train Epoch: 46 [25600/50000 ( 51%)]  Loss: 0.676122\n","Train Epoch: 46 [38400/50000 ( 77%)]  Loss: 0.675769\n","Test set: Average loss: 0.0057, Accuracy: 37575/50000 (75.15%)\n","Test set: Average loss: 0.0081, Accuracy: 7274/10000 (72.74%)\n","[47/100] \n","Train Epoch: 47 [    0/50000 (  0%)]  Loss: 0.681423\n","Train Epoch: 47 [12800/50000 ( 26%)]  Loss: 0.598559\n","Train Epoch: 47 [25600/50000 ( 51%)]  Loss: 0.772916\n","Train Epoch: 47 [38400/50000 ( 77%)]  Loss: 0.709947\n","Test set: Average loss: 0.0057, Accuracy: 37774/50000 (75.55%)\n","Test set: Average loss: 0.0081, Accuracy: 7270/10000 (72.70%)\n","[48/100] \n","Train Epoch: 48 [    0/50000 (  0%)]  Loss: 0.720150\n","Train Epoch: 48 [12800/50000 ( 26%)]  Loss: 0.645638\n","Train Epoch: 48 [25600/50000 ( 51%)]  Loss: 0.594824\n","Train Epoch: 48 [38400/50000 ( 77%)]  Loss: 0.690046\n","Test set: Average loss: 0.0054, Accuracy: 38081/50000 (76.16%)\n","Test set: Average loss: 0.0079, Accuracy: 7393/10000 (73.93%)\n","[49/100] \n","Train Epoch: 49 [    0/50000 (  0%)]  Loss: 0.585303\n","Train Epoch: 49 [12800/50000 ( 26%)]  Loss: 0.579726\n","Train Epoch: 49 [25600/50000 ( 51%)]  Loss: 0.515374\n","Train Epoch: 49 [38400/50000 ( 77%)]  Loss: 0.667088\n","Test set: Average loss: 0.0056, Accuracy: 37618/50000 (75.24%)\n","Test set: Average loss: 0.0080, Accuracy: 7243/10000 (72.43%)\n","[50/100] \n","Train Epoch: 50 [    0/50000 (  0%)]  Loss: 0.708973\n","Train Epoch: 50 [12800/50000 ( 26%)]  Loss: 0.792018\n","Train Epoch: 50 [25600/50000 ( 51%)]  Loss: 0.817580\n","Train Epoch: 50 [38400/50000 ( 77%)]  Loss: 0.649795\n","Test set: Average loss: 0.0059, Accuracy: 37132/50000 (74.26%)\n","Test set: Average loss: 0.0085, Accuracy: 7154/10000 (71.54%)\n","[51/100] \n","Train Epoch: 51 [    0/50000 (  0%)]  Loss: 0.683213\n","Train Epoch: 51 [12800/50000 ( 26%)]  Loss: 0.670877\n","Train Epoch: 51 [25600/50000 ( 51%)]  Loss: 0.524437\n","Train Epoch: 51 [38400/50000 ( 77%)]  Loss: 0.800571\n","Test set: Average loss: 0.0053, Accuracy: 38639/50000 (77.28%)\n","Test set: Average loss: 0.0077, Accuracy: 7406/10000 (74.06%)\n","[52/100] \n","Train Epoch: 52 [    0/50000 (  0%)]  Loss: 0.719563\n","Train Epoch: 52 [12800/50000 ( 26%)]  Loss: 0.567498\n","Train Epoch: 52 [25600/50000 ( 51%)]  Loss: 0.630885\n","Train Epoch: 52 [38400/50000 ( 77%)]  Loss: 0.766570\n","Test set: Average loss: 0.0056, Accuracy: 38014/50000 (76.03%)\n","Test set: Average loss: 0.0080, Accuracy: 7369/10000 (73.69%)\n","[53/100] \n","Train Epoch: 53 [    0/50000 (  0%)]  Loss: 0.599710\n","Train Epoch: 53 [12800/50000 ( 26%)]  Loss: 0.620852\n","Train Epoch: 53 [25600/50000 ( 51%)]  Loss: 0.594682\n","Train Epoch: 53 [38400/50000 ( 77%)]  Loss: 0.730094\n","Test set: Average loss: 0.0055, Accuracy: 38117/50000 (76.23%)\n","Test set: Average loss: 0.0079, Accuracy: 7376/10000 (73.76%)\n","[54/100] \n","Train Epoch: 54 [    0/50000 (  0%)]  Loss: 0.647725\n","Train Epoch: 54 [12800/50000 ( 26%)]  Loss: 0.809034\n","Train Epoch: 54 [25600/50000 ( 51%)]  Loss: 0.922816\n","Train Epoch: 54 [38400/50000 ( 77%)]  Loss: 0.586625\n","Test set: Average loss: 0.0053, Accuracy: 38482/50000 (76.96%)\n","Test set: Average loss: 0.0076, Accuracy: 7431/10000 (74.31%)\n","[55/100] \n","Train Epoch: 55 [    0/50000 (  0%)]  Loss: 0.494341\n","Train Epoch: 55 [12800/50000 ( 26%)]  Loss: 0.657068\n","Train Epoch: 55 [25600/50000 ( 51%)]  Loss: 0.692396\n","Train Epoch: 55 [38400/50000 ( 77%)]  Loss: 0.666077\n","Test set: Average loss: 0.0054, Accuracy: 38226/50000 (76.45%)\n","Test set: Average loss: 0.0078, Accuracy: 7412/10000 (74.12%)\n","[56/100] \n","Train Epoch: 56 [    0/50000 (  0%)]  Loss: 0.831727\n","Train Epoch: 56 [12800/50000 ( 26%)]  Loss: 0.868982\n","Train Epoch: 56 [25600/50000 ( 51%)]  Loss: 0.639196\n","Train Epoch: 56 [38400/50000 ( 77%)]  Loss: 0.763863\n","Test set: Average loss: 0.0052, Accuracy: 38718/50000 (77.44%)\n","Test set: Average loss: 0.0075, Accuracy: 7475/10000 (74.75%)\n","[57/100] \n","Train Epoch: 57 [    0/50000 (  0%)]  Loss: 0.715732\n","Train Epoch: 57 [12800/50000 ( 26%)]  Loss: 0.531950\n","Train Epoch: 57 [25600/50000 ( 51%)]  Loss: 0.627065\n","Train Epoch: 57 [38400/50000 ( 77%)]  Loss: 0.698558\n","Test set: Average loss: 0.0054, Accuracy: 38130/50000 (76.26%)\n","Test set: Average loss: 0.0079, Accuracy: 7319/10000 (73.19%)\n","[58/100] \n","Train Epoch: 58 [    0/50000 (  0%)]  Loss: 0.771332\n","Train Epoch: 58 [12800/50000 ( 26%)]  Loss: 0.530781\n","Train Epoch: 58 [25600/50000 ( 51%)]  Loss: 0.529921\n","Train Epoch: 58 [38400/50000 ( 77%)]  Loss: 0.819567\n","Test set: Average loss: 0.0052, Accuracy: 38599/50000 (77.20%)\n","Test set: Average loss: 0.0076, Accuracy: 7400/10000 (74.00%)\n","[59/100] \n","Train Epoch: 59 [    0/50000 (  0%)]  Loss: 0.624281\n","Train Epoch: 59 [12800/50000 ( 26%)]  Loss: 0.803244\n","Train Epoch: 59 [25600/50000 ( 51%)]  Loss: 0.748685\n","Train Epoch: 59 [38400/50000 ( 77%)]  Loss: 0.636969\n","Test set: Average loss: 0.0055, Accuracy: 38047/50000 (76.09%)\n","Test set: Average loss: 0.0078, Accuracy: 7387/10000 (73.87%)\n","[60/100] \n","Train Epoch: 60 [    0/50000 (  0%)]  Loss: 0.597774\n","Train Epoch: 60 [12800/50000 ( 26%)]  Loss: 0.776501\n","Train Epoch: 60 [25600/50000 ( 51%)]  Loss: 0.759500\n","Train Epoch: 60 [38400/50000 ( 77%)]  Loss: 0.601743\n","Test set: Average loss: 0.0053, Accuracy: 38608/50000 (77.22%)\n","Test set: Average loss: 0.0078, Accuracy: 7421/10000 (74.21%)\n","[61/100] \n","Train Epoch: 61 [    0/50000 (  0%)]  Loss: 0.635429\n","Train Epoch: 61 [12800/50000 ( 26%)]  Loss: 0.628040\n","Train Epoch: 61 [25600/50000 ( 51%)]  Loss: 0.669052\n","Train Epoch: 61 [38400/50000 ( 77%)]  Loss: 0.550039\n","Test set: Average loss: 0.0053, Accuracy: 38267/50000 (76.53%)\n","Test set: Average loss: 0.0079, Accuracy: 7414/10000 (74.14%)\n","[62/100] \n","Train Epoch: 62 [    0/50000 (  0%)]  Loss: 0.632725\n","Train Epoch: 62 [12800/50000 ( 26%)]  Loss: 0.803332\n","Train Epoch: 62 [25600/50000 ( 51%)]  Loss: 0.620360\n","Train Epoch: 62 [38400/50000 ( 77%)]  Loss: 0.582694\n","Test set: Average loss: 0.0054, Accuracy: 38175/50000 (76.35%)\n","Test set: Average loss: 0.0078, Accuracy: 7371/10000 (73.71%)\n","[63/100] \n","Train Epoch: 63 [    0/50000 (  0%)]  Loss: 0.494686\n","Train Epoch: 63 [12800/50000 ( 26%)]  Loss: 0.716808\n","Train Epoch: 63 [25600/50000 ( 51%)]  Loss: 0.726269\n","Train Epoch: 63 [38400/50000 ( 77%)]  Loss: 0.571441\n","Test set: Average loss: 0.0051, Accuracy: 38788/50000 (77.58%)\n","Test set: Average loss: 0.0076, Accuracy: 7415/10000 (74.15%)\n","[64/100] \n","Train Epoch: 64 [    0/50000 (  0%)]  Loss: 0.678535\n","Train Epoch: 64 [12800/50000 ( 26%)]  Loss: 0.609016\n","Train Epoch: 64 [25600/50000 ( 51%)]  Loss: 0.891135\n","Train Epoch: 64 [38400/50000 ( 77%)]  Loss: 0.637833\n","Test set: Average loss: 0.0052, Accuracy: 38750/50000 (77.50%)\n","Test set: Average loss: 0.0077, Accuracy: 7438/10000 (74.38%)\n","[65/100] \n","Train Epoch: 65 [    0/50000 (  0%)]  Loss: 0.515697\n","Train Epoch: 65 [12800/50000 ( 26%)]  Loss: 0.676985\n","Train Epoch: 65 [25600/50000 ( 51%)]  Loss: 0.675472\n","Train Epoch: 65 [38400/50000 ( 77%)]  Loss: 0.852067\n","Test set: Average loss: 0.0050, Accuracy: 39086/50000 (78.17%)\n","Test set: Average loss: 0.0074, Accuracy: 7423/10000 (74.23%)\n","[66/100] \n","Train Epoch: 66 [    0/50000 (  0%)]  Loss: 0.720993\n","Train Epoch: 66 [12800/50000 ( 26%)]  Loss: 0.561331\n","Train Epoch: 66 [25600/50000 ( 51%)]  Loss: 0.849138\n","Train Epoch: 66 [38400/50000 ( 77%)]  Loss: 0.646464\n","Test set: Average loss: 0.0053, Accuracy: 38327/50000 (76.65%)\n","Test set: Average loss: 0.0079, Accuracy: 7409/10000 (74.09%)\n","[67/100] \n","Train Epoch: 67 [    0/50000 (  0%)]  Loss: 0.751599\n","Train Epoch: 67 [12800/50000 ( 26%)]  Loss: 0.627062\n","Train Epoch: 67 [25600/50000 ( 51%)]  Loss: 0.797460\n","Train Epoch: 67 [38400/50000 ( 77%)]  Loss: 0.783150\n","Test set: Average loss: 0.0050, Accuracy: 39097/50000 (78.19%)\n","Test set: Average loss: 0.0076, Accuracy: 7459/10000 (74.59%)\n","[68/100] \n","Train Epoch: 68 [    0/50000 (  0%)]  Loss: 0.650940\n","Train Epoch: 68 [12800/50000 ( 26%)]  Loss: 0.625518\n","Train Epoch: 68 [25600/50000 ( 51%)]  Loss: 0.844924\n","Train Epoch: 68 [38400/50000 ( 77%)]  Loss: 0.709303\n","Test set: Average loss: 0.0051, Accuracy: 38933/50000 (77.87%)\n","Test set: Average loss: 0.0076, Accuracy: 7493/10000 (74.93%)\n","[69/100] \n","Train Epoch: 69 [    0/50000 (  0%)]  Loss: 0.598464\n","Train Epoch: 69 [12800/50000 ( 26%)]  Loss: 0.517088\n","Train Epoch: 69 [25600/50000 ( 51%)]  Loss: 0.633226\n","Train Epoch: 69 [38400/50000 ( 77%)]  Loss: 0.671402\n","Test set: Average loss: 0.0051, Accuracy: 38895/50000 (77.79%)\n","Test set: Average loss: 0.0076, Accuracy: 7442/10000 (74.42%)\n","[70/100] \n","Train Epoch: 70 [    0/50000 (  0%)]  Loss: 0.559209\n","Train Epoch: 70 [12800/50000 ( 26%)]  Loss: 0.884815\n","Train Epoch: 70 [25600/50000 ( 51%)]  Loss: 0.608353\n","Train Epoch: 70 [38400/50000 ( 77%)]  Loss: 0.820913\n","Test set: Average loss: 0.0052, Accuracy: 38649/50000 (77.30%)\n","Test set: Average loss: 0.0078, Accuracy: 7394/10000 (73.94%)\n","[71/100] \n","Train Epoch: 71 [    0/50000 (  0%)]  Loss: 0.682887\n","Train Epoch: 71 [12800/50000 ( 26%)]  Loss: 0.655608\n","Train Epoch: 71 [25600/50000 ( 51%)]  Loss: 0.639725\n","Train Epoch: 71 [38400/50000 ( 77%)]  Loss: 0.529359\n","Test set: Average loss: 0.0051, Accuracy: 38779/50000 (77.56%)\n","Test set: Average loss: 0.0077, Accuracy: 7481/10000 (74.81%)\n","[72/100] \n","Train Epoch: 72 [    0/50000 (  0%)]  Loss: 0.625665\n","Train Epoch: 72 [12800/50000 ( 26%)]  Loss: 0.436420\n","Train Epoch: 72 [25600/50000 ( 51%)]  Loss: 0.733179\n","Train Epoch: 72 [38400/50000 ( 77%)]  Loss: 0.594189\n","Test set: Average loss: 0.0051, Accuracy: 38996/50000 (77.99%)\n","Test set: Average loss: 0.0076, Accuracy: 7498/10000 (74.98%)\n","[73/100] \n","Train Epoch: 73 [    0/50000 (  0%)]  Loss: 0.614822\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-acece55c8290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbin_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdistill_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-b50169840dd7>\u001b[0m in \u001b[0;36mdistill_training\u001b[0;34m(teacher, learner, data_loader, test_loader, optimizer, criterion, n_epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"bGPb5VBMe2O1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":372},"outputId":"9816ea8c-7942-4fb0-ebde-046633c267d9","executionInfo":{"status":"ok","timestamp":1581163339612,"user_tz":-330,"elapsed":1263,"user":{"displayName":"Vasisht Vasisht","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAVwnMADhdoq0kiAbKetXytg58LjCY-cepd5I4m=s64","userId":"18321686459009329525"}}},"source":["temptrainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=1, shuffle=True)\n","temptestloader = torch.utils.data.DataLoader(dataset=testset, batch_size=1, shuffle=True)\n","\n","train=[]\n","count=0\n","for data, target in temptrainloader:\n","  data, target = data.to(device), target.to(device)\n","  if target==3:\n","    out=learner(data)\n","    out=F.softmax(out)\n","    count+=1\n","    \n","    for i in out:\n","      for j in i:\n","        train.append(j.item())\n","    #print(out)\n","    pred = out.data.max(1, keepdim=True)[1]\n","    print(pred)\n","    break\n","\n","test=[]\n","count2=0\n","for data, target in temptestloader:\n","  data, target = data.to(device), target.to(device)\n","  if target==3:\n","    out=learner(data)\n","    out=F.softmax(out)\n","    count2+=1\n","    \n","    for i in out:\n","      for j in i:\n","        test.append(j.item())\n","    #print(out)\n","    pred = out.data.max(1, keepdim=True)[1]\n","    print(pred)\n","    break\n","\n","#print(train)\n","#print(test)\n","\n","\n","\n","\n","print(train)\n","print(test)\n","\n","import matplotlib.pyplot as plt\n","\n","ind = np.arange(len(train))\n","width = 1     \n","plt.xlim(right=10) \n","plt.bar(ind, train, width, label='train')\n","plt.bar(ind + width, test, width,label='test')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["tensor([[3]], device='cuda:0')\n","tensor([[2]], device='cuda:0')\n","[0.07000309228897095, 0.017400363460183144, 0.019030457362532616, 0.29907193779945374, 0.10515590757131577, 0.22833536565303802, 0.04266407713294029, 0.04584269970655441, 0.11086355894804001, 0.061632607132196426]\n","[0.12253900617361069, 0.0015809950418770313, 0.27642616629600525, 0.07850959151983261, 0.2582301199436188, 0.14231687784194946, 0.004606064409017563, 0.10550655424594879, 0.005763859022408724, 0.004520699847489595]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<BarContainer object of 10 artists>"]},"metadata":{"tags":[]},"execution_count":41},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJIaZQBIUqP+D/V8h4DlzNoWAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"O3Eh90Rzk5io","colab_type":"code","colab":{}},"source":["def softmax_by_row(logits, T = 1.0):\n","    mx = np.max(logits, axis=-1, keepdims=True)\n","    exp = np.exp((logits - mx)/T)\n","    denominator = np.sum(exp, axis=-1, keepdims=True)\n","    return exp/denominator\n","\n","def classifier_performance(model, train_loader, test_loader):\n","\n","    output_train_benign = []\n","    train_label = []\n","    for num, data in enumerate(train_loader):\n","        images,labels = data\n","        image_tensor= images.to(device)\n","        img_variable = Variable(image_tensor, requires_grad=True)\n","        output = model.forward(img_variable)\n","\n","        train_label.append(labels.numpy())\n","        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n","\n","\n","    train_label = np.concatenate(train_label)\n","    output_train_benign=np.concatenate(output_train_benign)\n","\n","    test_label = []\n","    output_test_benign = []\n","\n","    for num, data in enumerate(test_loader):\n","        images,labels = data\n","\n","        image_tensor= images.to(device)\n","        img_variable = Variable(image_tensor, requires_grad=True)\n","\n","        output = model.forward(img_variable)\n","\n","        test_label.append(labels.numpy())\n","        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n","\n","\n","    test_label = np.concatenate(test_label)\n","    output_test_benign=np.concatenate(output_test_benign)\n","\n","\n","    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n","    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n","\n","    print('Accuracy: ', (train_acc1, test_acc1))\n","\n","    return output_train_benign, output_test_benign, train_label, test_label\n","\n","\n","\n","\n","def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n","    \n","    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n","    confidence1 = []\n","    confidence2 = []\n","    acc1 = 0\n","    acc2 = 0\n","    for num in range(confidence_mtx1.shape[0]):\n","        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n","        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n","            acc1 += 1\n","            \n","    for num in range(confidence_mtx2.shape[0]):\n","        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n","        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n","            acc2 += 1\n","    confidence1 = np.array(confidence1)\n","    confidence2 = np.array(confidence2)\n","    \n","    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n","    \n","    \n","    #sort_confidence = np.sort(confidence1)\n","    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n","    max_accuracy = 0.5\n","    best_precision = 0.5\n","    best_recall = 0.5\n","    for num in range(len(sort_confidence)):\n","        delta = sort_confidence[num]\n","        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n","        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n","        accuracy_now = 0.5*(ratio1+1-ratio2)\n","        if accuracy_now > max_accuracy:\n","            max_accuracy = accuracy_now\n","            best_precision = ratio1/(ratio1+ratio2)\n","            best_recall = ratio1\n","    print('membership inference accuracy is:', max_accuracy)\n","    return max_accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOgF49n9k-46","colab_type":"code","colab":{}},"source":["from torch.autograd import Variable\n","import os\n","import numpy as np\n","import math \n","import scipy\n","import sys  \n","\n","\n","output_train, output_test, train_label, test_label = classifier_performance(learner, trainloader, testloader)\n","inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n","print(\"Maximum Accuracy:\",inference_accuracy)"],"execution_count":0,"outputs":[]}]}