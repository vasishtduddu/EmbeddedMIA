\section{Selecting the primitive for Efficient Architecture}

In designing our training methodology, \method, we aim to satisfy the following main requirements for embedded systems:

\begin{itemize}
\item {\em Privacy-}
The model should preserve the privacy of an individual's data record in the training set of the model against inference attacks.

\item {\em Accuracy-}
The drop in accuracy of the private model should be minimum as compared to the original accuracy.

\item {\em Efficiency-}
The private model should demonstrate high efficiency, i.e, energy, memory and computation efficiency.
\end{itemize}

To this end, we present \method --- a technique to construct deep neural network algorithms that are optimized for efficiency and accuracy while ensuring privacy of the input data. With \method, we present a novel approach of building NN models that
with efficiency as a key property. We argue that fixing a neural network architecture and then finding cryptographic schemes to ensure privacy does not provide optimal balance between efficiency and accuracy. On the contrary, our approach advocates building neural network architecture considering the strengths and drawbacks of the underlying cryptographic primitives. We assert that this is a practical solution as deep learning algorithms are flexible with respect to their architectures i.e., different neural network models can be trained to achieve the same accuracy for a given dataset.


\begin{table}[!htb]
\begin{center}
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\begin{tabular}{|l|l|l|l|}
\hline
Efficiency & Compression & Quantization & Architecture \\
\hline
Computation & $\times$   & $\checkmark$   & $\times$ \\
\hline
Memory &  $\checkmark$ & $\checkmark$   & $\checkmark$ \\
\hline
Energy &  $\checkmark$   & $\checkmark$   & $\times$ \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of different algorithms for efficient Deep learning.}
\label{tbl:comparison}
\end{table}

\input{fig_efficientArch}

In designing \method, we make several design choices with the goal of achieving efficiency over existing solutions. The most important among them is the selection of the underlying cryptographic primitive to ensure privacy of data.
Several cryptographic primitives such as Partially (PHE) and Fully Homomorphic Encryption (FHE) schemes, Goldreich-Micali-Widgerson (GMW) protocol, Arithmetic Secret Sharing (SS) schemes and Yao's Garbled Circuit (GC) have been proposed to enable two-party secure computation. However, each of these primitives perform differently with respect to the factors like efficiency, functionality, resources required and so on. PHE schemes are called so because they allow either addition or multiplication operations but not both on encrypted data~\cite{paillier1999public,elgamal1985public}. These schemes are efficient to use in applications that require limited functionalities. In contrast, FHE schemes enable both addition and multiplication on encrypted data~\cite{gentry,full-hom,van2010fully,brakerski2011fully}. Using these scheme, one can perform encrypted computation for any arbitrary operations. However, the computation cost increases for higher degree multiplication operations making it unsuitable for use in practical purposes. SS involves distributing the  secret shares among non-trusting users such any operation can be computed on encrypted data without revealing the individual inputs of each user~\cite{beaver1991efficient}.  GMW~\cite{goldreich1987play} and GC~\cite{yao1982protocols} allows designing boolean circuits and evaluating them between the client and server. Although expressive with respect to functionality, GMW, GC and SS require communication between the client and server to exchange keys or secrets shares for computing on encrypted data.
These differences makes it difficult to decide which primitive is the best fit for designing a privacy-preserving system for a particular application.
Therfore, we first outline the desirable properties specifically for private neural network inference and then compare these primitives with respect to these properties (as shown in Table~\ref{tbl:comparison}). We select a cryptographic scheme that satisfies all our requirements.


\noindent\textbf{Computation Efficiency.}

\noindent\textbf{Memory Efficiency.}

\noindent\textbf{Energy Efficiency.}


\subsection{Efficient Architecture Design}

Compact Network Architectures: The number of parameters and complexity of etwork can be optimized by careful design of the network architecture itself.
Here, instead of replacing larger filters with a set of smaller filters which have fewer weights in total when the filters are applied sequentially they have same overall receptive field.
For instance, one 5x5 filter can be replace with 2 3x3 filters. 1x1 convolutional layers can be used to reduce the number of channels in output feature map and hence the overall computation.
Forexample,32 ltersof1x1x64can transform an input with 64 channels to an output of 32 channels and reduce the number of  lter channels in the next layer to 32. SqueezeNet uses many 1 x 1  lters to aggressively reduce the number of weights [157]. It proposes a  re module that  rst “squeezes” the net- work with 1 x 1 convolution  lters and then expands it with multiple 1 x 1 and 3 x 3 convolution  lters.
It achieves an overall 50x reduction in the number of weights compared to AlexNet, while maintaining the same accuracy
Replace 3x3 filters with 1x1 filters.Given a budget of a certain number of convolutionfilters,  we will choose to ma
ecrease the number of input channels to 3x3 filters.Consider a convolution layerthat is comprised entirely of 3x3 filters. , to maintain a small total number of parametersin a CNN, it is important not only to decrease the number of 3x3 filters (see Strategy 1 above), butalso to decrease the number ofinput channelsto the 3x3 filters.  We decrease the number of inputchannels to 3x3 filters usingsqueeze layers, which we describe in the next section.
Downsample late in the network so that convolution layers have large activationmaps.In a convolutional network, each convolution layer produces an output activation map witha spatial resolution that is at least 1x1 and often much larger than 1x1.


\begin{table}[!htb]
\begin{center}
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Architecture} & \textbf{Train}  & \textbf{Test}  & \textbf{Inference}  & \textbf{Parameters} & \textbf{Memory} \\
 & \textbf{Accuracy} & \textbf{Accuracy} & \textbf{Accuracy} & & \textbf{Footprint} \\
\hline
SqueezeNet & 88.21\% & 81.92\% & \cellcolor{green!25}53.07\% & 1.2M & 5 MB\\
MobileNetV2 & 97.50\% & 87.24\% & \cellcolor{green!25}55.57\% & 3.5M & 14 MB\\
\hline
AlexNet & 97.86\% & 80.34\% & \cellcolor{red!25}60.40\% & 61M & 240 MB\\
VGG11 & 99.13\% & 86.43\% & \cellcolor{red!25}58.04\% & 132M & 507 MB\\
VGG16 & 99.58\% & 88.95\% & \cellcolor{red!25}58.70\% & 138M &  528 MB\\
VGG19 & 99.09\% & 88.18\% & \cellcolor{red!25}57.85\% & 143M & 549 MB\\
\hline
\end{tabular}
\end{center}
\caption{Model complexity influences the membership inference leakage. Model specifically designed for efficiency leak less information.}
\label{stdarch}
\end{table}


\subsection{Model Compression}

Network Pruning: Neural Networks are generally overparameterised. Hence, a large amount of weights are redundant adn can be removed (set to zero) referred to as pruning.
Aggressive pruning, however, requires to finetune the model to ensure no loss in the accuracy. Typically, this is done by removing the least significant nodes in the network by computing a threshold using the senstitivity hyperparameter which is used to estimate the percentile of values.

\[
    f(W)=
\begin{cases}
    0, & \text{if } w\geq T\\
    0, & \text{if } w\leq -T\\
    w,  & \text{otherwise}
\end{cases}
\]

The original pruning requires the model to be retrained after pruning to restore accruacy while ensuring lower network size.
Further, computation on these sparse parameter matrices using specialized accelerators enable to avoid computation on 0's lower the computation complexity significantly and increasing the overall inference speed.
Prior work has indicated that over 80\% of the parameters can be pruned and the accuracy can be restored after retraining.

Sparsity: Sparsity reduces some of the values in the network close to zero replaces them as zero. This results in the distribution with two modes instead of a single gaussian distribution for the parameters.
The sparsity constraint ensures that the parameters in the middle are zeroed out while the parameter values near the tail end are updated.

Sparse weights can be stored in a compressed format in the hardware using the compressed sparse row or column format which reduces the overall memory bandwidth[].
The decision on whether the computation is done is based on the parameter value which requires additional logic, i.e, replace output as zero without computation if parameter value is zero else perform the computation.
This also benefits in usage with SIMD or data parallel architectures and improves compression and reduces overall storage cost by using indices of weights with a zero values instead of actually storing zero values in the memory for each occurance.

Quantization reduces the precision of the operands while techniques such as pruning, sparsity reduce the number of model operation and model size.
Quantization maps parameters/activtions to a smaller set of quantization levels [qunaitized neural networks].
The ultimate goal is to minimize the error between the reconstructed data from hte quantized levels and the original data.
The number of quantized levels reflects the precision and ultimately the nuber of bits required to represent the data (log2(number of levels))
Reducing precision results in lower number of bits to represent the data which results in a lower storage cost (parameters) and/or computation cost (activation). less are and less energy [M Horowitz]
For instance, using kbits instead of 32 bits or 64 bits weights reduces the storage cost by 32/k or 64/k and hence, requires lower number of parameters to be read form the memory which further improves the energy efficiency by 32/kx or 64/kx.

The precision can be reduced aggresively to a single bit to get binary nets. Here, the weights  and activation are binarized during inference to take values \{-1,+1\} [binaryconnect, binarynet]. This allows to reduce the MAC operation to an XNOR however, with a singificant accuracy loss[xnor-net].
Several optimisaition have been considered to reduce the loss such as multiplying the outputs with a scaling factor to recover the dynamic range( weights become -w and w), keeping the first and last layer as 32 bit floating point precision and performing normatlisation before convolution to reduce the dynamic range of activations.
Further, hybrid and varying bit precision of weights and activations have been used in Dorefanet, HWGQnet and QNNs to reduce the qunatization loss.
Further, there are benefits for allowing weights to be zero (-w, 0, w) although this requires an additional bit per weight compared to binary weights, the sparsity can be used to reduce computation and storage cost [Ternary weight networks, trained ternary quantization].
Here, we consider two cases: uniform quanrization
Weight sharing forces several weight values to share a single value which reduces the number of unique weights in the model.
The weights are grouped using hashing function or k means algorithm and one value is assigned to each group.
A codebook is then built to map each grou of weights to its shared value. accordingly, the index to the corresponging group in the codebook is strored for each position n the filter rather than a eright value. This leads to a two step process to fetch the weights:
(a) read the group index from the weight memory and (b) using the griup index read the value of weight from the codebook or dictionary.



WHy pruning/ model compression is not the best choice.. leaks more information

\input{fig_prune}
\input{fig_pruneRetrain}
\input{fig_wtsharing}
