\section{Introduction}\label{introduction}

The tremendous performance of Machine Learning, especially Deep Learning, has resulted in their deployment to low-powered edge devices and embedded systems.
Specifically, Internet of Things (IoT) devices extensively prefer on-device processing to reduce communication latency and overhead, while also preserving the privacy of data from an untrusted data curator~\cite{8110880}.
The design of efficient Neural Networks (NNs) requires algorithm-hardware co-design such as model compression, quantization, and designing special architectures with higher efficiency~\cite{8114708}.
Such NN architecture design optimizations should conform to efficiency constraints on memory, energy, and computation overhead on embedded devices, and also maintain high prediction accuracy.
However, such designs often result in \textit{efficiency-accuracy} trade-off~\cite{rastegari2016xnornet}.

Additionally, privacy laws, such as HIPAA and GDPR, require on-device processing to maintain the privacy of user's sensitive data (e.g, medical records, location traces, and purchase preferences).
In this work we focus on Membership Inference Attack~\cite{shokri2017membership}, where given a target model and a target record, the adversary determines if the target data record was part of the target model's training data by analyzing the target model's output predictions.
For instance, wearable devices, which monitor its user's health, commonly rely on NNs for various health related predictions.
Such devices are continuously trained on the private data of a large number of users, and therefore, by mounting membership inference attacks on target device, an adversary can determine if the data of a target user was used to train NNs on the target device.
In such cases, it is crucial to design NNs resistant to inference attacks, where the adversary infers unobservable, sensitive information (e.g, user's health status) from the observable information (e.g., model predictions).
We refer to computations that achieve privacy through inference-resistance as {\em privacy-preserving computation}.
Such privacy preserving computation mechanisms affect the model's predictive accuracy resulting in \textit{privacy-accuracy} trade-off~\cite{Abadi:2016:DLD:2976749.2978318,DBLP:conf/ccs/NasrSH18,shejwalkar2019reconciling}.

Considering the trade-offs described above, the three objectives to consider while designing NNs for embedded devices are: (a) high prediction accuracy, (b) efficiency constraints on memory, energy, and computation overhead, and (c) preserving privacy of on-device data.
However, designing a model to preserve privacy while satisfying efficiency requirements without a significant cost of the model’s predictive accuracy is challenging.
In this paper, we address this challenge by proposing \method\hspace{0.02in} — a two phase training methodology for designing NNs optimized specifically for performance, accuracy and privacy.
We evaluate the privacy leakage of three state of the art hardware software co-design techniques, namely, model compression, quantization and efficient off-the shelf architectures.
We show that model compression leaks more information compared to baseline (uncompressed) models indicating a higher privacy risk to the user’s data while off-the-shelf architectures (MobileNet and SqueezeNet) do not meet all the efficiency requirements but can provide limited privacy leakage.
These observations motivate our design choice of quantizing NNs as part of \method\hspace{0.02in} training algorithm.


In Phase-I of \method, the model parameters and activations are binarized, i.e., constrained to \{-1,+1\} to reduce the memory, energy consumption, and computation overhead.
To ensure computation efficiency, we replace the expensive multiply accumulate operations between parameter matrices and activation vectors to simple and cheaper XNOR operations.
As our main contribution, we show that aggressively quantized NN architectures obtained in Phase-I ensures efficient privacy-preserving computation with higher resistance to membership inference attacks.
Phase-I of \method\hspace{0.02in} optimizes for efficiency \textit{and} privacy but at the cost of a significant drop in accuracy.
In Phase-II, we restore this accuracy by transferring knowledge from larger full precision models to the quantized models~\cite{44873}.
Here, the quantized XNOR model uses the output predictions of the full precision state of the art models as labels instead of using the true labels during training.
This significantly increases the prediction accuracy of the model while limiting privacy leakage.


Finally, we compare the models trained using \method\hspace{0.02in} with prior state of the art defences against membership inference attacks, namely, Adversarial Regularization~\cite{DBLP:conf/ccs/NasrSH18} and Differential Privacy~\cite{Abadi:2016:DLD:2976749.2978318}.
We show that our proposed models improve the trade-offs between the efficiency, accuracy, and privacy compared to the baselines approaches.
Our work provides the first systematic evaluation efficiency-accuracy-privacy trade-offs to design a novel training methodology.
The code of \method is publicly available\footnote{Anonymized for Submission}.

The paper is outlined as follows: Section~\ref{background} presents background in embedded Deep Learning and privacy threat, and Section~\ref{design} describes \method\hspace{0.02in}.
We describe the baselines and experimental setup in Section~\ref{setting}.
The evaluation of the proposed algorithm and a comparative analysis with state of the art baselines is given in Section~\ref{compare}.
Related work are then presented Section~\ref{related} before concluding in Section ~\ref{conclusions}.
