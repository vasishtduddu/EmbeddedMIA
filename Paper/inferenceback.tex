\section{Membership Inference Attacks}\label{inferenceback}

For a target machine learning model, the membership inferenceattacks aim to determine whether a given data point was used totrain the model or not [18,32,37,41,47,64]. The attack poses aserious privacy risk to the individuals whose data is used for modeltraining, for example in the setting of health analytics.Shokri et al. [47] design a membership inference attack methodbased on training an inference model to distinguish between pre-dictions on training set members versus non-members. To train theinference model, they introduce theshadow training technique: (1)the adversary first trains multiple “shadow models” which simulatethe behavior of the target model, (2) based on the shadow models’outputs on their own training and test examples, the adversaryobtains a labeled (member vs non-member) dataset, and (3) finallytrains the inference model as a neural network to perform mem-bership inference attack against the target model. The input to theinference model is the prediction vector of the target model on atarget data record.A simpler inference model, such as a linear classifier, can alsodistinguish significantly vulnerable members from non-members.Yeom et al. [64] suggest comparing the prediction confidence valueof a target example with a threshold (learned for example throughshadow training). Large confidence indicates membership. Theirresults show that such a simple confidence-thresholding method isreasonably effective and achieves membership inference accuracyclose to that of a complex neural network classifier learned fromshadow training.In this paper, we use this confidence-thresholding membershipinference approach in most cases. Note that when evaluating theprivacy leakage with targeted adversarial examples in Section 3.3.1and Section 5.2.5, the confidence-thresholding approach does notapply as there are multiple prediction vectors for each data point. In-stead, we follow Shokri et al. [47] to train a neural network classifierfor membership inference.

Passive adversary who can query the model and infer from the
no actively manipulation or tampering of the model parameters or inputs

\subsection{Adversary Assumptions}

In this work, we consider a weak black box adversary with no knowledge about the target model parameters.
This is typically the setting seen in many real world applications such as Machine Learning as a Service (MLaaS) where the users interact with the model through an API.
Within this adversarial setting, Shokri et al proposed a Membership Inference attacks based on "shadow model training".
Here, the adversary trains multiple models to mimic the behaviour of the target model and labels the membership of different output predictions.
These labeled output predictions are used to train the attack model which is a binary classifier predicting whether a given data point was IN or OUT of the dataset.
This attack relies on strong assumptions and requirements: dataset to train the shadow models is obtained from the same distribution as training data and the attacker has the capability to train large number of shadow models.
In this work, we utilize a weaker and more practical attack based on thresholding of confidence scores which does not require shadow model training and makes no assumption on the adversary's knowledge about the data distribution.
The inference accuracy is higher in case of shadow training is higher than the inference accuracy of the thresholding approach, however, this attack model is more practical and can be used by any weak adversary without additional computation overhead or knowledge assumptions.

Black-box.In this setting, the adversary’s observation is lim-ited to the output of the model on arbitrary inputs. For any datapointx, the attacker can only obtainf(x;W). The parametersof the modelWand the intermediate steps of the computationarenotaccessible to the attacker. This is the setting of machinelearning as a service platforms. Membership inference attacksagainst black-box models are already designed, which exploitthe statistical differences between a model’s predictions on itstraining set versus unseen data [6].

White-box.In  this  setting,  the  attacker  obtains  the  modelf(x;W)including  its  parameters  which  are  needed  for  pre-diction.  Thus,  for  any  inputx,  in  addition  to  its  output,  theattacker  can  compute  all  the  intermediate  computations  ofthe  model.  That  is,  the  adversary  can  compute  any  function
overWandxgiven  the  model.  The  most  straightforwardfunctions  are  the  outputs  of  the  hidden  layers,hi(x)on  theinputx.
For  a target  data  record(x,y),  the adversary  can  computethe  loss  of  the  modelL(f(x;W),y),  and  can  compute  thegradients  of  the  loss  with  respect  to  all  parametersWusing  a  simple  back-propagation  algorithm.  Given  the  largenumber of parameters used in deep neural networks (millionsof  parameters),  the  vector  with  such  a  significantly  largedimension  cannot  properly  generalize  over  the  training  data(which  in  many  cases  is  an  order  of  magnitude  smaller  insize).  Therefore,  the  distribution  of  the  model’s  gradients  onmembers  of  its  training  data,  versus  non-members,  is  likelyto  be  distinguishable.  This  can  help  the  adversary  to  runan  accurate  membership  inference  attack,  even  though  theclassification  model  (with  respect  to  its  predictions)  is  well-generalized.

Gradient vector over all parameters are cmbined to compute membership probability of the target data point using autoencoder with unsupervised learning and further use clustering algorithm to serparate members and non-members given membership probability as the feature vector.
Pass all the weights/gradients to the encoder to obtain single value representing the membership prob. Train the decoder to reconstruct original vector from the membership prob.

the adversary first obtains M(xTarget ). Then, she extracts the highest pos- terior and compares whether this maximum is above a certain threshold. If the answer is yes, then she predicts the data point is in the training set of the target model and vice versa. The reason we pick maximum as the feature follows the reasoning that an ML model is more confident, i.e., one posterior is much higher than others, when facing a data point that it was trained on. In another words, the maximal posterior of a member data point is much higher than the one of a non-member data point.

We use the fraction of correct membership predictions, as themetric to evaluate membership inference accuracy. We use a testsetDtestwhich does not overlap with the training set, to representnon-members. We sample a random data point (x,y) from eitherDtrainorDtestwith an equal50%probability, to test the membershipinference attack. We measure the membership inference accuracyas follows.Ainf(F,Bε,I)=Iz∈DtrainI(F,B,z)2·|Dtrain|+Iz∈Dtest1−I(F,Bε,z)2·|Dtest|,(15)where|·|measures the size of a dataset.

The membership inference accuracy evaluates the probabilitythat the adversary can guess correctly whether an input is fromtraining set or test set. Note that a random guessing strategy willlead to a50%inference accuracy. To further measure the effective-ness of our membership inference strategy, we also use the notionof membership inference advantage proposed by Yeom et al. [64],which is defined as the increase in inference accuracy over randomguessing (multiplied by2).ADVTinf=2×(Ainf−0.5)



Mtrics: Ainf for membership inference accuracy
