\section{Privacy Aware Pruning}

To close the gap between CNN design and energy consumption optimization, we propose an privacy-aware-pruning algorithm that directly uses the membership inference accuracy of the model to guide the pruning process.
In addition, there is currently no way for researchers to estimate the inference accuracy consumption of a NN at design time.
To this extent, at each step of the pruning process, the membership inference accuracy is monitored which helps in deciding the pruning rate, i.e, how aggresively should the parameters be pruned.

We further propose a new CNN pruning algorithm with the goal to minimize overall energy consumption with marginal accuracy degradation.
We propose a new layer-by- layer pruning method that can aggressively reduce the number of non-zero weights
Instead of considering just the accuracy as a metric, we also incorporate membership privacy as an objective to be minimized while designing an efficient model.

\input{privatePrune_flow}

\ballnumber{1}

The loss tolerance and the membership inference tolerance are used as constraints to the algorithm allowing the user to optimise for them while ensuring the accuracy and membership loss as required by an applicaations.
Some applications might be tolerant to accuracy loss while for other applications, even a small drop in accuracy results in a significant iimplication due to the critical nature of the application.
Similarly, some applications might forego the privacy aspect of training while for other applications, privacy is utmost crucial.
These adjustments and requirements for the application can be programmed within the algorithm to design an efficiently pruned and private model which satisfies the application requirements.

To sparsify the filters without impacting the accuracy, the simplest method is pruning weights with magnitudes smaller than a threshold, which is referred to as magnitude- based pruning [6â€“10]. The advantage of this approach is that it is fast, and works well when a few weights are re- moved, and thus the correlation between weights only has a minor impact on the output.

After all the layers are pruned, we fine-tune the whole network using back-propagation with the pruned weights fixed at zero. This step can be used to globally fine-tune the weights to achieve a higher accuracy. Fine-tuning the whole network is time-consuming and requires careful tuning of several hyper-parameters.
