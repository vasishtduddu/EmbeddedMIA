\documentclass[sigconf]{acmart}
%\documentclass[conference]{IEEEtran}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{dblfloatfix}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepackage{wasysym}
\usetikzlibrary{backgrounds, positioning, fit}
\usetikzlibrary{shapes.geometric}
\usepackage{amsmath}
\usetikzlibrary{patterns}
\usetikzlibrary{pgfplots.groupplots}
\newcommand{\ballnumber}[1]{\tikz[baseline=(myanchor.base)] \node[circle,fill=.,inner sep=1pt] (myanchor) {\color{-.}\bfseries\footnotesize #1};}
\newcommand{\method}{{\scshape Gecko}}
\newcommand{\cmark}{\CIRCLE}
\newcommand{\xmark}{\Circle}
\newcommand{\smark}{\LEFTcircle}


\begin{filecontents}{comparedef.txt}
z   n   pFA pFB
10  FP   98.16   86.16
10  AdvReg   87.48   83.66
10  DP   92.80   85.11
10  Gecko   90.49   83.52
20  {}  0   0
20  FP   97.86   80.34
20  AdvReg   73.97  71.02
20  DP   84.76  79.27
20  Gecko   76.79  73.5
30  {}  0   0
30  DP   99.58  88.95
30  AdvReg   89.09  85.19
30  DP   90.46  84.91
30  Gecko   93.45  85.8
\end{filecontents}

\begin{document}

\title{\method: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep Learning}

\author{Vasisht Duddu$^1$, Virat Shejwalkar$^2$, Antoine Boutet$^1$}
\affiliation{\institution{$^1$ \large Univ Lyon, INSA Lyon, Inria, CITI \\ $^2$ \large University of Massachusetts Amherst}}
\email{vduddu@tutamail.com, vshejwalkar@cs.umass.edu, antoine.boutet@insa-lyon.fr}


\begin{abstract}
Embedded systems demand on-device processing of data using Neural Networks while conforming to the memory, power and computation constraints.
In order to bring the high performing Neural Networks to edge devices, several state of the art optimizations such as model compression through \textit{pruning}, \textit{quantization}, and careful \textit{design of efficient architectures} have been extensively adopted.
These algorithms when deployed to real world sensitive applications, such as healthcare monitoring wearables, requires to resist inference attacks to protect privacy of user's training data.
We quantify this privacy leakage using membership inference attacks where the adversary aims to infer whether a given data point was a member of the model's training data or not.
In this work, we address the three-dimensional \textit{privacy-accuracy-efficiency} tradeoff in Neural Networks for embedded systems and propose \method\hspace{0.02in} training methodology where we explicitly add efficiency of private inference as a design objective of Neural Networks.
We use the inference-time memory, computation and power constraints of embedded devices as a criterion for designing Neural Network architecture while preserving membership privacy.
Given the flexibility of modifying a model during training, we choose quantization as our design choice for highly efficient and private models.
This choice is driven by the observation that compressed models leaks more information compared to baseline models while off-the-shelf efficient architecture designs indicate poor efficiency and privacy trade-off.
We show that models trained using \method\hspace{0.02in} methodology are comparable to prior state of the art defences against blackbox membership inference attacks in terms of test accuracy and privacy while additionally providing efficiency.
\end{abstract}
\keywords{Membership Privacy, Inference Attacks, Efficient Deep Learning, Embedded Computing.}

\maketitle


\input{introduction}
\input{setting}
\input{motivation}
\input{design}
\input{evaluation}
\input{related}
\input{conclusions}


%{\footnotesize
%\bibliographystyle{IEEEtranS}
%\bibliography{paper.bib}
%}

\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}



\end{document}
\endinput
