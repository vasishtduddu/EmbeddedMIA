\section{Comparative analysis}
\label{analysis}

we compare the three baseline algorithms....
In this work, we specifically select the three state of the art design techniques for efficient model computation:
\begin{itemize}
\item Model Compression via pruning redundant parameters and nodes
\item Quantization to lower the precision of model parameters and activations
\item Efficient off-the-shelf architectures
\end{itemize}

\subsection{Datasets and Architectures}


For evaluating and comparing different efficiency algorithms, we use three simple datasets: FashionMNIST, Purchase100 and Location.
These provide us with the necessary direction to choose the optimal efficiency algorithm which satisfies all the efficiency and privacy requirement as describes in Section~\ref{motivate}.

\noindent\textbf{FashionMNIST.} The FashionMNIST dataset consists of 60,000 training examples and a test set of 10,000 examples.
Each data record is a 28$\times$28 grayscale image which is mapped to one of 10 classes consisting of fashion products such as coat, sneaker, shirt, shoes.
For FashionMNIST dataset, we use a modified LeNet architecture with two convolution layers followed by maxpool and dense layers: [Conv 32 (3,3), Conv 64 (3,3), Maxpool (2,2), Dense 128, Dense 10].

\noindent\textbf{Purchase100.} The  Purchase100  dataset  is a privacy sensitive dataset capturing the purchase preferences of online customers.
The pre-processed dataset is taken from the authors of~\cite{} which has been taken from the Kaggle's "acquired valued shopper" challenge\footnote{https://kaggle.com/c/acquire-valued-shoppers-challenge/data}.
The data records have 600 binary features and each record is classified into one of 100 classes identifying each user's purchase.
For Purchase100 dataset, we use a fully connected architecture with the nodes in each layers as [1024,512,256,128,100].

\noindent\textbf{Location.} The Location dataset is a privacy sensitive dataset capturing user's location "check-ins" in Bangkok collected from the Foursquare social network\footnote{https://sites.google.com/site/yangdingqi/home/foursquare-dataset} from April 2012 to September 2013.
We use the pre-processed dataset from the authors of~\cite{} where each record has 446 binary features which is mapped to one of 30 classes each representing a different geosocial location type (e.g., Restaurant, fast food joint, etc.). We use 1,600 data records to train the model.
For Location dataset we use a fully connected architecture with hyperparameters as [512,256,128,30].

However, the actual \method\hspace{0.02in} NN design methodology is evaluated on more sophisticated datasets such as CIFAR10.
Further, the dataset has been commonly used for evaluating defences against membership inference attacks, it enables to accurately compare our work with prior state of the art defences[][][].
The optimization described as part of \method\hspace{0.02in} is for large convolutional NNs and does not cover the fully connected dense layers (detailed description in Section~\ref{}) which are used for Purchase100 and Location datasets.
It is important to evaluate on standard architectures as different custom classifiers tend to underestimate the inference leakage due to hyperparameter settings.

\noindent\textbf{CIFAR10.} The CIFAR10 dataset is a major image classification benchmarking dataset where the data records are composed of 32$\times$32 RGB images where each record is mapped to one of 10 classes of common objects such as airplane, bird, cat, dog.
For CIFAR10 dataset, we use standard state of the art architectures: Network in Network (NiN), AlexNet and VGGNet.

For Location and Purchase datasets, we train the model for 50 epochs while for FashionMNIST dataset we train the model for 75 epochs; for CIFAR10 we train the models using standard hyperparameter setting for 100-150 epochs.
We train all the model until convergence and do not specifically overfit the models.



\subsection{Membership Inference Attacks}


On querying a trained model $f(x;\theta)$, the adversary learns aggregate information about the entire data population which the model generalizes from the train dataset to an unseen test data.
This is desirable and quantified using the accuracy of the model.
On the other hand, if the adversary learns something specific about a user's data record used in the training dataset, we refer to such information as privacy leakage.
In other words, in context of machine learning, there is a privacy breach if the adversary learns unobservable information specific to an individual user's data record from observable information such as model's output predictions.
This inferred unobservable information about a user's record can be, for instance, the membership details of the record in the training set of the model, referred to as membership inference attacks.
Alternatively, an adversary can learn sensitive attributes about the user's data record which can be used to reconstruct the sensitive training dataset.
In this work, we specifically use membership inference attacks to quantify information leakage in machine learning models.

Machine learning models are more confident while predicting the class of already seen train data record compared to an unseen test data record.
Membership inference attacks exploits this difference in the model's confidence to classify a new data record as being a "Member" or "Non-Member" of the model's training data.
This is a binary decisional problem where the adversary classifies the membership of a given input $x$ using the model's output prediction $f(x;\theta)$.
We describe the threat model and attack details below.

\noindent\textbf{Adversary Knowledge.} We assume a blackbox setting where the adversary is assumed to have no knowledge about the target model.
Formally, given a target model $f()$ which maps an input data record $x$ to its correct label $y$, the adversary only sees the final model prediction $f(x;\theta)$.
The adversary does not know the architecture of $f()$ and the model parameters $\theta$.
This is a practical setting seen typically in Machine Learning as a Service (MLaaS) where the adversary submits an input query to the trained model on the Cloud via an API and receives the corresponding output predictions.

\noindent\textbf{Adversary Goal.} The goal of adversary in membership inference attack is to infer whether a given data record was used in the model's training data or not.
Formally, given a user's data record $x$ $\sim$ $P(X,Y)$, where $P(X,Y)$ is the data distribution from which the training data $D_{train}$ was sampled, the adversary estimates $P(x \in D_{train})$ using the model's prediction $f(x;W)$.
Empirically, the adversary identifies a threshold to estimate whether $x \in D_{train}$ which can also be learnt using a binary classifier.

\noindent\textbf{Attack Methodology.} In this work, we use the confidence score attack where the adversary leverages the prediction entropy of the target model $f(x;W)$ to perform the membership inference attack [][].
In particular, the adversary obtains $f(x;W)$ and finds the maximum posterior and infers $x \in D_{train}$ if the maximum is greater than a threshold.
The attack is based on the observation that the maximal posterior of a member data record is higher (more confident) than a non-member data record of the training dataset.



\subsection{Metrics}

We use the inference attack accuracy to estimate the success of membership inference attack.
An accuracy above random guess $50\%$ indicates a training data leakage through membership inference attack.
This indicates that the adversary is able to identify the membership details of a data record with an accuracy higher than random guess.
The success of inference attack accuracy is strongly correlated with the model's extent of overfitting empirically measured as the difference between the train and test accuracy.
Additionally, the accuracy of the model is computed using the model's performance on unseen test data.


%\subsection{Optimization for Efficiency}
\subsection{Evaluating Efficiency}


Here, in the view of the above efficiency requirements, we compare the three baseline algorithms.
We then select an efficient design scheme for NNs that satisfies all our requirements.


\noindent\textbf{Memory Efficiency.} Off the shelf models are designed to specifically reduce the memory footprint.
For instance, the memory footprint of Squeezenet and MobileNet is 5MB and 14Mb compared to 250Mb of Alexnet and >500Mb of VGG architectures.
Additionally, lowering the model precision from 64 or 32 bit floating point to binary precision results in a direct reduction of 64x or 32x in the overall memory footprint of the model.
However, in case of model compression the model parameters which are pruned are simply replaced by a value of "0".
Hence, storing even the "0" parameter takes up memory and does not necessarily decrease the overall memory footprint unless the hardware is optimized to skip the storage of all the zero values in the memory.
This requires additional logic to check for zero valued parameters in a dictionary.

\input{fig_prune}
\input{fig_pruneRetrain}
\input{fig_wtsharing}

\noindent\textbf{Computation Efficiency.} Design of efficient off-the-shelf architectures replaces the complex matrix-vector multiplications by multiple matrix-vector multiplications with smaller dimensions.
This reduces the overall number of parameters but it has been shown empirically\footnote{https://github.com/albanie/convnet-burden} that this does not necessarily reduce the number of multiply accumulate operations or FLOPS~\cite{article}.
In case of parameter pruning, achieving efficiency requires additional hardware optimization. Particularly, instead of actually computing the the multiplications with "0"pruned values, the hardware optimization enable the user to skip the computation and replace the output by a "0" directly.
For quantized models with binarized parameters and activations the MAC operations can be replaced by binary operations such as XNOR and the maxpool operations can be replaced by OR operation, while the activations can be replaced by checking the parity bit (denotes the sign) operation and hence reducing the FLOPS drastically[XONN].
This results in high computational efficiency and hence, faster inference.


\noindent\textbf{Energy Efficiency.} Energy efficiency has been shown to not vary much in case of reduction with number of parameters and data type, number of memory accesses play vital role[CVPR 2017 yang et al][].
Specifically, for the case of off-the-shelf architectures the while the computation efficiency has been shown to improve, the energy efficiency has been shown to be close to large scale state of the art models like AlexNet[suqeezenet][].
Alternatively, for the case of model compression, energy efficiency can be achieved by additionally providing hardware optimization and shows small improvement in the energy consumption[].
For quantization, however, the energy efficiency has been shown to be high [][] where the memory access can be drastically reduced by increasing the throughput of data fetched from the memory.
Specifically, lowering the precision from 32 bit floating point to binary results in lowering the memory accesses and 32x improvement in energy consumption[][][].
While some improvement is seen natively for quantized models (from replacing MACs with XNOR), higher benefits can be achieved via additional hardware optimization[].
The benchmarking of energy consumption for different optimization and architectures is well explored in the literature and out of scope of this work. We refer the authors to [][][] for more details.

In summary, compared to different optimization techniques for NNs, the quantized architectures show significant benefit for different efficiency requirements over the other alternatives.




\subsection{Evaluating Privacy Leakage}

In this section, we evaluate the information leakage through membership inference attacks for the three baseline algorithms considered.
This is the main contribution of our work where we evaluate the privacy leakage for different optimization and design algorithms for NNs.

\subsubsection{Model Compression}

We evaluate the privacy leakage on compressing a model by pruning the connections in the model.
Here, pruning is achieved by replacing some of the parameters with "0" value.
As described in the original paper~\cite{}, pruning is followed by retraining the model to restore the model's original accuracy with the pruned connections.

We evaluate and validate the impact on membership privacy on compressing the model trained on three datasets: FashionMNIST, Location and Purchase100.
On pruning the model, the model's test accuracy decreases but also lowers the membership inference accuracy (Figure~\ref{fig:prune}).
This is expected as the parameters are responsible for memorizing the training data information and pruning the parameters lowers the adversary's attack success~\cite{rezawhite}.

However, interestingly, on retraining the pruned model, we observe that the membership inference accuracy is much higher than the original unpruned baseline model (Figure~\ref{fig:retrain}).
This indicates that model compression in turn increases the overall privacy leakage.
This can be attributed to the lower number of parameters forced to learn the same amount of information stored previously in the unpruned model with larger number of parameters.
In other words, the same amount of information is now captured by less number of parameters resulting in higher memorization of information per parameter.
To analyze the information stored per parameter, we first compute the model capacity as the mutual information of a trained network between the true label $Y$ and the predicted label $Y_{\theta}$ for a random input $X$ as derived in~\cite{45932,cap}.
Here, model's information $I(Y;Y_{\theta}|X) = $
\begin{equation}
\footnotesize
N_{train}\left(1 - (r_{train}log_2(\frac{1}{r_{train}}) + (1-r_{train})log_2(\frac{1}{1-r_{train}}))\right)
\end{equation}
where $r_{train}$ is the classification train accuracy for all the $N_{train}$ samples in the training data.
For $r_{train} = 1$, the model completely memorizes all random samples as the information stored equals the number of samples $N_{train}$, while for $r_{train}=0.5$, the training accuracy 0.
We divide the above equation by the model's total number of parameters $N_{param}$ to get the per parameter information stored as
\begin{equation}
I_p(Y;Y_{\theta}|X) = \frac{I(Y;Y_{\theta}|X)}{N_{param}}
\end{equation}
As the model is compressed (pruned), the number of parameters $N_{param}$ decreases which results in increase in $I_p(Y;Y_{\theta}|X)$. However, on aggressive pruning, the train accuracy $r_{train}$ also decreases resulting in a decrease in the information per parameter, which is empirically indicated by a decrease in membership inference accuracy at the end in Figure~\ref{fig:retrain}.


\textbf{Mitigating the Privacy Risks in Pruned Models.} We describe on potential approach to mitigate the privacy risk of the compressed models without requiring to modify the model's training.
The post-hoc apporach utilizes the weight sharing for the compressed model. This is however, accompanied by a decrease in the model's prediction accuracy indicating a privacy-utility trade-off.
As seen in Figure~\ref{fig:wtsharing}, reducing the precision from 32 bits to 2 bits results in a decrease in inference accuracy from 56.57\% to 52.64\% for FashionMNIST, 62.08\% to 53.38\% for Location and 81.82\% to 61.95\% for Purchase100 dataset.
This decrease in inference attack accuracy is closely followed by a decrease in generalization error which is indicative of decrease in prediction accuracy of the model.
We evaluate the effectiveness of pruning followed by quantization which has been shown to have significant impact on reducing the model complexity through compression more significantly than either pruning or quantization alone.
For the experiments, we use the compressed model indicating highest privacy leakage to evaluate the effectiveness of weight sharing on the worst case condition.
This pipelined approach of pruning followed by retraining followed by weight sharing, not only maintains the algorithm's objective for efficiency but is used as a post-hoc approach to reduces the overall inference risk~\cite{DBLP:journals/corr/HanMD15,DBLP:journals/corr/HanPNMTECTD16}.






\subsubsection{Off-the-Shelf Efficient Architectures}


\begin{table}[!htb]
\begin{center}
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Architecture} & \textbf{Memory} & \textbf{Train}  & \textbf{Test}  & \textbf{Inference}   \\
 & \textbf{Footprint} & \textbf{Accuracy} & \textbf{Accuracy} & \textbf{Accuracy}  \\
\hline
SqueezeNet & 5 MB & 88.21\% & 81.92\% & \cellcolor{green!25}53.07\% \\
MobileNetV2 & 14 MB & 97.50\% & 87.24\% & \cellcolor{green!25}55.57\% \\
\hline
AlexNet & 240 MB & 97.86\% & 80.34\% & \cellcolor{red!25}60.40\% \\
VGG11 & 507 MB & 99.13\% & 86.43\% & \cellcolor{red!25}58.04\% \\
VGG16 & 528 MB & 99.58\% & 88.95\% & \cellcolor{red!25}58.70\%  \\
VGG19 & 549 MB & 99.09\% & 88.18\% & \cellcolor{red!25}57.85\% \\
\hline
\end{tabular}
\end{center}
\caption{Model complexity influences the membership inference leakage. Model specifically designed for efficiency leak less information.}
\label{stdarch}
\end{table}

In this section, we evaluate two popular state of the art architectures, SqueezeNet and MobileNet, trained on CIFAR10 dataset used for low powered systems.
As seen in Table~\ref{stdarch}, the SqueezeNet and MobileNet models shows lower inference accuracy of 53.07\% and 55.57\% compared to larger models which have higher privacy leakage.

\input{fig_efficientArch}

Further, the inference accuracy of SqueezeNet and MobileNet can be further reduced close to random guess by increasing the temperature parameter of the softmax function applied to the output.
Increasing the temperature parameter reduces the granularity of the model's output and is given by $F_i(x) = \frac{e^{\frac{z_i(x)}{T}}}{\sum_{j}e^{\frac{z_j(x)}{T}}}$ where $z(x)$ computes output of the model before the softmax layer.
For the case of SqueezeNet, we are able to reduce the inference accuracy to 50.93\% from 53.07\% while for MobileNet we can reduce the inference accuracy to 52.62\% from 55.57\% as seen in Figure~\ref{softmax}.
This reduction in inference accuracy is without any cost of the prediction test accuracy of the model.




\subsubsection{Quantization}

In this section, we evaluate the technique of reducing the precision of both model's parameters and intermediate activations.
Further, we consider the extreme case of binarizing the parameters and activations allowing to evaluate on the most optimized case.
We evaluate on FashionMNIST dataset for two architectures with convolutional and fully connected layers.

\begin{table}[!htb]
\begin{center}
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{5}{|c|}{\textbf{FashionMNIST}}\\
\hline
\textbf{Architecture} & \textbf{Memory} & \textbf{Train}  & \textbf{Test}  & \textbf{Inference}  \\
 & \textbf{Accuracy} &  \textbf{Footprint} & \textbf{Accuracy} & \textbf{Accuracy}  \\
\hline
\multicolumn{5}{|c|}{Architecture 1}\\
Full & 38.39 MB & 100\% & 92.35\% & \cellcolor{red!25}57.46\%\\
BinaryNet & 1.62 MB & 88.68\% & 86.9\% & \cellcolor{green!25}55.45\%\\
XNOR-Net & 1.62 MB & 87.19\% & 85.68\% & \cellcolor{green!25}51.05\%\\ %1,626,824 parameters
\hline
\multicolumn{5}{|c|}{Architecture 2}\\
Full & 29.83 MB & 99.34\% & 89.88\% & \cellcolor{red!25}54.86\% \\
BinaryNet & 0.93 MB & 97.61\% & 89.60\% & \cellcolor{green!25}54.30\%\\
XNOR-Net & 0.93 MB & 92.67\% & 86.68\% & \cellcolor{green!25}51.74\%\\ %937,000parameters
\hline
\end{tabular}
\end{center}
\caption{Reducing the model precision lowers the inference attack accuracy but at the cost of test accuracy.}
\label{fmnist_quantize}
\end{table}

In both the architectures, we see that computation on  binarized parameters and activations reduces the inference risk by a small value.
However, on replacing the MAC operations with XNOR operations, we observe that the inference risk decreases close to random guess, however, at the cost of prediction test accuracy.

In summary, we observe that quantization, specifically binarization of parameters and activation along with XNOR computation, provides strong resistance against inference attacks compared to model compression and off-the-shelf architectures.

\subsection{Summary of Comparison}

We summarize the properties satisfied by each of the technique in terms of privacy, computation, memory and energy efficiency in Table~\ref{tbl:comparison}.
Here, we mark the attributes which are satisfied with $\cmark$, requires additional hardware optimization as $\smark$ and does not satisfy the property with a $\xmark$.

\begin{table}[!htb]
\begin{center}
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\begin{tabular}{|l||l|l|l|}
\hline
Requirements & Compression & Quantization & Off-the-shelf  \\
\hline
Computation Efficiency & $\smark$  & $\cmark$   & $\xmark$ \\
\hline
Memory Efficiency &  $\smark$ & $\cmark$   & $\cmark$ \\
\hline
Energy Efficiency &  $\smark$   & $\cmark$   & $\xmark$ \\
\hline
Privacy &  $\xmark$   & $\cmark$   & $\smark$ \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of different optimizations for NNs. $\smark$: additional hardware optimization; $\xmark$: requirement not satisfied; $\cmark$: requirement satisfied.}
\label{tbl:comparison}
\end{table}

For our requirement, quantization of NNs is an attractive design choice which not only satisfies the computation, memory and energy efficiency but also provides high resistance against inference attacks.
Specifically, in this work we only consider the aggressive quantization of binarizing the parameters and activations to \{-1,+1\} values while additionally replacing the MAC operations with cheap and efficient binary arithmetic (XNOR operations).
Hence, we choose this particular design for NNs to provide a good three dimensional trade-off between privacy-efficiency-accuracy.




