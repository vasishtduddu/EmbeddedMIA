\section{Comparison with Prior Defenses}\label{compare}

Mitigating membership inference attacks has been a major research challenge requiring immediate attention.
The major reason for privacy leakage through membership inference attacks is due to overfitting in Machine Learning models.
The model performance is distinct for training data which the model has encountered previously and the unseen test data.
This difference in training and testing performance enables the adversary to train a classifier to classify a new data point into training or testing data.


\textbf{Explicit Regularization.} A straight-forward approach to mitigate membership inference attacks is to generalise the model by explicitly adding an additional regularization term.
These include Dropout, L2 penalty, model stacking (ensemble) and Knowledge Distillation.
Alternatively, a specific adversarial regularization approach that regularizes the model according to the noise
All these techniques rely on enhancing the generalisation of the model by modifying the objective function during training to reduce the difference between training and testing data.
Further, this additional regularization in the objective function results in a trade-off between utility and privacy controlled using the regularization hyperparameter.


\textbf{Provable Defenses.} Differential Privacy is a provable defense which provides a bound on the total leakage of the training data for individuals in the training data.
Particularly, Differential privacy adds noise to the gradients during training. This noise is sampled from either Gaussian and Laplacian distribution proportional to the model's sensitivity.
The current state of Differential Privacy, however, results in a privacy-utility trade-off  since adding noise to the gradients degrades the performance.
PATE framework provides a way to train model privately in a framework based on teacher-student models similar to Knowledge Distillation.
Alternatively, adding carefully crafted perturbation to the confidence scores to act as an adversarial example for the attack classifier model has also been proposed.
This approach is 


However, none of the above proposed defenses take into account the aspect of efficiency in the model design.
Such model how it impacts the deployment of models for real world applications such as wearable devices.
The proposed \textit{quantization with distillation} and \textit{privacy aware pruning} in this work, \textbf{guarantee model efficiency while mitigating privacy risks}.
We provide a comparison between the proposed defenses with the prior defenses in terms of utility and privacy loss.
