\section{Related Work}

\textbf{Attacking Efficient Deep Learning Models.} Security attacks have been proposed \cite{khalil2018combinatorial}
Defense against adversarial attacks while maintaining adversarial robustness \cite{lin2018defensive}
Further, low precision NNs have been shown to be more robust compared to full precision networks \cite{8723317}\cite{galloway2018attacking}



\textbf{Privacy Preserving Deep Learning.} Recent work addresses combining XNOR based BNNs to improve private computation with garbled circuits \cite{235489} \cite{8824120}.
The goal in this line of work is to improve the communication and computation cost for privacy preserving inference in a two-party setting for Deep Neural Networks.
This includes computation on encrypted data using Homomorphic encryption and combining it with descritized NNs to improve the performance \cite{Bourse2017FastHE}.
Hybrid approaches combining both Secure MPC (Garbled Circuits) and Homomorphic Encryption \cite{217515}
Trusted execution environments..
using algorithmic optimisation such as Fourier Transform to improve performance \cite{Li2018FALCONAF}


Multiple research papers have studied membership inferenceattacks  in  a  black-box  setting  [6],  [16],  [7].  Homer  et  al.  [4]performed  one  of  the  first  membership  inference  attacks  ongenomic  data.  Shokri  et  al.  [6]  showed  that  an  ML  model’soutput  has  distinguishable  properties  about  its  training  data,which could be exploited by the adversary’s inference model.They  introduced  shadow  models  that  mimic  the  behavior  ofthe  target  model,  which  are  used  by  the  attacker  to  trainthe  attack  model.  Salem  et  al.  [17]  extended  the  attacks  ofShokri  et  al.  [6]  and  showed  empirically  that  it  is  possibleto  use  a  single  shadow  model  (instead  of  several  shadowmodels used in   [6]) to perform the same attack. They furtherdemonstrated  that  even  if  the  attacker  does  not  have  accessto  the  target  model’s  training  data,  she  can  use  statisticalproperties of outputs (e.g., entropy) to perform membership in-ference. Yeom et al. [7] demonstrated the relationship betweenoverfitting and membership inference attacks. Hayes et al. [18]used  generative  adversarial  networks  to  perform  membershipattacks on generative models.Melis  et  al.  [19]  developed  a  new  set  of  membershipinference  attacks  for  the  collaborative  learning.  The  attackassumes  that  the  participants  update  the  central  server  aftereach  mini-batch,  as  opposed  to  updating  after  each  trainingepoch  [20],  [21].  Also,  the  proposed  membership  inferenceattack  is  designed  exclusively  for  models  that  use  explicitword embeddings (which reveal the set of words used in thetraining  sentences  in  a  mini-batch)  with  very  small  trainingmini-batches.In  this  paper,  we  evaluate  standard  learning  mechanismsfor  deep  learning  and  standard  target  models  for  variousarchitectures.  We  showed  that  our  attacks  work  even  if  weuse pre-trained, state-of-the-art target models.Differential  privacy  [22],  [23]  has  been  used  as  a  strongdefense mechanism against inference attacks in the context ofmachine  learning  [24],  [25],  [26],  [27].  Several  works  [28],[29], [30] have shown that by using adversarial training, onecan find a better trade-off between privacy and model accuracy.However, the focus of this line of work is on the membershipinference attack in the black-box setting

An  attacker  with  additional  information  about  the  trainingdata  distribution  can  perform  various  types  of  inference  at-tacks. Input inference [31], attribute inference [32], parameterinference [33], [34], and side-channel attacks [35] are severalexamples  of  such  attacks.  Ateniese  et  al.  [36]  show  that  anadversary  with  access  to  the  parameters  of  machine  learningmodels  such  as  Support  Vector  Machines  (SVM)  or  HiddenMarkov Models (HMM) [37] can extract valuable informationabout  the  training  data  (e.g.,  the  accent  of  the  speakers  inspeech recognition models).
