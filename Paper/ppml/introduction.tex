\section{Introduction}

The tremendous performance of Machine Learning, especially Deep Learning, has resulted in their deployment to low-powered edge devices and embedded systems.
Specifically, Internet of Things (IoT) devices extensively prefer on-device processing to reduce communication latency and overhead, while also preserving the privacy of data from an untrusted data curator~\cite{8110880}.
The design of efficient Neural Networks (NNs) requires algorithm-hardware co-design such as model compression, quantization, and designing special architectures with higher efficiency~\cite{8114708}.
However, such designs often result in \textit{efficiency-accuracy} trade-off~\cite{rastegari2016xnornet}.
Additionally, privacy laws, such as HIPAA and GDPR, require on-device processing to maintain the privacy of user's sensitive data (e.g, medical records, location traces, and purchase preferences).
In this work we focus on Membership Inference Attack~\cite{shokri2017membership}, where given a target model and a target record, the adversary determines if the target data record was part of the target model's training data by analyzing the target model's output predictions.
Such privacy preserving computation mechanisms affect the model's predictive accuracy resulting in \textit{privacy-accuracy} trade-off~\cite{Abadi:2016:DLD:2976749.2978318,DBLP:conf/ccs/NasrSH18}.

Considering the trade-offs described above, the three objectives to consider while designing NNs for embedded devices are: (a) high prediction accuracy, (b) efficiency constraints on memory, energy, and computation overhead, and (c) preserving privacy of on-device data.
However, designing a model to preserve privacy while satisfying efficiency requirements without a significant cost of the model’s predictive accuracy is challenging.
We evaluate the privacy leakage of three state of the art hardware software co-design techniques, i.e., namely, model compression, quantization and efficient off-the shelf architectures.
We show that model compression (i.e., pruning the network) leaks more information compared to baseline (uncompressed) models indicating a higher privacy risk to the user’s data while off-the-shelf architectures (MobileNet and SqueezeNet) do not meet all the efficiency requirements but can provide limited privacy leakage.
These observations motivate our design choice of quantizing NNs (i.e., reducing the number of bits that represent a number) as potential training algorithm to train models inherently private against membership inference attacks.
Our work provides the first systematic evaluation of efficiency-accuracy-privacy trade-offs to design a novel training methodology.
