\section{Membership Inference Attacks}\label{inferenceback}

Machine Learning models are trained dataset aggregated from user's sensitive and personal information such as location history, purchase preferences, medical records and financial data.
On querying a trained model, the adversary learns aggregate information about the entire data population which the model generalizes using the train dataset to an unseen test data. This is desirable and is quantified using the accuracy of the model.
On the other hand, if the adversary learns something specific about a user's data record used in the training dataset, we refer to such information as privacy leakage.
Alternatively, we define a privacy breach if the adversary learns unobservable information specific to an individual user's data record from observable information  such as model's output predictions.
This inferred unobservable information about the users record can be, for instance, the membership details of the record in the training set of the model, referred to as membership inference attacks.
We use this specific attack to quantify information leakage in machine learning models in this work. Further, resistance against such inference attacks is desirable in a machine learning model trained on sensitive information.
Alternatively, an adversary can learn sensitive attributes about the user's data record which can be used to reconstruct the sensitive training dataset. However, such attribute inference and reconstruction attacks are not the focus of this paper.

\subsection{Datasets}

For evaluating the pruning algorithm and estimating the privacy leakage we use three basic datasets: FashionMNIST, Purchase100 and Location.
We evaluate the model compression algorithms for the architectures trained on these datasets.

\noindent\textbf{FashionMNIST.} The FashionMNIST dataset consists of 60,000 training examples and a test set of 10,000 examples.
Each data record is a 28$\times$28 grayscale image which is mapped to one of 10 classes consisting of fashion products such as coat, sneaker, shirt, shoes.

\noindent\textbf{Purchase100.} The  Purchase100  dataset  is a privacy sensitive dataset capturing the purchase preferences of online customers.
The pre-processed dataset is taken from the authors of~\cite{} which has been taken from the Kaggle's "acquired valued shopper" challenge\footnote{https://kaggle.com/c/acquire-valued-shoppers-challenge/data}.
The data records have 600 binary features and each record is classified into one of 100 classes identifying each user's purchase.

\noindent\textbf{Location.} The Location dataset is a privacy sensitive dataset capturing user's location "check-ins" in Bangkok collected from the Foursquare social network\footnote{https://sites.google.com/site/yangdingqi/home/foursquare-dataset} from April 2012 to September 2013.
We use the pre-processed dataset from the authors of~\cite{} where each record has 446 binary features which is mapped to one of 30 classes each representing a different geosocial location type (e.g., Restaurant, fast food joint, etc.). We use 1,600 data records to train the model.

However, the above datasets are simple compared to the more sophisticated datasets such as CIFAR10.
A large number of state of the art neural networks are benchmarked on this dataset and hence, we use this for evaluating our proposed framework \method\hspace{0.02in}.
Further, the use of this dataset further allows to compare the models with prior defences against membership inference attacks.

\noindent\textbf{CIFAR10.} The CIFAR10 dataset is a major image classification benchmarking dataset where the data records are composed of 32$\times$32 RGB images where each record is mapped to one of 10 classes of common objects such as airplane, bird, cat, dog.


\subsection{Architecture and Methodology}

For the three datasets: FashionMNIST, Location and Purchase100, we use custom architectures which are described in Table~\ref{tab:architectures}.
For Location and Purchase datasets, we train the model for 50 epochs while for FashionMNIST dataset we train the model for 75 epochs.
For CIFAR10 dataset, we use state of the art architectures such as Network in Network (NiN), AlexNet and VGGNet which are trained for 100-150 epochs.
We train the model till convergence where the loss does not decrease significantly and is stable.

\begin{table}[!htb]
\caption{Custom architectures used for training FashionMNIST, Location and Purchase100 datasets.}
\centering
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Purchase100} & \textbf{Location} & \textbf{FMNIST CNN}\\
\hline
Dense 1024 & Dense 512 & Convolution 32 (3,3)\\
Dense 512  & Dense 256 & Convolution 64 (3,3)\\
Dense 256  & Dense 128 & MaxPool (2, 2)\\
Dense 128  & Dense 30  & Dense 128 \\
Dense 100  &           & Dense 10\\
           &           & \textbf{FMNIST MLP}\\
           &           & Dense 512\\
           &           & Dense 512\\
           &           & Dense 512\\
\hline
\end{tabular}
}
\label{tab:architectures}
\end{table}


\subsection{Threat Model}

\noindent\textbf{Adversary Knowledge.} We assume a blackbox setting where the adversary is assumed to have no knowledge about the target model.
Formally, given a target model $f()$ which maps an input data record $x$ to its correct label $y$.
The adversary only sees the final model prediction $f(x;W)$ and the adversary does not know the architecture of $f()$ and the model parameters $W$.
This is a practical setting seen typically in Machine Learning as a Service (MLaaS) where the adversary submits an input query to the trained model on the Cloud via an API and receives the corresponding output predictions.

\noindent\textbf{Adversary Goal.} The adversary in membership inference attack aims to infer whether a given data point was used as part of the model's training data or not.
Formally, given a user's data record $x$ $\sim$ $P(X,Y)$ where $P(X,Y)$ is the underlying and $D_{train}$ is the training dataset of the target model, the adversary uses the model's prediction $f(x;W)$ to estimate $P(x \in D_{train})$.
The adversary uses a threshold to estimate whether $x \in D_{train}$. This is a decisional problem which can be modelled as a binary classifier.

\noindent\textbf{Attack Methodology.} We leverage the prediction entropy of the target model $f(x;W)$ to perform the membership inference attack.
In particular, the adversary obtains $f(x;W)$ and finds the maximum posterior and infers $x \in D_{train}$ if the maximum is greater than a threshold (=0.5 in this work).
The idea is that the maximal posterior of a member data record of $D_{train}$ (already seen during training) is much higher (more confident) than a non-member data point.

\noindent\textbf{Metrics.} We use the inference attack accuracy as the estimate for the success of membership inference attack.
An accuracy above random guess $50\%$ indicates a training data leakage through membership inference attack.
This indicates that the adversary is able to identify the membership details of a data record with an accuracy higher than random guess.
Further, we quantify the adversary's advantage $A_{adv}$ beyond random guess as $A_{adv} = 2 * (A_{inf}-0.5)$ where $A_{inf}$ is the inference accuracy.
Particularly, every 1\% increase in inference accuracy indicates a $10\%$ membership attack advantage to the adversary.
