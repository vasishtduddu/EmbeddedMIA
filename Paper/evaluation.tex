\section{Evaluation}\label{evaluation}

Difficult to train binarized model~\cite{AAAI1714619}

\begin{table}[!htb]
\begin{center}
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{5}{|c|}{\textbf{CIFAR10}} \\
\hline
\multicolumn{2}{|c|}{\textbf{Architecture}} & \textbf{Train}  & \textbf{Test}  & \textbf{Inference}  \\
 \multicolumn{2}{|c|}{} & \textbf{Accuracy} & \textbf{Accuracy} & \textbf{Accuracy}  \\
\hline
\multirow{2}{*}{NiN} & Full Precision & 98.16\% & 86.16\% & \cellcolor{red!25}56.69\% \\
& Binary Precision & 81.93\% & 78.74\% & \cellcolor{green!25}51.76\% \\
\hline
\multirow{2}{*}{AlexNet} & Full Precision & 97.86\% & 80.34\% & \cellcolor{red!25}60.40\% \\
& Binary Precision & 68.62\% & 66.8\% & \cellcolor{green!25}51.40\% \\
\hline
\multirow{2}{*}{VGG16} & Full Precision & 99.58\% & 88.95\% & \cellcolor{red!25}58.70\%\\
& Binary Precision & 79.67\% & 74.64\% & \cellcolor{green!25}52.65\%\\
\hline
\end{tabular}
\end{center}
\caption{Reducing the precision of models lowers the membership privacy leakage through membership inference attacks but at the cost of accuracy}
\label{fmnist_quantize}
\end{table}


\input{plot_defence}


\begin{table}[!htb]
\begin{center}
\renewcommand\arraystretch{1.5}
\fontsize{6.7pt}{6.7pt}\selectfont
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Teacher} & \textbf{Student} & \textbf{Train}  & \textbf{Test}  & \textbf{Inference}  \\
&  & \textbf{Accuracy} & \textbf{Accuracy} & \textbf{Accuracy}  \\
\hline
Binary NiN & None & 81.93\% & 78.74\% & 51.76\% \\
Binary AlexNet & None & 68.62\% & 66.8\% & 51.40\% \\
Binary VGG16 & None & 79.67\% & 74.64\% & 52.65\%\\
\hline
NiN & Binary NiN & 90.49\% & 83.52\% & 53.90\% \\
AlexNet & Binary AlexNet & 76.79\% & 73.5\% & 51.85\% \\
VGG16 & Binary VGG16 & 89.45\% & 81.58\% & 54.98\%\\
\hline
DenseNet169 & NiN & 92.84\% & 83.71\% & 54.95\%\\
DenseNet169 & AlexNet & 81.87\% & 76.23\% & 53.51\%\\
DenseNet169 & VGG16 & 93.45\% & 85.8\% & 54.17\%\\
\hline
ResNet50 & NiN & 91.74\% & 83.77\% & 54.53\% \\
ResNet50 & AlexNet & 80.12\% & 74.92\% & 53.12\%\\
ResNet50 & VGG16 & 94.23\% & 86.52\% & 54.46\%\\
\hline
\end{tabular}
\end{center}
\caption{CIFAR10 Cross Architecture. Heterogeneous}
\label{kd}
\end{table}


\subsection{Comparison with prior Defenses}

\subsubsection{Baselines}

Several works have explored approaches to defend against membership inference attacks.
These defences have mainly focuessed on improving the model's generalization and reduce overfitting which has been considered as the main cause for leakage through membership inference attacks.


\noindent\textbf{Adversarial Regularization (AdvReg).}~\cite{DBLP:conf/ccs/NasrSH18}


\noindent\textbf{Differential Privacy (DP).}~\cite{Abadi:2016:DLD:2976749.2978318}

\input{fig_comparedef}

%plot gen error cdf


\subsubsection{Comparison with MemGuard}

The defences proposed so far can be categorized into (a) regularization based train-time defences and (b) post-training inference time defence.
Adversarial Regularization, Differential Privacy and other standard regularization techniques such as L2 and Dropout modify the training of the neural network.
Our proposed training framework exploits is part of category (a) where we modify the training of the machine learning model in order to provide acceptable levels of privacy and accuracy.
MemGuard~\cite{10.1145/3319535.3363201}, on the other hand, is a post-training defence, where the defender adds carefully crafted noise to the target model's output observations to ensure the misclassification of the adversary's attack classifier network.
The defence is based on the idea that the adversary's attack model is a machine learning classifier which is vulnerable to change in output with a carefully added noise to input (referred to as adversarial examples).
However, this post-training approach can be used in addition to the models trained using the \method\hspace{0.02in} framework.
Further, the attack that we use does not rely on an attack classification network but rather relies on output posterior to perform the attack.
Hence, within the threat model considered, the defence is not valid and we only compare our work with the state of the art defences that modify the training algorithm namely, Adversarial Regularization and Differential Privacy.
