\section{Background}
\label{background}

\subsection{Embedded Deep Learning}

On-device processing is an attractive alternative compared to centralized processing of data from IoT devices.
Such on-device processing reduces the overhead of communicating data from the devices to the servers, lowers the privacy and security risk associated with storing sensitive data on untrusted central server and lowers the latency for obtaining results from processing~\cite{8110880}.

Different efficiency requirements can be adopted to design NNs for embedded systems.
These differences make it difficult to decide which primitive is the best fit for designing a privacy-preserving system for a particular application.
The list below presents the most important efficiency requirements accounted by designers of embedded systems using NNs, and privacy is not part of this list:

\begin{itemize}[leftmargin=*]
\item {\em Energy Efficiency.} Energy consumption is a vital constraint for low powered embedded or IoT devices which operate for long duration while maximising their battery lifetime.
While executing NNs, every Multiply Accumulate (MAC, a common step that computes the product of two numbers and adds that product to a register in which intermediate results are stored) requires memory access for reading weights, inputs and intermediate output from previous layer and one write to store the computed output. These read-write operations consume significantly higher energy than actually performing the MAC operation in the CPU.
Energy efficiency is achieved by reducing the memory access by (a) optimizing hardware to exploit sparsity in MACs and (b) reducing the precision to increase the throughput of data.

\item {\em Computation Efficiency.} The total MAC operations between the parameter matrix and input activation function quantifies the requirement of computation efficiency.
The processing rate of MAC operations is constrained by the CPU on embedded device which is reduced by reducing the total number of parameters.
Additionally, replacing MACs with cheaper binary arithmetic significantly lowers the computational overhead.

\item {\em Memory Efficiency.} The total size of the model measured in terms of the memory storage for model parameters and additional runtime storage for intermediate outputs should be within the memory constraints of the embedded device.
This is achieved in two ways: (a) reducing the precision of the parameters and intermediate outputs and (b) pruning the parameters by increasing sparsity.
\end{itemize}


\subsection{Privacy Threat: Membership Attacks}
\label{threatmodel}

% section to reduce
NNs for embedded systems do not account for privacy threats, however these threats still exist if personal and potentially sensitive data feed the system.
%Although commonly not taken into account for the design of NNs for embedded systems, privacy threats still exist if personal and potentially sensitive data feed the system.
%Indeed, an adversary learns aggregate information about the entire data population through the model which generalizes this information from the train dataset to an unseen test data.
%This is desirable and quantified using the accuracy of the model.
%On the other hand, 
For instance, if the adversary learns something specific about a user's data record used in the training dataset, we refer to such information as privacy leakage.
%In other words, in context of machine learning, there is a privacy breach if the adversary learns unobservable information specific to an individual user's data record from observable information such as model's output predictions.
%This inferred unobservable information about a user's record can be, for instance, the membership details of the record in the training set of the model, referred to as membership inference attacks.
This privacy leakage about a user's record can be, for instance, the membership details of the record in the training set of the model, referred to as membership inference attacks.
Alternatively, an adversary can learn sensitive attributes about the user's data record which can be used to reconstruct the sensitive training dataset.
In this work, we specifically use membership inference attacks to quantify information leakage in machine learning models~\cite{shokri2017membership}.


Machine Learning algorithms learn a function $f:X \rightarrow Y$ mapping from the input space $X$ to the space of corresponding class labels $Y$.
This is modeled as an optimization where the objective is to find the parameters $\theta$ by minimizing the model's loss, $min_{\theta} L(f(x),y;\theta)$.
Machine learning models are more confident while predicting the class of already seen train data record compared to an unseen test data record.
Membership inference attacks exploit this difference in the model's confidence to classify a new data record as being a "Member" or "Non-Member" of the model's training data.
This is a binary decision problem where the adversary classifies the membership of a given input $x$ using the model's output prediction $f(x;\theta)$ to infer whether a given data record was used in the model's training data or not.
Formally, given a user's data record $x$ $\sim$ $P(X,Y)$, where $P(X,Y)$ is the data distribution from which the training data $D_{train}$ was sampled, the adversary estimates $P(x \in D_{train})$ using the model's prediction $f(x;W)$.
Empirically, the adversary identifies a threshold to estimate whether $x \in D_{train}$ which can also be learnt using a binary classifier.
In this work, we use the confidence score attack where the adversary obtains $f(x;W)$ and finds the maximum posterior and infers $x \in D_{train}$ if the maximum is greater than a threshold~\cite{salem2018ml,8429311}.
The attack is based on the observation that the maximal posterior of a member data record is higher (more confident) than a non-member data record of the training dataset.


In this threat model, we consider a blackbox setting where the adversary is assumed to have no knowledge about the target model.
Formally, given a target model $f()$, the adversary only sees the final model prediction $f(x;\theta)$.
The adversary does not know the architecture of $f()$ and the model parameters $\theta$.
We do not consider whitebox setting where the adversary has the access to both the model output predictions $f(x;\theta)$ as well as the architecture of $f()$ and the model parameters $\theta$.
Indeed, this whitebox setting does not necessarily result in any benefit to the adversary in terms of attack accuracy (shown theoretically~\cite{pmlr-v97-sablayrolles19a} and empirically~\cite{DBLP:journals/corr/abs-1906-06589, song2020systematic}.
Consequently, blackbox setting is the more practical setting seen typically in Machine Learning as a Service (MLaaS) where the adversary submits an input query to the trained model on the Cloud via an API and receives the corresponding output.











